

bash: l: command not found...
[postgres@localhost jfrog]$ ls
artifactory
[postgres@localhost jfrog]$ cd artifactory/
[postgres@localhost artifactory]$ ls
app  var
[postgres@localhost artifactory]$ cd var/
[postgres@localhost var]$ ls
backup  bootstrap  data  etc  log  work
[postgres@localhost var]$ cd etc/
[postgres@localhost etc]$ ls
access       filebeat.yaml  logrotate  observability  router    system.basic-template.yaml  system.yaml
artifactory  jfconnect      metadata   replicator     security  system.full-template.yaml
[postgres@localhost etc]$ vi system.yaml
[postgres@localhost etc]$ exit
logout
[root@localhost ~]# ls
anaconda-ks.cfg  jfrog-artifactory-jcr-7.98.19.rpm
[root@localhost ~]# cd /opt/jfrog/
[root@localhost jfrog]# ls
artifactory
[root@localhost jfrog]# cd artifactory/
[root@localhost artifactory]# ls
app  var
[root@localhost artifactory]# cd app/
[root@localhost app]# ls
access  artifactory  artifactory.product.version.properties  bin  doc  event  frontend  metadata  misc  observability  router  run  third-party
[root@localhost app]# cd ..
[root@localhost artifactory]# cd var/etc/
[root@localhost etc]# ls
access       filebeat.yaml  logrotate  observability  router    system.basic-template.yaml  system.yaml
artifactory  jfconnect      metadata   replicator     security  system.full-template.yaml
[root@localhost etc]# vi system.yaml
[root@localhost etc]# systemctl start artifactory.service
[root@localhost etc]# systemctl status artifactory.service
● artifactory.service - Artifactory service
     Loaded: loaded (/usr/lib/systemd/system/artifactory.service; enabled; preset: disabled)
     Active: active (running) since Thu 2025-07-31 16:25:03 IST; 6min ago
   Main PID: 52128 (java)
      Tasks: 0 (limit: 23020)
     Memory: 20.0K
        CPU: 4.386s
     CGroup: /system.slice/artifactory.service
             ‣ 52128 /opt/jfrog/artifactory/app/third-party/java/bin/java -Djava.util.logging.config.file=/opt/jfrog/artifactory/app/artifactory/tomcat/conf>

Jul 31 16:25:00 localhost.localdomain artifactoryManage.sh[46387]: /usr/bin/netstat
Jul 31 16:25:01 localhost.localdomain artifactoryManage.sh[46387]: /usr/bin/netstat
Jul 31 16:25:02 localhost.localdomain artifactoryManage.sh[46387]: /usr/bin/netstat
Jul 31 16:25:03 localhost.localdomain artifactoryManage.sh[46387]: /usr/bin/netstat
Jul 31 16:25:03 localhost.localdomain artifactoryManage.sh[46387]: 2025-07-31T10:55:03.443Z [shell] [INFO ] [] [artifactoryManage.sh:135      ] [main] - Art>
Jul 31 16:25:03 localhost.localdomain artifactoryManage.sh[46387]: 2025-07-31T10:55:03.482Z [shell] [INFO ] [] [artifactoryCommon.sh:1248     ] [main] - Art>
Jul 31 16:25:03 localhost.localdomain systemd[1]: Started Artifactory service.
Jul 31 16:27:54 localhost.localdomain systemd[1]: /usr/lib/systemd/system/artifactory.service:11: PIDFile= references a path below legacy directory /var/run>
Jul 31 16:27:55 localhost.localdomain systemd[1]: /usr/lib/systemd/system/artifactory.service:11: PIDFile= references a path below legacy directory /var/run>
Jul 31 16:28:13 localhost.localdomain systemd[1]: /usr/lib/systemd/system/artifactory.service:11: PIDFile= references a path below legacy directory /var/run>

[root@localhost etc]# systemctl restart artifactory.service
[root@localhost etc]# systemctl status artifactory.service
● artifactory.service - Artifactory service
     Loaded: loaded (/usr/lib/systemd/system/artifactory.service; enabled; preset: disabled)
     Active: active (running) since Thu 2025-07-31 16:32:54 IST; 54s ago
    Process: 54684 ExecStart=/opt/jfrog/artifactory/app/bin/artifactoryManage.sh start (code=exited, status=0/SUCCESS)
   Main PID: 60433 (java)
      Tasks: 0 (limit: 23020)
     Memory: 3.3M
        CPU: 4.582s
     CGroup: /system.slice/artifactory.service
             ‣ 60433 /opt/jfrog/artifactory/app/third-party/java/bin/java -Djava.util.logging.config.file=/opt/jfrog/artifactory/app/artifactory/tomcat/conf>

Jul 31 16:32:47 localhost.localdomain artifactoryManage.sh[54684]: /usr/bin/netstat
Jul 31 16:32:48 localhost.localdomain artifactoryManage.sh[54684]: /usr/bin/netstat
Jul 31 16:32:49 localhost.localdomain artifactoryManage.sh[54684]: /usr/bin/netstat
Jul 31 16:32:50 localhost.localdomain artifactoryManage.sh[54684]: /usr/bin/netstat
Jul 31 16:32:51 localhost.localdomain artifactoryManage.sh[54684]: /usr/bin/netstat
Jul 31 16:32:52 localhost.localdomain artifactoryManage.sh[54684]: /usr/bin/netstat
Jul 31 16:32:54 localhost.localdomain artifactoryManage.sh[54684]: /usr/bin/netstat
Jul 31 16:32:54 localhost.localdomain artifactoryManage.sh[54684]: 2025-07-31T11:02:54.071Z [shell] [INFO ] [] [artifactoryManage.sh:135      ] [main] - Art>
Jul 31 16:32:54 localhost.localdomain artifactoryManage.sh[54684]: 2025-07-31T11:02:54.122Z [shell] [INFO ] [] [artifactoryCommon.sh:1248     ] [main] - Art>
Jul 31 16:32:54 localhost.localdomain systemd[1]: Started Artifactory service.

[root@localhost etc]# yum install nginx
Updating Subscription Management repositories.
Last metadata expiration check: 0:10:07 ago on Thursday 31 July 2025 04:26:42 PM.
Dependencies resolved.
=============================================================================================================================================================
 Package                             Architecture            Version                                 Repository                                         Size
=============================================================================================================================================================
Installing:
 nginx                               x86_64                  2:1.20.1-22.el9_6.3                     rhel-9-for-x86_64-appstream-rpms                   37 k
Installing dependencies:
 nginx-core                          x86_64                  2:1.20.1-22.el9_6.3                     rhel-9-for-x86_64-appstream-rpms                  571 k
 nginx-filesystem                    noarch                  2:1.20.1-22.el9_6.3                     rhel-9-for-x86_64-appstream-rpms                   10 k
 redhat-logos-httpd                  noarch                  90.5-1.el9_6.1                          rhel-9-for-x86_64-appstream-rpms                   16 k

Transaction Summary
=============================================================================================================================================================
Install  4 Packages

Total download size: 635 k
Installed size: 1.8 M
Is this ok [y/N]: y
Downloading Packages:
(1/4): nginx-1.20.1-22.el9_6.3.x86_64.rpm                                                                                     81 kB/s |  37 kB     00:00
(2/4): nginx-filesystem-1.20.1-22.el9_6.3.noarch.rpm                                                                          18 kB/s |  10 kB     00:00
(3/4): nginx-core-1.20.1-22.el9_6.3.x86_64.rpm                                                                               879 kB/s | 571 kB     00:00
(4/4): redhat-logos-httpd-90.5-1.el9_6.1.noarch.rpm                                                                           55 kB/s |  16 kB     00:00
-------------------------------------------------------------------------------------------------------------------------------------------------------------
Total                                                                                                                        837 kB/s | 635 kB     00:00
Red Hat Enterprise Linux 9 for x86_64 - AppStream (RPMs)                                                                     1.0 MB/s | 3.6 kB     00:00
Importing GPG key 0xFD431D51:
 Userid     : "Red Hat, Inc. (release key 2) <security@redhat.com>"
 Fingerprint: 567E 347A D004 4ADE 55BA 8A5F 199E 2F91 FD43 1D51
 From       : /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release
Is this ok [y/N]: y
Key imported successfully
Importing GPG key 0x5A6340B3:
 Userid     : "Red Hat, Inc. (auxiliary key 3) <security@redhat.com>"
 Fingerprint: 7E46 2425 8C40 6535 D56D 6F13 5054 E4A4 5A63 40B3
 From       : /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release
Is this ok [y/N]: y
Key imported successfully
Running transaction check
yTransaction check succeeded.
Running transaction test
Transaction test succeeded.
Running transaction
  Preparing        :                                                                                                                                     1/1
  Running scriptlet: nginx-filesystem-2:1.20.1-22.el9_6.3.noarch                                                                                         1/4
  Installing       : nginx-filesystem-2:1.20.1-22.el9_6.3.noarch                                                                                         1/4
  Installing       : nginx-core-2:1.20.1-22.el9_6.3.x86_64                                                                                               2/4
  Installing       : redhat-logos-httpd-90.5-1.el9_6.1.noarch                                                                                            3/4
  Installing       : nginx-2:1.20.1-22.el9_6.3.x86_64                                                                                                    4/4
  Running scriptlet: nginx-2:1.20.1-22.el9_6.3.x86_64                                                                                                    4/4
  Verifying        : nginx-2:1.20.1-22.el9_6.3.x86_64                                                                                                    1/4
  Verifying        : nginx-core-2:1.20.1-22.el9_6.3.x86_64                                                                                               2/4
  Verifying        : nginx-filesystem-2:1.20.1-22.el9_6.3.noarch                                                                                         3/4
  Verifying        : redhat-logos-httpd-90.5-1.el9_6.1.noarch                                                                                            4/4
Installed products updated.

Installed:
  nginx-2:1.20.1-22.el9_6.3.x86_64                    nginx-core-2:1.20.1-22.el9_6.3.x86_64            nginx-filesystem-2:1.20.1-22.el9_6.3.noarch
  redhat-logos-httpd-90.5-1.el9_6.1.noarch

Complete!
[root@localhost etc]# cd /etc/nginx/
conf.d/    default.d/
[root@localhost etc]# cd /etc/nginx/
conf.d/    default.d/
[root@localhost etc]# cd /etc/nginx/
[root@localhost nginx]# ll
total 68
drwxr-xr-x. 2 root root    6 May 15 01:52 conf.d
drwxr-xr-x. 2 root root    6 May 15 01:52 default.d
-rw-r--r--. 1 root root 1077 May 15 01:51 fastcgi.conf
-rw-r--r--. 1 root root 1077 May 15 01:51 fastcgi.conf.default
-rw-r--r--. 1 root root 1007 May 15 01:51 fastcgi_params
-rw-r--r--. 1 root root 1007 May 15 01:51 fastcgi_params.default
-rw-r--r--. 1 root root 2837 May 15 01:51 koi-utf
-rw-r--r--. 1 root root 2223 May 15 01:51 koi-win
-rw-r--r--. 1 root root 5231 May 15 01:51 mime.types
-rw-r--r--. 1 root root 5231 May 15 01:51 mime.types.default
-rw-r--r--. 1 root root 2334 May 15 01:50 nginx.conf
-rw-r--r--. 1 root root 2656 May 15 01:51 nginx.conf.default
-rw-r--r--. 1 root root  636 May 15 01:51 scgi_params
-rw-r--r--. 1 root root  636 May 15 01:51 scgi_params.default
-rw-r--r--. 1 root root  664 May 15 01:51 uwsgi_params
-rw-r--r--. 1 root root  664 May 15 01:51 uwsgi_params.default
-rw-r--r--. 1 root root 3610 May 15 01:51 win-utf
[root@localhost nginx]# podman login 192.168.95.71:8081
Username: admin
Password:
Error: authenticating creds for "192.168.95.71:8081": pinging container registry 192.168.95.71:8081: Get "https://192.168.95.71:8081/v2/": http: server gave HTTP response to HTTPS client
[root@localhost nginx]# podman login http://192.168.95.71:8081
Username: admin
Password:
Error: authenticating creds for "192.168.95.71:8081": pinging container registry 192.168.95.71:8081: Get "https://192.168.95.71:8081/v2/": http: server gave HTTP response to HTTPS client
[root@localhost nginx]# podman login http://192.168.95.71:8081
Username: admin
Password:
Error: authenticating creds for "192.168.95.71:8081": pinging container registry 192.168.95.71:8081: Get "https://192.168.95.71:8081/v2/": http: server gave HTTP response to HTTPS client
[root@localhost nginx]# podman login http://192.168.95.71:8081
Username: admin
Password:
Error: authenticating creds for "192.168.95.71:8081": pinging container registry 192.168.95.71:8081: Get "https://192.168.95.71:8081/v2/": http: server gave HTTP response to HTTPS client
[root@localhost nginx]#
[root@localhost nginx]#
[root@localhost nginx]#
[root@localhost nginx]#
[root@localhost nginx]# pwd
/etc/nginx
[root@localhost nginx]# ll
total 68
drwxr-xr-x. 2 root root    6 May 15 01:52 conf.d
drwxr-xr-x. 2 root root    6 May 15 01:52 default.d
-rw-r--r--. 1 root root 1077 May 15 01:51 fastcgi.conf
-rw-r--r--. 1 root root 1077 May 15 01:51 fastcgi.conf.default
-rw-r--r--. 1 root root 1007 May 15 01:51 fastcgi_params
-rw-r--r--. 1 root root 1007 May 15 01:51 fastcgi_params.default
-rw-r--r--. 1 root root 2837 May 15 01:51 koi-utf
-rw-r--r--. 1 root root 2223 May 15 01:51 koi-win
-rw-r--r--. 1 root root 5231 May 15 01:51 mime.types
-rw-r--r--. 1 root root 5231 May 15 01:51 mime.types.default
-rw-r--r--. 1 root root 2334 May 15 01:50 nginx.conf
-rw-r--r--. 1 root root 2656 May 15 01:51 nginx.conf.default
-rw-r--r--. 1 root root  636 May 15 01:51 scgi_params
-rw-r--r--. 1 root root  636 May 15 01:51 scgi_params.default
-rw-r--r--. 1 root root  664 May 15 01:51 uwsgi_params
-rw-r--r--. 1 root root  664 May 15 01:51 uwsgi_params.default
-rw-r--r--. 1 root root 3610 May 15 01:51 win-utf
[root@localhost nginx]# ip r l
default via 192.168.10.1 dev ens192 proto static metric 100
192.168.0.0/16 dev ens192 proto kernel scope link src 192.168.95.71 metric 100
[root@localhost nginx]# mkdir -p ssl
[root@localhost nginx]# vim nginx.conf
[root@localhost nginx]# pwd
/etc/nginx
[root@localhost nginx]# vim nginx.conf
[root@localhost nginx]#
[root@localhost nginx]#
[root@localhost nginx]# systemctl retart nginx
Unknown command verb retart.
[root@localhost nginx]# systemctl restart nginx
[root@localhost nginx]# systemctl status nginx
● nginx.service - The nginx HTTP and reverse proxy server
     Loaded: loaded (/usr/lib/systemd/system/nginx.service; disabled; preset: disabled)
     Active: active (running) since Thu 2025-07-31 16:48:49 IST; 5s ago
    Process: 92849 ExecStartPre=/usr/bin/rm -f /run/nginx.pid (code=exited, status=0/SUCCESS)
    Process: 92850 ExecStartPre=/usr/sbin/nginx -t (code=exited, status=0/SUCCESS)
    Process: 92851 ExecStart=/usr/sbin/nginx (code=exited, status=0/SUCCESS)
   Main PID: 92853 (nginx)
      Tasks: 3 (limit: 23020)
     Memory: 3.3M
        CPU: 44ms
     CGroup: /system.slice/nginx.service
             ├─92853 "nginx: master process /usr/sbin/nginx"
             ├─92854 "nginx: worker process"
             └─92855 "nginx: worker process"

Jul 31 16:48:49 localhost.localdomain systemd[1]: Starting The nginx HTTP and reverse proxy server...
Jul 31 16:48:49 localhost.localdomain nginx[92850]: nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
Jul 31 16:48:49 localhost.localdomain nginx[92850]: nginx: configuration file /etc/nginx/nginx.conf test is successful
Jul 31 16:48:49 localhost.localdomain systemd[1]: Started The nginx HTTP and reverse proxy server.
[root@localhost nginx]# ip r l
default via 192.168.10.1 dev ens192 proto static metric 100
192.168.0.0/16 dev ens192 proto kernel scope link src 192.168.95.71 metric 100
[root@localhost nginx]#
[root@localhost nginx]#
[root@localhost nginx]# podman login -u admin 192.168.95.71:9443
Password:
Error: authenticating creds for "192.168.95.71:9443": pinging container registry 192.168.95.71:9443: Get "https://192.168.95.71:9443/v2/": tls: failed to verify certificate: x509: certificate signed by unknown authority
[root@localhost nginx]#
[root@localhost nginx]#
[root@localhost nginx]#
[root@localhost nginx]# openssl s_client -connect 192.168.95.71:9443
CONNECTED(00000003)
Can't use SSL_get_servername
depth=0 C = US, ST = TestState, L = TestCity, O = MyOrg, OU = MyUnit, CN = jfrog.prodevans.com
verify error:num=18:self-signed certificate
verify return:1
depth=0 C = US, ST = TestState, L = TestCity, O = MyOrg, OU = MyUnit, CN = jfrog.prodevans.com
verify return:1
---
Certificate chain
 0 s:C = US, ST = TestState, L = TestCity, O = MyOrg, OU = MyUnit, CN = jfrog.prodevans.com
   i:C = US, ST = TestState, L = TestCity, O = MyOrg, OU = MyUnit, CN = jfrog.prodevans.com
   a:PKEY: rsaEncryption, 2048 (bit); sigalg: RSA-SHA256
   v:NotBefore: Jul 31 11:15:24 2025 GMT; NotAfter: Jul 31 11:15:24 2026 GMT
---
Server certificate
-----BEGIN CERTIFICATE-----
MIIDvjCCAqagAwIBAgIUZ/sbWZPF+VZ6hYWEHQ6eBhgdfOgwDQYJKoZIhvcNAQEL
BQAwczELMAkGA1UEBhMCVVMxEjAQBgNVBAgMCVRlc3RTdGF0ZTERMA8GA1UEBwwI
VGVzdENpdHkxDjAMBgNVBAoMBU15T3JnMQ8wDQYDVQQLDAZNeVVuaXQxHDAaBgNV
BAMME2pmcm9nLnByb2RldmFucy5jb20wHhcNMjUwNzMxMTExNTI0WhcNMjYwNzMx
MTExNTI0WjBzMQswCQYDVQQGEwJVUzESMBAGA1UECAwJVGVzdFN0YXRlMREwDwYD
VQQHDAhUZXN0Q2l0eTEOMAwGA1UECgwFTXlPcmcxDzANBgNVBAsMBk15VW5pdDEc
MBoGA1UEAwwTamZyb2cucHJvZGV2YW5zLmNvbTCCASIwDQYJKoZIhvcNAQEBBQAD
ggEPADCCAQoCggEBAOOPCBr1oVDkX877JkERIRJIDuIPIrahAMs6KSW+1ERoYEw7
WRTdheHdeXqIxdEubfOuN0stKjWkV1/zzVWryoP9uYz0dho/M9xsGeClJqIw3bcz
NHqinpNoBV0r8OLmGNwEaMY+Gd6PjWkxLex3IaWRNBiJUI/3DW2q8W4jwo9axEWe
xMOqWGaU5zwUrKSmC+T6ehTQ3kihohxl7rWRFh++TboMwvT7GzIc6qhAfiKhIESV
DyvlmfUeBk/hPWgK7KZZP2tEPJfljJe7HM1AWZ+hd2rhlNVbFCQlqVD/73A0/a3P
dVYVLosLsu9N7tobn+O2GaOogsRMbmdEC86EkHsCAwEAAaNKMEgwJwYDVR0RBCAw
HocEwKhfRYcEwKhfRocEwKhfR4cEwKhfSIcEwKhfSTAdBgNVHQ4EFgQUkS+yMhkE
1cWfMsmf7+xfOYr08NEwDQYJKoZIhvcNAQELBQADggEBADAKioARB53aXDUPBGGt
AJUtV15HN4NF570hRX9df5NMCz580bhAjv6XQFAlYSN2rdxCqKGnq4vu47KnvSbZ
we0arOpxmZqvp0VuL9dFllvURrIS25Xi/wx6E30wtp3t+ubS1Md7ZzxsK3sC82VP
NMSU7PsKah3qAolvKY4PUvJgreUA0haF4nHyqrAPWTnumfAuKH2Wx9v6lT5A343j
EGITEmnMdIl0vsPeU2DizjibFLwW0kEiK2gK+I+TSxdib70U4z3KyX1K8Z161aIG
W0wpAdnbpJ4Q4XzyiqCyyqB71l8xyNbExkLWWgshBTdUiu/Hsmtnbv0Ebw9mHv66
9Z8=
-----END CERTIFICATE-----
subject=C = US, ST = TestState, L = TestCity, O = MyOrg, OU = MyUnit, CN = jfrog.prodevans.com
issuer=C = US, ST = TestState, L = TestCity, O = MyOrg, OU = MyUnit, CN = jfrog.prodevans.com
---
No client certificate CA names sent
Peer signing digest: SHA256
Peer signature type: RSA-PSS
Server Temp Key: X25519, 253 bits
---
SSL handshake has read 1518 bytes and written 375 bytes
Verification error: self-signed certificate
---
New, TLSv1.3, Cipher is TLS_AES_256_GCM_SHA384
Server public key is 2048 bit
Secure Renegotiation IS NOT supported
Compression: NONE
Expansion: NONE
No ALPN negotiated
Early data was not sent
Verify return code: 18 (self-signed certificate)
---
---
Post-Handshake New Session Ticket arrived:
SSL-Session:
    Protocol  : TLSv1.3
    Cipher    : TLS_AES_256_GCM_SHA384
    Session-ID: 73A5B083D0D05431AFE6C635402A02C7E6A552C7A537978A16C4700E1C62A360
    Session-ID-ctx:
    Resumption PSK: 9086F96958F185678D210C84F30EEDB137B2B9A2BF1987C4D29CAD03389DFB5719359F2C4BE05ABB80A3BE892C595CFF
    PSK identity: None
    PSK identity hint: None
    SRP username: None
    TLS session ticket lifetime hint: 600 (seconds)
    TLS session ticket:
    0000 - 61 ce b3 3f f8 e9 3a 59-24 c4 b8 de bd b9 2a 42   a..?..:Y$.....*B
    0010 - 7f e6 49 bb 6b 47 15 8e-f8 4e 41 92 d5 39 d0 65   ..I.kG...NA..9.e
    0020 - 67 e5 8b 43 13 78 e4 86-50 dc 4a 36 d9 91 15 75   g..C.x..P.J6...u
    0030 - cd 94 97 52 cf 51 e0 26-94 a0 be 89 48 6a 7e 2e   ...R.Q.&....Hj~.
    0040 - 42 ca 5f 3c 26 c8 36 67-d3 4f b6 c7 28 6c d5 f5   B._<&.6g.O..(l..
    0050 - d9 80 74 c5 7f 0f ae ba-74 f5 da 4b 72 a5 15 48   ..t.....t..Kr..H
    0060 - 79 c2 ce 5b a2 33 30 4d-57 fa 00 0e 59 da f4 e7   y..[.30MW...Y...
    0070 - 56 2d a2 80 09 4d 10 34-dd 3e 73 1e 8c d7 a5 c0   V-...M.4.>s.....
    0080 - 88 cd b1 47 7b ce b7 d5-50 ec 59 08 23 9a e7 27   ...G{...P.Y.#..'
    0090 - 01 28 d7 ba c6 1e 30 64-c7 79 0e f4 8e 81 cc b8   .(....0d.y......
    00a0 - 75 44 26 f3 95 e9 ec 12-32 f9 dd 6f 9c 6d e0 4c   uD&.....2..o.m.L
    00b0 - b1 65 91 be 13 e3 35 19-d6 2c 57 70 8e 9e b7 a5   .e....5..,Wp....
    00c0 - 34 49 86 2a e6 0d 46 e0-fa 1f a7 60 63 ce 34 2e   4I.*..F....`c.4.
    00d0 - cc e2 a1 e3 9b 82 0c ae-74 42 f9 bf f1 f7 f8 4b   ........tB.....K

    Start Time: 1753960874
    Timeout   : 7200 (sec)
    Verify return code: 18 (self-signed certificate)
    Extended master secret: no
    Max Early Data: 0
---
read R BLOCK
---
Post-Handshake New Session Ticket arrived:
SSL-Session:
    Protocol  : TLSv1.3
    Cipher    : TLS_AES_256_GCM_SHA384
    Session-ID: 4768515611B63E43173A10C186B6FF3066F3D6552D23F359F1E0253BB04B3D24
    Session-ID-ctx:
    Resumption PSK: 8A3F17E960B9BD92A22582485B2F7510A083910B1D51717594662165A05E283D9DD5EEF77F4584F2FF64644E8B8D7BE1
    PSK identity: None
    PSK identity hint: None
    SRP username: None
    TLS session ticket lifetime hint: 600 (seconds)
    TLS session ticket:
    0000 - 61 ce b3 3f f8 e9 3a 59-24 c4 b8 de bd b9 2a 42   a..?..:Y$.....*B
    0010 - f1 83 29 84 a4 52 01 f9-1a 0b 95 65 ff d1 9e c0   ..)..R.....e....
    0020 - 33 65 2d 82 2a b1 5f 08-96 64 d8 74 65 2c a4 d9   3e-.*._..d.te,..
    0030 - 3a 1f 01 c2 fa cf 27 f8-4d 53 42 e0 0c 62 e4 ad   :.....'.MSB..b..
    0040 - fc 4c 47 97 f4 84 0b ee-33 87 57 88 35 7b be 96   .LG.....3.W.5{..
    0050 - 6a e4 ec c9 3b 92 11 0e-11 1b 20 06 b1 98 d3 74   j...;..... ....t
    0060 - 18 f2 44 b6 c7 8c 20 f0-4d 47 6a d3 bf 4c ac 31   ..D... .MGj..L.1
    0070 - 64 13 0e f9 b9 ba 77 9e-0c d1 a2 ca b1 ac b1 46   d.....w........F
    0080 - 90 13 c7 c8 a5 ab 5a 16-59 77 93 94 e0 5e ac 4c   ......Z.Yw...^.L
    0090 - f4 d9 b4 53 15 10 76 8a-bc 2c 01 94 49 42 bc 95   ...S..v..,..IB..
    00a0 - 26 e3 4a 1c 96 7e 0d 29-a6 f1 e1 a2 d2 01 aa f9   &.J..~.)........
    00b0 - 77 a5 02 c3 80 e1 28 4e-02 30 97 fd 11 ed 85 14   w.....(N.0......
    00c0 - 27 6e 49 32 10 3a d6 b9-28 d5 cf 2f 1a 50 a0 4f   'nI2.:..(../.P.O
    00d0 - f3 55 7e 8b a5 44 c6 44-f9 a1 c1 4d 11 c2 48 e7   .U~..D.D...M..H.

    Start Time: 1753960874
    Timeout   : 7200 (sec)
    Verify return code: 18 (self-signed certificate)
    Extended master secret: no
    Max Early Data: 0
---
read R BLOCK
^C
[root@localhost nginx]# pwd
/etc/nginx
[root@localhost nginx]# cd /etc/containers/certs.d/
[root@localhost certs.d]# mkdir 192.168.95.71:9443
[root@localhost certs.d]# cd 192.168.95.71\:9443/
[root@localhost 192.168.95.71:9443]# vi jfrog.cert
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]# podman login -u admin 192.168.95.71:9443
Password:
Error: authenticating creds for "192.168.95.71:9443": creating new docker client: missing key jfrog.key for client certificate jfrog.cert. Note that CA certificates should use the extension .crt
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]# ll
total 4
-rw-r--r--. 1 root root 1359 Jul 31 16:52 jfrog.cert
[root@localhost 192.168.95.71:9443]# cp jfrog.cert ca.crt
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]# podman login -u admin 192.168.95.71:9443
Password:
Error: authenticating creds for "192.168.95.71:9443": creating new docker client: missing key jfrog.key for client certificate jfrog.cert. Note that CA certificates should use the extension .crt
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]# ll
total 8
-rw-r--r--. 1 root root 1359 Jul 31 16:52 ca.crt
-rw-r--r--. 1 root root 1359 Jul 31 16:52 jfrog.cert
[root@localhost 192.168.95.71:9443]# rm -rf jfrog.cert
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]# podman login -u admin 192.168.95.71:9443
Password:
Error: authenticating creds for "192.168.95.71:9443": Get "https://192.168.95.71/v2/token?account=admin&service=192.168.95.71": dial tcp 192.168.95.71:443: connect: connection refused
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]# cd /etc/nginx/ssl
[root@localhost ssl]# ll
total 12
-rw-r--r--. 1 root root 2296 Jul 31 16:47 nginx.conf
-rw-r--r--. 1 root root 1359 Jul 31 16:46 server.crt
-rw-------. 1 root root 1708 Jul 31 16:46 server.key
[root@localhost ssl]# podman login -u admin 192.168.95.71:9443
Password:
Error: authenticating creds for "192.168.95.71:9443": Get "https://192.168.95.71/v2/token?account=admin&service=192.168.95.71": dial tcp 192.168.95.71:443: connect: connection refused
[root@localhost ssl]#
[root@localhost ssl]#
[root@localhost ssl]#
[root@localhost ssl]# pwd
/etc/nginx/ssl
[root@localhost ssl]# ip r l
default via 192.168.10.1 dev ens192 proto static metric 100
192.168.0.0/16 dev ens192 proto kernel scope link src 192.168.95.71 metric 100
[root@localhost ssl]# cd /etc/nginx/
[root@localhost nginx]# cp nginx.conf old_nginx.conf
[root@localhost nginx]# > nginx.conf
[root@localhost nginx]# vim nginx.conf
[root@localhost nginx]#
[root@localhost nginx]#
[root@localhost nginx]# systemctl restart nginx.service
[root@localhost nginx]# systemctl status nginx.service
● nginx.service - The nginx HTTP and reverse proxy server
     Loaded: loaded (/usr/lib/systemd/system/nginx.service; disabled; preset: disabled)
     Active: active (running) since Thu 2025-07-31 16:59:04 IST; 4s ago
    Process: 93295 ExecStartPre=/usr/bin/rm -f /run/nginx.pid (code=exited, status=0/SUCCESS)
    Process: 93296 ExecStartPre=/usr/sbin/nginx -t (code=exited, status=0/SUCCESS)
    Process: 93297 ExecStart=/usr/sbin/nginx (code=exited, status=0/SUCCESS)
   Main PID: 93299 (nginx)
      Tasks: 3 (limit: 23020)
     Memory: 3.6M
        CPU: 41ms
     CGroup: /system.slice/nginx.service
             ├─93299 "nginx: master process /usr/sbin/nginx"
             ├─93300 "nginx: worker process"
             └─93301 "nginx: worker process"

Jul 31 16:59:04 localhost.localdomain systemd[1]: Starting The nginx HTTP and reverse proxy server...
Jul 31 16:59:04 localhost.localdomain nginx[93296]: nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
Jul 31 16:59:04 localhost.localdomain nginx[93296]: nginx: configuration file /etc/nginx/nginx.conf test is successful
Jul 31 16:59:04 localhost.localdomain systemd[1]: Started The nginx HTTP and reverse proxy server.
[root@localhost nginx]#
[root@localhost nginx]# netstat -tunlpe
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       User       Inode      PID/Program name
tcp        0      0 127.0.0.1:5432          0.0.0.0:*               LISTEN      26         289031     53027/postgres
tcp        0      0 0.0.0.0:9443            0.0.0.0:*               LISTEN      0          346011     93299/nginx: master
tcp        0      0 127.0.0.1:8049          0.0.0.0:*               LISTEN      980        300278     59691/jf-router
tcp        0      0 127.0.0.1:8047          0.0.0.0:*               LISTEN      980        301084     59691/jf-router
tcp        0      0 127.0.0.1:8046          0.0.0.0:*               LISTEN      980        300282     59691/jf-router
tcp        0      0 127.0.0.1:8036          0.0.0.0:*               LISTEN      980        301248     60248/jf-observabil
tcp        0      0 127.0.0.1:8086          0.0.0.0:*               LISTEN      980        301165     59888/jf-metadata
tcp        0      0 127.0.0.1:8070          0.0.0.0:*               LISTEN      980        301414     60082/node
tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      0          21855      1079/cupsd
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      0          22789      1081/sshd: /usr/sbi
tcp6       0      0 :::8081                 :::*                    LISTEN      980        299544     60433/java
tcp6       0      0 :::8082                 :::*                    LISTEN      980        300283     59691/jf-router
tcp6       0      0 ::1:631                 :::*                    LISTEN      0          21854      1079/cupsd
tcp6       0      0 127.0.0.1:8040          :::*                    LISTEN      980        298573     59523/java
tcp6       0      0 127.0.0.1:8045          :::*                    LISTEN      980        299881     59523/java
tcp6       0      0 127.0.0.1:8016          :::*                    LISTEN      980        299932     59523/java
tcp6       0      0 127.0.0.1:8015          :::*                    LISTEN      980        300072     60433/java
tcp6       0      0 127.0.0.1:8091          :::*                    LISTEN      980        299545     60433/java
tcp6       0      0 ::1:5432                :::*                    LISTEN      26         289030     53027/postgres
tcp6       0      0 :::22                   :::*                    LISTEN      0          22791      1081/sshd: /usr/sbi
udp        0      0 127.0.0.1:323           0.0.0.0:*                           0          20317      923/chronyd
udp        0      0 0.0.0.0:44235           0.0.0.0:*                           70         21165      855/avahi-daemon: r
udp        0      0 0.0.0.0:5353            0.0.0.0:*                           70         21163      855/avahi-daemon: r
udp6       0      0 :::34988                :::*                                70         21166      855/avahi-daemon: r
udp6       0      0 ::1:323                 :::*                                0          20318      923/chronyd
udp6       0      0 :::5353                 :::*                                70         21164      855/avahi-daemon: r
[root@localhost nginx]# podman login -u admin 192.168.95.71:9443
Password:
Login Succeeded!
[root@localhost nginx]# ip r
default via 192.168.10.1 dev ens192 proto static metric 100
192.168.0.0/16 dev ens192 proto kernel scope link src 192.168.95.71 metric 100
[root@localhost nginx]# cd
[root@localhost ~]# podman pull registry.k8s.io/kube-apiserver:v1.32.0
Trying to pull registry.k8s.io/kube-apiserver:v1.32.0...
Getting image source signatures
Copying blob 2e4cf50eeb92 done   |
Copying blob d82bc7a76a83 done   |
Copying blob 0674a6f58b64 done   |
Copying blob b6824ed73363 done   |
Copying blob 6f4cfee9177b done   |
Copying blob 7c12895b777b done   |
Copying blob 5664b15f108b done   |
Copying blob 27be814a09eb done   |
Copying blob 4aa0ea1413d3 done   |
Copying blob 3f4e2c586348 done   |
Copying blob 9aee425378d2 done   |
Copying blob a88274b8bba7 done   |
Copying blob 890390ebdc76 done   |
Copying config c2e17b8d0f done   |
Writing manifest to image destination
c2e17b8d0f4a39ed32f1c1fd4eb408627c94111ae9a46c2034758e4ced4f79c4
[root@localhost ~]#  podman pull registry.k8s.io/kube-controller-manager:v1.32.0
 podman pull registry.k8s.io/kube-scheduler:v1.32.0
 podman pull registry.k8s.io/kube-proxy:v1.32.0
 podman pull registry.k8s.io/pause:3.9
 podman pull registry.k8s.io/etcd:3.5.9-0
 podman pull registry.k8s.io/coredns/coredns:v1.10.1
Trying to pull registry.k8s.io/kube-controller-manager:v1.32.0...
Getting image source signatures
Copying blob 2e4cf50eeb92 skipped: already exists
Copying blob d82bc7a76a83 skipped: already exists
Copying blob 6f4cfee9177b skipped: already exists
Copying blob b6824ed73363 skipped: already exists
Copying blob 7c12895b777b skipped: already exists
Copying blob 5664b15f108b skipped: already exists
Copying blob 27be814a09eb skipped: already exists
Copying blob 0674a6f58b64 skipped: already exists
Copying blob 3f4e2c586348 skipped: already exists
Copying blob 9aee425378d2 skipped: already exists
Copying blob a88274b8bba7 skipped: already exists
Copying blob 9b34c506a565 done   |
Copying blob 4aa0ea1413d3 skipped: already exists
Copying config 8cab3d2a8b done   |
Writing manifest to image destination
8cab3d2a8bd0fe4127810f35afe0ffd42bfe75b2a4712a84da5595d4bde617d3
Trying to pull registry.k8s.io/kube-scheduler:v1.32.0...
Getting image source signatures
Copying blob d82bc7a76a83 skipped: already exists
Copying blob 6f4cfee9177b skipped: already exists
Copying blob 2e4cf50eeb92 skipped: already exists
Copying blob b6824ed73363 skipped: already exists
Copying blob 7c12895b777b skipped: already exists
Copying blob 5664b15f108b skipped: already exists
Copying blob a88274b8bba7 skipped: already exists
Copying blob 0674a6f58b64 skipped: already exists
Copying blob 4aa0ea1413d3 skipped: already exists
Copying blob 3f4e2c586348 skipped: already exists
Copying blob 9aee425378d2 skipped: already exists
Copying blob 27be814a09eb skipped: already exists
Copying blob 72b3eca693e1 done   |
Copying config a389e107f4 done   |
Writing manifest to image destination
a389e107f4ff1130c69849f0af08cbce9a1dfe3b7c39874012587d233807cfc5
Trying to pull registry.k8s.io/kube-proxy:v1.32.0...
Getting image source signatures
Copying blob 4d7aa327eef4 done   |
Copying blob 4d90f05edc6a done   |
Copying config 040f9f8aac done   |
Writing manifest to image destination
040f9f8aac8cd21d78f05ebfa9621ffb84e3257300c3cb1f72b539a3c3a2cd08
Trying to pull registry.k8s.io/pause:3.9...
Getting image source signatures
Copying blob 61fec91190a0 done   |
Copying config e6f1816883 done   |
Writing manifest to image destination
e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c
Trying to pull registry.k8s.io/etcd:3.5.9-0...
Getting image source signatures
Copying blob 1e3d9b7d1452 done   |
Copying blob fe5ca62666f0 done   |
Copying blob a7ca0d9ba68f done   |
Copying blob fcb6f6d2c998 done   |
Copying blob b02a7525f878 done   |
Copying blob e8c73c638ae9 done   |
Copying blob 4aa0ea1413d3 skipped: already exists
Copying blob 7c881f9ab25e done   |
Copying blob 5627a970d25e done   |
Copying blob 3f4a72e37652 done   |
Copying blob 93182a730d98 done   |
Copying blob 5198587edd6d done   |
Copying config 73deb9a3f7 done   |
Writing manifest to image destination
73deb9a3f702532592a4167455f8bf2e5f5d900bcc959ba2fd2d35c321de1af9
Trying to pull registry.k8s.io/coredns/coredns:v1.10.1...
Getting image source signatures
Copying blob 3799eae1a077 done   |
Copying blob 25b7032c281a done   |
Copying config ead0a4a53d done   |
Writing manifest to image destination
ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc
[root@localhost ~]# podman images
REPOSITORY                               TAG         IMAGE ID      CREATED       SIZE
registry.k8s.io/kube-apiserver           v1.32.0     c2e17b8d0f4a  7 months ago  98.1 MB
registry.k8s.io/kube-controller-manager  v1.32.0     8cab3d2a8bd0  7 months ago  90.8 MB
registry.k8s.io/kube-scheduler           v1.32.0     a389e107f4ff  7 months ago  70.6 MB
registry.k8s.io/kube-proxy               v1.32.0     040f9f8aac8c  7 months ago  95.3 MB
registry.k8s.io/etcd                     3.5.9-0     73deb9a3f702  2 years ago   295 MB
registry.k8s.io/coredns/coredns          v1.10.1     ead0a4a53df8  2 years ago   53.6 MB
registry.k8s.io/pause                    3.9         e6f181688397  2 years ago   750 kB
[root@localhost ~]# podman tag registry.k8s.io/kube-apiserver:v1.32.0 192.168.95.71:9443/k8s/kube-apiserver:v1.32.0
[root@localhost ~]# podman push 192.168.95.71:9443/k8s/kube-apiserver:v1.32.0
Getting image source signatures
Copying blob 6f1cdceb6a31 done   |
Copying blob af5aa97ebe6c done   |
Copying blob d37950ece3d3 done   |
Copying blob 8fa10c0194df done   |
Copying blob 4d049f83d9cf done   |
Copying blob ddc6e550070c done   |
Copying blob bbb6cacb8c82 done   |
Copying blob 1a73b54f556b done   |
Copying blob f4aee9e53c42 done   |
Copying blob b336e209998f done   |
Copying blob 2a92d6ac9e4f done   |
Copying blob b0934aa74a46 done   |
Copying blob 94fbc2e8456a done   |
Copying config c2e17b8d0f done   |
Writing manifest to image destination
[root@localhost ~]# podman tag registry.k8s.io/kube-controller-manager:v1.32.0 192.168.95.71:9443/k8s/kube-controller-manager:v1.32.0
[root@localhost ~]# podman push 192.168.95.71:9443/k8s/kube-controller-manager:v1.32.0
Getting image source signatures
Copying blob 2dedfe187a19 skipped: already exists
Copying blob 02c91f6b395a skipped: already exists
Copying blob ffdf8cfdbafe skipped: already exists
Copying blob 818beb2a6ac5 skipped: already exists
Copying blob 70f76ff64431 skipped: already exists
Copying blob 0b2215fb0760 skipped: already exists
Copying blob 1e45aa110bce skipped: already exists
Copying blob 36e6687b1ede skipped: already exists
Copying blob 12b98db1f9a3 skipped: already exists
Copying blob 2bc97598cf09 skipped: already exists
Copying blob b4aa04aa577f skipped: already exists
Copying blob fffa1ce2a781 skipped: already exists
Copying blob 4d408fe32c6b done   |
Copying config 8cab3d2a8b done   |
Writing manifest to image destination
[root@localhost ~]# podman tag registry.k8s.io/kube-scheduler:v1.32.0 192.168.95.71:9443/k8s/kube-scheduler:v1.32.0
[root@localhost ~]# podman push 192.168.95.71:9443/k8s/kube-scheduler:v1.32.0
Getting image source signatures
Copying blob 818beb2a6ac5 skipped: already exists
Copying blob 02c91f6b395a skipped: already exists
Copying blob 70f76ff64431 skipped: already exists
Copying blob 12b98db1f9a3 skipped: already exists
Copying blob 2dedfe187a19 skipped: already exists
Copying blob b4aa04aa577f skipped: already exists
Copying blob 36e6687b1ede skipped: already exists
Copying blob fffa1ce2a781 skipped: already exists
Copying blob 0b2215fb0760 skipped: already exists
Copying blob 3cc147f35962 done   |
Copying blob ffdf8cfdbafe skipped: already exists
Copying blob 1e45aa110bce skipped: already exists
Copying blob 2bc97598cf09 skipped: already exists
Copying config a389e107f4 done   |
Writing manifest to image destination
[root@localhost ~]# podman tag registry.k8s.io/kube-proxy:v1.32.0 192.168.95.71:9443/k8s/kube-proxy:v1.32.0
[root@localhost ~]# podman push 192.168.95.71:9443/k8s/kube-proxy:v1.32.0
Getting image source signatures
Copying blob f3d4d2f2afaf done   |
Copying blob aeda145147aa done   |
Copying config 040f9f8aac done   |
Writing manifest to image destination
[root@localhost ~]# podman images
REPOSITORY                                      TAG         IMAGE ID      CREATED       SIZE
192.168.95.71:9443/k8s/kube-apiserver           v1.32.0     c2e17b8d0f4a  7 months ago  98.1 MB
registry.k8s.io/kube-apiserver                  v1.32.0     c2e17b8d0f4a  7 months ago  98.1 MB
192.168.95.71:9443/k8s/kube-controller-manager  v1.32.0     8cab3d2a8bd0  7 months ago  90.8 MB
registry.k8s.io/kube-controller-manager         v1.32.0     8cab3d2a8bd0  7 months ago  90.8 MB
192.168.95.71:9443/k8s/kube-scheduler           v1.32.0     a389e107f4ff  7 months ago  70.6 MB
registry.k8s.io/kube-scheduler                  v1.32.0     a389e107f4ff  7 months ago  70.6 MB
192.168.95.71:9443/k8s/kube-proxy               v1.32.0     040f9f8aac8c  7 months ago  95.3 MB
registry.k8s.io/kube-proxy                      v1.32.0     040f9f8aac8c  7 months ago  95.3 MB
registry.k8s.io/etcd                            3.5.9-0     73deb9a3f702  2 years ago   295 MB
registry.k8s.io/coredns/coredns                 v1.10.1     ead0a4a53df8  2 years ago   53.6 MB
registry.k8s.io/pause                           3.9         e6f181688397  2 years ago   750 kB
[root@localhost ~]# podman images | grep reg
registry.k8s.io/kube-apiserver                  v1.32.0     c2e17b8d0f4a  7 months ago  98.1 MB
registry.k8s.io/kube-controller-manager         v1.32.0     8cab3d2a8bd0  7 months ago  90.8 MB
registry.k8s.io/kube-scheduler                  v1.32.0     a389e107f4ff  7 months ago  70.6 MB
registry.k8s.io/kube-proxy                      v1.32.0     040f9f8aac8c  7 months ago  95.3 MB
registry.k8s.io/etcd                            3.5.9-0     73deb9a3f702  2 years ago   295 MB
registry.k8s.io/coredns/coredns                 v1.10.1     ead0a4a53df8  2 years ago   53.6 MB
registry.k8s.io/pause                           3.9         e6f181688397  2 years ago   750 kB
[root@localhost ~]# podman tag registry.k8s.io/etcd:3.5.9-0 192.168.95.71:9443/k8s/etcd:3.5.9-0
[root@localhost ~]# podman push 192.168.95.71:9443/k8s/etcd:3.5.9-0
Getting image source signatures
Copying blob e624a5370eca done   |
Copying blob ff5700ec5418 done   |
Copying blob e023e0e48e6e done   |
Copying blob d52f02c6501c done   |
Copying blob 7bea6b893187 done   |
Copying blob 6fbdf253bbc2 done   |
Copying blob d2d7ec0f6756 done   |
Copying blob 4cb10dd2545b done   |
Copying blob b4aa04aa577f skipped: already exists
Copying blob ba9afb2b3e0c done   |
Copying blob 22bba3da6b0d done   |
Copying blob a4563151d59b done   |
Copying config 73deb9a3f7 done   |
Writing manifest to image destination
[root@localhost ~]# podman tag registry.k8s.io/coredns/coredns:v1.10.1 192.168.95.71:9443/k8s/coredns/coredns:v1.10.1
[root@localhost ~]# podman push 192.168.95.71:9443/k8s/coredns/coredns:v1.10.1
Getting image source signatures
Copying blob 398c9baff0ce done   |
Copying blob 6a4a177e62f3 done   |
Copying config ead0a4a53d done   |
Writing manifest to image destination
[root@localhost ~]# podman tag registry.k8s.io/pause:3.9 192.168.95.71:9443/k8s/pause:3.9
[root@localhost ~]# podman push 192.168.95.71:9443/k8s/pause:3.9
Getting image source signatures
Copying blob e3e5579ddd43 done   |
Copying config e6f1816883 done   |
Writing manifest to image destination
[root@localhost ~]# mkdir k8s
[root@localhost ~]# ls
anaconda-ks.cfg  jfrog-artifactory-jcr-7.98.19.rpm  k8s
[root@localhost ~]# cd k8s/
[root@localhost k8s]# ls
[root@localhost k8s]# cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=0
EOF
[root@localhost k8s]# yum install --downloadonly --downloaddir=. kubeadm kubelet kubectl
Updating Subscription Management repositories.
Kubernetes                                                                                                                   1.9 kB/s | 1.4 kB     00:00
Errors during downloading metadata for repository 'kubernetes':
  - Status code: 404 for https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64/repodata/repomd.xml (IP: 142.251.222.174)
Error: Failed to download metadata for repo 'kubernetes': Cannot download repomd.xml: Cannot download repodata/repomd.xml: All mirrors were tried
[root@localhost k8s]# cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.32/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.32/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
EOF
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.32/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.32/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
[root@localhost k8s]# ll
total 0
[root@localhost k8s]# cd /etc/yum.repos.d/
[root@localhost yum.repos.d]# ls
kubernetes.repo  pgdg-redhat-all.repo  redhat.repo
[root@localhost yum.repos.d]# yum repo list
Updating Subscription Management repositories.
No such command: repo. Please use /usr/bin/yum --help
It could be a YUM plugin command, try: "yum install 'dnf-command(repo)'"
[root@localhost yum.repos.d]# yum repolist
Updating Subscription Management repositories.
repo id                                                         repo name
kubernetes                                                      Kubernetes
pgdg-common                                                     PostgreSQL common RPMs for RHEL / Rocky / AlmaLinux 9 - x86_64
pgdg13                                                          PostgreSQL 13 for RHEL / Rocky / AlmaLinux 9 - x86_64
pgdg14                                                          PostgreSQL 14 for RHEL / Rocky / AlmaLinux 9 - x86_64
pgdg15                                                          PostgreSQL 15 for RHEL / Rocky / AlmaLinux 9 - x86_64
pgdg16                                                          PostgreSQL 16 for RHEL / Rocky / AlmaLinux 9 - x86_64
pgdg17                                                          PostgreSQL 17 for RHEL / Rocky / AlmaLinux 9 - x86_64
rhel-9-for-x86_64-appstream-rpms                                Red Hat Enterprise Linux 9 for x86_64 - AppStream (RPMs)
rhel-9-for-x86_64-baseos-rpms                                   Red Hat Enterprise Linux 9 for x86_64 - BaseOS (RPMs)
[root@localhost yum.repos.d]# cd ~/k8s
[root@localhost k8s]# ll
total 0
[root@localhost k8s]# yum install --downloadonly --downloaddir=. kubeadm kubelet kubectl
Updating Subscription Management repositories.
Kubernetes                                                                                                                    16 kB/s |  20 kB     00:01
All matches were filtered out by exclude filtering for argument: kubeadm
All matches were filtered out by exclude filtering for argument: kubelet
All matches were filtered out by exclude filtering for argument: kubectl
Error: Unable to find a match: kubeadm kubelet kubectl
[root@localhost k8s]# yum search kubedam
Updating Subscription Management repositories.
^CKeyboardInterrupt: Terminated.
[root@localhost k8s]# yum search kubeadm
Updating Subscription Management repositories.
Last metadata expiration check: 0:00:25 ago on Thursday 31 July 2025 05:26:10 PM.
No matches found.
[root@localhost k8s]# pwd
/root/k8s
[root@localhost k8s]# vim /etc/yum.repos.d/kubernetes.repo
[root@localhost k8s]#
[root@localhost k8s]#
[root@localhost k8s]# yum search kubeadm
Updating Subscription Management repositories.
Last metadata expiration check: 0:02:41 ago on Thursday 31 July 2025 05:26:10 PM.
=============================================================== Name Exactly Matched: kubeadm ===============================================================
kubeadm.aarch64 : Command-line utility for administering a Kubernetes cluster
kubeadm.ppc64le : Command-line utility for administering a Kubernetes cluster
kubeadm.s390x : Command-line utility for administering a Kubernetes cluster
kubeadm.src : Command-line utility for administering a Kubernetes cluster
kubeadm.x86_64 : Command-line utility for administering a Kubernetes cluster
[root@localhost k8s]#
[root@localhost k8s]#
[root@localhost k8s]#
[root@localhost k8s]# pwd
/root/k8s
[root@localhost k8s]# yum install --downloadonly --downloaddir=. kubeadm kubelet kubectl
Updating Subscription Management repositories.
Kubernetes                                                                                                                   3.7 kB/s | 1.7 kB     00:00
Dependencies resolved.
=============================================================================================================================================================
 Package                                 Architecture            Version                             Repository                                         Size
=============================================================================================================================================================
Installing:
 kubeadm                                 x86_64                  1.32.7-150500.1.1                   kubernetes                                         12 M
 kubectl                                 x86_64                  1.32.7-150500.1.1                   kubernetes                                         11 M
 kubelet                                 x86_64                  1.32.7-150500.1.1                   kubernetes                                         15 M
Installing dependencies:
 conntrack-tools                         x86_64                  1.4.7-4.el9_5                       rhel-9-for-x86_64-appstream-rpms                  240 k
 cri-tools                               x86_64                  1.32.0-150500.1.1                   kubernetes                                        7.1 M
 kubernetes-cni                          x86_64                  1.6.0-150500.1.1                    kubernetes                                        8.0 M
 libnetfilter_cthelper                   x86_64                  1.0.0-22.el9                        rhel-9-for-x86_64-appstream-rpms                   26 k
 libnetfilter_cttimeout                  x86_64                  1.0.0-19.el9                        rhel-9-for-x86_64-appstream-rpms                   25 k
 libnetfilter_queue                      x86_64                  1.0.5-1.el9                         rhel-9-for-x86_64-appstream-rpms                   31 k

Transaction Summary
=============================================================================================================================================================
Install  9 Packages

Total download size: 53 M
Installed size: 289 M
YUM will only download packages for the transaction.
Is this ok [y/N]: y
Downloading Packages:
(1/9): kubeadm-1.32.7-150500.1.1.x86_64.rpm                                                                                   13 MB/s |  12 MB     00:00
(2/9): kubectl-1.32.7-150500.1.1.x86_64.rpm                                                                                  9.4 MB/s |  11 MB     00:01
(3/9): cri-tools-1.32.0-150500.1.1.x86_64.rpm                                                                                6.0 MB/s | 7.1 MB     00:01
(4/9): kubelet-1.32.7-150500.1.1.x86_64.rpm                                                                                   22 MB/s |  15 MB     00:00
(5/9): libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm                                                                          45 kB/s |  26 kB     00:00
(6/9): libnetfilter_queue-1.0.5-1.el9.x86_64.rpm                                                                              98 kB/s |  31 kB     00:00
(7/9): libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm                                                                         46 kB/s |  25 kB     00:00
(8/9): conntrack-tools-1.4.7-4.el9_5.x86_64.rpm                                                                              609 kB/s | 240 kB     00:00
(9/9): kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm                                                                            4.2 MB/s | 8.0 MB     00:01
-------------------------------------------------------------------------------------------------------------------------------------------------------------
Total                                                                                                                         17 MB/s |  53 MB     00:03
Kubernetes                                                                                                                   3.9 kB/s | 1.7 kB     00:00
Importing GPG key 0x9A296436:
 Userid     : "isv:kubernetes OBS Project <isv:kubernetes@build.opensuse.org>"
 Fingerprint: DE15 B144 86CD 377B 9E87 6E1A 2346 54DA 9A29 6436
 From       : https://pkgs.k8s.io/core:/stable:/v1.32/rpm/repodata/repomd.xml.key
Is this ok [y/N]: y
Key imported successfully
Complete!
The downloaded packages were saved in cache until the next successful transaction.
You can remove cached packages by executing 'yum clean packages'.
[root@localhost k8s]# ls
1                                         kubectl-1.32.7-150500.1.1.x86_64.rpm           libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm  kubelet-1.32.7-150500.1.1.x86_64.rpm           libnetfilter_queue-1.0.5-1.el9.x86_64.rpm
cri-tools-1.32.0-150500.1.1.x86_64.rpm    kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm
kubeadm-1.32.7-150500.1.1.x86_64.rpm      libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm
[root@localhost k8s]# yum install --downloadonly --downloaddir=. kubeadm kubelet kubectlyum install --downloadonly --downloaddir=. containerd^C
[root@localhost k8s]# ^C
[root@localhost k8s]#
[root@localhost k8s]#
[root@localhost k8s]#
[root@localhost k8s]# yum install --downloadonly --downloaddir=. containerd
Updating Subscription Management repositories.
Last metadata expiration check: 0:00:53 ago on Thursday 31 July 2025 05:29:56 PM.
No match for argument: containerd
Error: Unable to find a match: containerd
[root@localhost k8s]# cat <<EOF > /etc/yum.repos.d/kubernetes1.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=0
EOF
[root@localhost k8s]# cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=0
EOF^C
[root@localhost k8s]# yum install --downloadonly --downloaddir=. containerd
Updating Subscription Management repositories.
Repository kubernetes is listed more than once in the configuration
Last metadata expiration check: 0:01:40 ago on Thursday 31 July 2025 05:29:56 PM.
No match for argument: containerd
Error: Unable to find a match: containerd
[root@localhost k8s]# yum search containerd
Updating Subscription Management repositories.
Repository kubernetes is listed more than once in the configuration
Last metadata expiration check: 0:02:04 ago on Thursday 31 July 2025 05:29:56 PM.
No matches found.
[root@localhost k8s]# sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
Updating Subscription Management repositories.
Repository kubernetes is listed more than once in the configuration
Adding repo from: https://download.docker.com/linux/centos/docker-ce.repo
[root@localhost k8s]# cat <<EOF > /etc/yum.repos.d/kubernetes1.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=0
EOF
^C
[root@localhost k8s]# yum search containerd
Updating Subscription Management repositories.
Repository kubernetes is listed more than once in the configuration
Docker CE Stable - x86_64                                                                                                    290 kB/s |  78 kB     00:00
Last metadata expiration check: 0:00:01 ago on Thursday 31 July 2025 05:38:24 PM.
================================================================= Name Matched: containerd ==================================================================
containerd.io.x86_64 : An industry-standard container runtime
[root@localhost k8s]# cat <<EOF > /etc/yum.repos.d/kubernetes1.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=0
EOF
^C
[root@localhost k8s]# yum info containerd
Updating Subscription Management repositories.
Repository kubernetes is listed more than once in the configuration
Last metadata expiration check: 0:00:16 ago on Thursday 31 July 2025 05:38:24 PM.
Error: No matching Packages to list
[root@localhost k8s]# yum info containerd.io.x86_64
Updating Subscription Management repositories.
Repository kubernetes is listed more than once in the configuration
Last metadata expiration check: 0:00:35 ago on Thursday 31 July 2025 05:38:24 PM.
Available Packages
Name         : containerd.io
Version      : 1.7.27
Release      : 3.1.el9
Architecture : x86_64
Size         : 44 M
Source       : containerd.io-1.7.27-3.1.el9.src.rpm
Repository   : docker-ce-stable
Summary      : An industry-standard container runtime
URL          : https://containerd.io
License      : Apache-2.0
Description  : containerd is an industry-standard container runtime with an emphasis on
             : simplicity, robustness and portability. It is available as a daemon for Linux
             : and Windows, which can manage the complete container lifecycle of its host
             : system: image transfer and storage, container execution and supervision,
             : low-level storage and network attachments, etc.

[root@localhost k8s]#
[root@localhost k8s]#
[root@localhost k8s]#
[root@localhost k8s]# yum install --downloadonly --downloaddir=. containerd
Updating Subscription Management repositories.
Repository kubernetes is listed more than once in the configuration
Last metadata expiration check: 0:00:50 ago on Thursday 31 July 2025 05:38:24 PM.
Dependencies resolved.
=============================================================================================================================================================
 Package                               Architecture                   Version                                 Repository                                Size
=============================================================================================================================================================
Installing:
 containerd.io                         x86_64                         1.7.27-3.1.el9                          docker-ce-stable                          44 M

Transaction Summary
=============================================================================================================================================================
Install  1 Package

Total download size: 44 M
Installed size: 155 M
YUM will only download packages for the transaction.
Is this ok [y/N]: y
Downloading Packages:
containerd.io-1.7.27-3.1.el9.x86_64.rpm                                                                                       46 MB/s |  44 MB     00:00
-------------------------------------------------------------------------------------------------------------------------------------------------------------
Total                                                                                                                         46 MB/s |  44 MB     00:00
Docker CE Stable - x86_64                                                                                                     14 kB/s | 1.6 kB     00:00
Importing GPG key 0x621E9F35:
 Userid     : "Docker Release (CE rpm) <docker@docker.com>"
 Fingerprint: 060A 61C5 1B55 8A7F 742B 77AA C52F EB6B 621E 9F35
 From       : https://download.docker.com/linux/centos/gpg
Is this ok [y/N]: y
Key imported successfully
Complete!
The downloaded packages were saved in cache until the next successful transaction.
You can remove cached packages by executing 'yum clean packages'.
[root@localhost k8s]# yum install --downloadonly --downloaddir=. haproxy
Updating Subscription Management repositories.
Repository kubernetes is listed more than once in the configuration
Last metadata expiration check: 0:01:21 ago on Thursday 31 July 2025 05:38:24 PM.
Dependencies resolved.
=============================================================================================================================================================
 Package                       Architecture                 Version                             Repository                                              Size
=============================================================================================================================================================
Installing:
 haproxy                       x86_64                       2.4.22-4.el9                        rhel-9-for-x86_64-appstream-rpms                       2.2 M

Transaction Summary
=============================================================================================================================================================
Install  1 Package

Total download size: 2.2 M
Installed size: 6.6 M
YUM will only download packages for the transaction.
Is this ok [y/N]: y
Downloading Packages:
haproxy-2.4.22-4.el9.x86_64.rpm                                                                                              3.0 MB/s | 2.2 MB     00:00
-------------------------------------------------------------------------------------------------------------------------------------------------------------
Total                                                                                                                        3.0 MB/s | 2.2 MB     00:00
Complete!
The downloaded packages were saved in cache until the next successful transaction.
You can remove cached packages by executing 'yum clean packages'.
[root@localhost k8s]# ls
1                                         haproxy-2.4.22-4.el9.x86_64.rpm       kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm  kubeadm-1.32.7-150500.1.1.x86_64.rpm  libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm
containerd.io-1.7.27-3.1.el9.x86_64.rpm   kubectl-1.32.7-150500.1.1.x86_64.rpm  libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm
cri-tools-1.32.0-150500.1.1.x86_64.rpm    kubelet-1.32.7-150500.1.1.x86_64.rpm  libnetfilter_queue-1.0.5-1.el9.x86_64.rpm
[root@localhost k8s]# cd ..
[root@localhost ~]# ls
anaconda-ks.cfg  jfrog-artifactory-jcr-7.98.19.rpm  k8s
[root@localhost ~]# cd /etc/containers/certs.d/
[root@localhost certs.d]# ls
192.168.95.71:9443
[root@localhost certs.d]# scp -r 192.168.95.7
192.168.95.70:        192.168.95.71\:9443/
[root@localhost certs.d]# scp -r 192.168.95.7
192.168.95.70:        192.168.95.71\:9443/
[root@localhost certs.d]# scp -r 192.168.95.7:^C

cp: missing destination file operand after '192.168.95.7:'
Try 'cp --help' for more information.
[root@localhost certs.d]#
[root@localhost certs.d]#
[root@localhost certs.d]# scp -r 192.168.95.71\:9443/ 192.168.95.69:/etc/containers/certs.d/
The authenticity of host '192.168.95.69 (192.168.95.69)' can't be established.
ED25519 key fingerprint is SHA256:UMisHyl9B2jg334LzycpJp3DJtnBXbUecdxQaO4qDdQ.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '192.168.95.69' (ED25519) to the list of known hosts.
root@192.168.95.69's password:
The authenticity of host '192.168.95.71 (192.168.95.71)' can't be established.
ED25519 key fingerprint is SHA256:b26YoekEpTFWNdTp6PNtAI14eUMa+XrDICmbeizT4ZQ.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '192.168.95.71' (ED25519) to the list of known hosts.
root@192.168.95.71's password:
File "9443" not found.
Failed to download file '9443'
[root@localhost certs.d]# ls
192.168.95.71:9443
[root@localhost certs.d]# scp -r 192.168.95.71\:9443/ root@192.168.95.69:/etc/containers/certs.d/
root@192.168.95.69's password:
root@192.168.95.71's password:
File "9443" not found.
Failed to download file '9443'
[root@localhost certs.d]# scp -r 192.168.95.71:9443 root@192.168.95.69:/etc/containers/certs.d/
root@192.168.95.69's password:
root@192.168.95.71's password:
File "9443" not found.
Failed to download file '9443'
[root@localhost certs.d]# scp -rp 192.168.95.71:9443 root@192.168.95.69:/etc/containers/certs.d/
root@192.168.95.69's password:
root@192.168.95.71's password:
File "9443" not found.
Failed to download file '9443'
[root@localhost certs.d]# scp -rp 192.168.95.71 root@192.168.95.69:/etc/containers/certs.d/
192.168.95.71:        192.168.95.71\:9443/
[root@localhost certs.d]# scp -rp 192.168.95.71\:9443/ root@192.168.95.69:/etc/containers/certs.d/
root@192.168.95.69's password:
root@192.168.95.71's password:
File "9443" not found.
Failed to download file '9443'
[root@localhost certs.d]# ls -ltrh
total 0
drwxr-xr-x. 2 root root 20 Jul 31 16:52 192.168.95.71:9443
[root@localhost certs.d]# pwd
/etc/containers/certs.d
[root@localhost certs.d]# scp -rp '192.168.95.71\:9443/' root@192.168.95.69:/etc/containers/certs.d/
root@192.168.95.69's password:
Permission denied, please try again.
root@192.168.95.69's password:
hostname contains invalid characters
Connection closed
[root@localhost certs.d]# scp -rp * root@192.168.95.69:/etc/containers/certs.d/
root@192.168.95.69's password:
root@192.168.95.71's password:
File "9443" not found.
Failed to download file '9443'
[root@localhost certs.d]# scp -rp "./192.168.95.71:9443" root@192.168.95.69:/etc/containers/certs.d/
root@192.168.95.69's password:
ca.crt                                                                                                                     100% 1359   140.6KB/s   00:00
[root@localhost certs.d]# scp -rp "./192.168.95.71:9443" root@192.168.95.70:/etc/containers/certs.d/
root@192.168.95.70's password:
ca.crt                                                                                                                     100% 1359     3.2MB/s   00:00
[root@localhost certs.d]# scp -rp "./192.168.95.71:9443" root@192.168.95.72:/etc/containers/certs.d/
The authenticity of host '192.168.95.72 (192.168.95.72)' can't be established.
ED25519 key fingerprint is SHA256:K+H3BFj7PV9Ewe6vouLMCuTzm3JltGbOemEQfBWWvL4.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '192.168.95.72' (ED25519) to the list of known hosts.
root@192.168.95.72's password:
ca.crt                                                                                                                     100% 1359     2.3MB/s   00:00
[root@localhost certs.d]# scp -rp "./192.168.95.71:9443" root@192.168.95.73:/etc/containers/certs.d/
The authenticity of host '192.168.95.73 (192.168.95.73)' can't be established.
ED25519 key fingerprint is SHA256:6AmRdL55F740oBG5S4K+WFjQz9yt6Fhg9Oq9Hj7DZ+k.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '192.168.95.73' (ED25519) to the list of known hosts.
root@192.168.95.73's password:
ca.crt                                                                                                                     100% 1359     2.3MB/s   00:00
[root@localhost certs.d]# scp -rp "./192.168.95.71:9443" root@192.168.95.74:/etc/containers/certs.d/
ssh: connect to host 192.168.95.74 port 22: No route to host
Connection closed
[root@localhost certs.d]# cd
[root@localhost ~]# cd k8s/
[root@localhost k8s]# ls
1                                         haproxy-2.4.22-4.el9.x86_64.rpm       kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm  kubeadm-1.32.7-150500.1.1.x86_64.rpm  libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm
containerd.io-1.7.27-3.1.el9.x86_64.rpm   kubectl-1.32.7-150500.1.1.x86_64.rpm  libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm
cri-tools-1.32.0-150500.1.1.x86_64.rpm    kubelet-1.32.7-150500.1.1.x86_64.rpm  libnetfilter_queue-1.0.5-1.el9.x86_64.rpm
[root@localhost k8s]# scp *.rpm root@192.168.95.74:~/k8s/
ssh: connect to host 192.168.95.74 port 22: No route to host
Connection closed
[root@localhost k8s]# scp *.rpm root@192.168.95.73:~/k8s/
root@192.168.95.73's password:
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm                                                                                   100%  240KB  20.2MB/s   00:00
containerd.io-1.7.27-3.1.el9.x86_64.rpm                                                                                    100%   44MB 155.4MB/s   00:00
cri-tools-1.32.0-150500.1.1.x86_64.rpm                                                                                     100% 7291KB 138.1MB/s   00:00
haproxy-2.4.22-4.el9.x86_64.rpm                                                                                            100% 2239KB 166.4MB/s   00:00
kubeadm-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   12MB 137.9MB/s   00:00
kubectl-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   11MB 174.5MB/s   00:00
kubelet-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   15MB 179.9MB/s   00:00
kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm                                                                                 100% 8216KB 108.8MB/s   00:00
libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm                                                                              100%   26KB  27.6MB/s   00:00
libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm                                                                             100%   25KB  28.4MB/s   00:00
libnetfilter_queue-1.0.5-1.el9.x86_64.rpm                                                                                  100%   31KB   8.8MB/s   00:00
[root@localhost k8s]# scp *.rpm root@192.168.95.72:~/k8s/
root@192.168.95.72's password:
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm                                                                                   100%  240KB  55.8MB/s   00:00
containerd.io-1.7.27-3.1.el9.x86_64.rpm                                                                                    100%   44MB 175.9MB/s   00:00
cri-tools-1.32.0-150500.1.1.x86_64.rpm                                                                                     100% 7291KB 175.9MB/s   00:00
haproxy-2.4.22-4.el9.x86_64.rpm                                                                                            100% 2239KB 159.9MB/s   00:00
kubeadm-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   12MB 161.0MB/s   00:00
kubectl-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   11MB 164.6MB/s   00:00
kubelet-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   15MB 159.1MB/s   00:00
kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm                                                                                 100% 8216KB 161.7MB/s   00:00
libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm                                                                              100%   26KB  53.0MB/s   00:00
libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm                                                                             100%   25KB  52.4MB/s   00:00
libnetfilter_queue-1.0.5-1.el9.x86_64.rpm                                                                                  100%   31KB  66.9MB/s   00:00
[root@localhost k8s]# scp *.rpm root@192.168.95.70:~/k8s/
root@192.168.95.70's password:
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm                                                                                   100%  240KB  92.4MB/s   00:00
containerd.io-1.7.27-3.1.el9.x86_64.rpm                                                                                    100%   44MB 199.5MB/s   00:00
cri-tools-1.32.0-150500.1.1.x86_64.rpm                                                                                     100% 7291KB 190.1MB/s   00:00
haproxy-2.4.22-4.el9.x86_64.rpm                                                                                            100% 2239KB 179.5MB/s   00:00
kubeadm-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   12MB 200.2MB/s   00:00
kubectl-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   11MB 221.9MB/s   00:00
kubelet-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   15MB 234.3MB/s   00:00
kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm                                                                                 100% 8216KB 211.2MB/s   00:00
libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm                                                                              100%   26KB  38.8MB/s   00:00
libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm                                                                             100%   25KB  41.2MB/s   00:00
libnetfilter_queue-1.0.5-1.el9.x86_64.rpm                                                                                  100%   31KB  46.5MB/s   00:00
[root@localhost k8s]# scp *.rpm root@192.168.95.69:~/k8s/
root@192.168.95.69's password:
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm                                                                                   100%  240KB  95.1MB/s   00:00
containerd.io-1.7.27-3.1.el9.x86_64.rpm                                                                                    100%   44MB 208.1MB/s   00:00
cri-tools-1.32.0-150500.1.1.x86_64.rpm                                                                                     100% 7291KB 193.9MB/s   00:00
haproxy-2.4.22-4.el9.x86_64.rpm                                                                                            100% 2239KB 181.4MB/s   00:00
kubeadm-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   12MB 201.2MB/s   00:00
kubectl-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   11MB 202.0MB/s   00:00
kubelet-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   15MB 198.6MB/s   00:00
kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm                                                                                 100% 8216KB 195.4MB/s   00:00
libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm                                                                              100%   26KB  46.9MB/s   00:00
libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm                                                                             100%   25KB  52.5MB/s   00:00
libnetfilter_queue-1.0.5-1.el9.x86_64.rpm                                                                                  100%   31KB  58.2MB/s   00:00
[root@localhost k8s]# ssh 192.168.95.69
root@192.168.95.69's password:
Permission denied, please try again.
root@192.168.95.69's password:
Activate the web console with: systemctl enable --now cockpit.socket

Last failed login: Thu Jul 31 17:52:25 IST 2025 from 192.168.95.71 on ssh:notty
There were 2 failed login attempts since the last successful login.
Last login: Thu Jul 31 15:10:52 2025 from 10.9.0.22
[root@internet-vm ~]# ls
anaconda-ks.cfg  java  jfrog  k8s  nginx  postgress
[root@internet-vm ~]# cd k8s/
[root@internet-vm k8s]# ls
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm  kubeadm-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm
containerd.io-1.7.27-3.1.el9.x86_64.rpm   kubectl-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm
cri-tools-1.32.0-150500.1.1.x86_64.rpm    kubelet-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_queue-1.0.5-1.el9.x86_64.rpm
haproxy-2.4.22-4.el9.x86_64.rpm           kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm
[root@internet-vm k8s]# rpm -ivh kubeadm-1.32.7-150500.1.1.x86_64.rpm
warning: kubeadm-1.32.7-150500.1.1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID 9a296436: NOKEY
error: Failed dependencies:
        cri-tools >= 1.30.0 is needed by kubeadm-1.32.7-150500.1.1.x86_64
[root@internet-vm k8s]# rpm -ihv cri-tools-1.32.0-150500.1.1.x86_64.rpm
warning: cri-tools-1.32.0-150500.1.1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID 9a296436: NOKEY
Verifying...                          ################################# [100%]
Preparing...                          ################################# [100%]
Updating / installing...
   1:cri-tools-1.32.0-150500.1.1      ################################# [100%]
[root@internet-vm k8s]# sudo yum localinstall *.rpm --disablerepo="*"
Updating Subscription Management repositories.
Package cri-tools-1.32.0-150500.1.1.x86_64 is already installed.
Dependencies resolved.
=============================================================================================================================================================
 Package                                      Architecture                 Version                                  Repository                          Size
=============================================================================================================================================================
Installing:
 conntrack-tools                              x86_64                       1.4.7-4.el9_5                            @commandline                       240 k
 containerd.io                                x86_64                       1.7.27-3.1.el9                           @commandline                        44 M
 haproxy                                      x86_64                       2.4.22-4.el9                             @commandline                       2.2 M
 kubeadm                                      x86_64                       1.32.7-150500.1.1                        @commandline                        12 M
 kubectl                                      x86_64                       1.32.7-150500.1.1                        @commandline                        11 M
 kubelet                                      x86_64                       1.32.7-150500.1.1                        @commandline                        15 M
 kubernetes-cni                               x86_64                       1.6.0-150500.1.1                         @commandline                       8.0 M
 libnetfilter_cthelper                        x86_64                       1.0.0-22.el9                             @commandline                        26 k
 libnetfilter_cttimeout                       x86_64                       1.0.0-19.el9                             @commandline                        25 k
 libnetfilter_queue                           x86_64                       1.0.5-1.el9                              @commandline                        31 k

Transaction Summary
=============================================================================================================================================================
Install  10 Packages

Total size: 92 M
Installed size: 421 M
Is this ok [y/N]: y
Downloading Packages:
Running transaction check
Transaction check succeeded.
Running transaction test
Transaction test succeeded.
Running transaction
  Preparing        :                                                                                                                                     1/1
  Installing       : libnetfilter_queue-1.0.5-1.el9.x86_64                                                                                              1/10
  Installing       : libnetfilter_cttimeout-1.0.0-19.el9.x86_64                                                                                         2/10
  Installing       : libnetfilter_cthelper-1.0.0-22.el9.x86_64                                                                                          3/10
  Installing       : conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               4/10
  Running scriptlet: conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               4/10
  Installing       : kubernetes-cni-1.6.0-150500.1.1.x86_64                                                                                             5/10
  Installing       : kubelet-1.32.7-150500.1.1.x86_64                                                                                                   6/10
  Running scriptlet: kubelet-1.32.7-150500.1.1.x86_64                                                                                                   6/10
  Installing       : kubectl-1.32.7-150500.1.1.x86_64                                                                                                   7/10
  Installing       : kubeadm-1.32.7-150500.1.1.x86_64                                                                                                   8/10
  Running scriptlet: haproxy-2.4.22-4.el9.x86_64                                                                                                        9/10
  Installing       : haproxy-2.4.22-4.el9.x86_64                                                                                                        9/10
  Running scriptlet: haproxy-2.4.22-4.el9.x86_64                                                                                                        9/10
  Installing       : containerd.io-1.7.27-3.1.el9.x86_64                                                                                               10/10
  Running scriptlet: containerd.io-1.7.27-3.1.el9.x86_64                                                                                               10/10
  Verifying        : conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               1/10
  Verifying        : containerd.io-1.7.27-3.1.el9.x86_64                                                                                                2/10
  Verifying        : haproxy-2.4.22-4.el9.x86_64                                                                                                        3/10
  Verifying        : kubeadm-1.32.7-150500.1.1.x86_64                                                                                                   4/10
  Verifying        : kubectl-1.32.7-150500.1.1.x86_64                                                                                                   5/10
  Verifying        : kubelet-1.32.7-150500.1.1.x86_64                                                                                                   6/10
  Verifying        : kubernetes-cni-1.6.0-150500.1.1.x86_64                                                                                             7/10
  Verifying        : libnetfilter_cthelper-1.0.0-22.el9.x86_64                                                                                          8/10
  Verifying        : libnetfilter_cttimeout-1.0.0-19.el9.x86_64                                                                                         9/10
  Verifying        : libnetfilter_queue-1.0.5-1.el9.x86_64                                                                                             10/10
Installed products updated.

Installed:
  conntrack-tools-1.4.7-4.el9_5.x86_64             containerd.io-1.7.27-3.1.el9.x86_64                 haproxy-2.4.22-4.el9.x86_64
  kubeadm-1.32.7-150500.1.1.x86_64                 kubectl-1.32.7-150500.1.1.x86_64                    kubelet-1.32.7-150500.1.1.x86_64
  kubernetes-cni-1.6.0-150500.1.1.x86_64           libnetfilter_cthelper-1.0.0-22.el9.x86_64           libnetfilter_cttimeout-1.0.0-19.el9.x86_64
  libnetfilter_queue-1.0.5-1.el9.x86_64

Complete!
[root@internet-vm k8s]# rpm -qa | grep kube
kubernetes-cni-1.6.0-150500.1.1.x86_64
kubelet-1.32.7-150500.1.1.x86_64
kubectl-1.32.7-150500.1.1.x86_64
kubeadm-1.32.7-150500.1.1.x86_64
[root@internet-vm k8s]# rpm -qa | grep conta
container-selinux-2.237.0-1.el9_6.noarch
containernetworking-plugins-1.6.2-2.el9_6.x86_64
containers-common-1-117.el9_6.x86_64
containers-common-extra-1-117.el9_6.x86_64
containerd.io-1.7.27-3.1.el9.x86_64
[root@internet-vm k8s]# exit
logout
Connection to 192.168.95.69 closed.
[root@localhost k8s]# ssh 192.168.95.70
root@192.168.95.70's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 16:42:55 2025 from 10.9.0.30
[root@JCR-vm ~]# cd k8s/
[root@JCR-vm k8s]# ls
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm  kubeadm-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm
containerd.io-1.7.27-3.1.el9.x86_64.rpm   kubectl-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm
cri-tools-1.32.0-150500.1.1.x86_64.rpm    kubelet-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_queue-1.0.5-1.el9.x86_64.rpm
haproxy-2.4.22-4.el9.x86_64.rpm           kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm
[root@JCR-vm k8s]# IP R
bash: IP: command not found...
Similar command is: 'ip'
[root@JCR-vm k8s]# ipr
bash: ipr: command not found...
[root@JCR-vm k8s]# ^C
[root@JCR-vm k8s]# ip r
default via 192.168.10.1 dev ens192
192.168.0.0/16 dev ens192 proto kernel scope link src 192.168.95.70 metric 100
[root@JCR-vm k8s]#  sudo yum localinstall *.rpm --disablerepo="*"
Updating Subscription Management repositories.
Dependencies resolved.
=============================================================================================================================================================
 Package                                      Architecture                 Version                                  Repository                          Size
=============================================================================================================================================================
Installing:
 conntrack-tools                              x86_64                       1.4.7-4.el9_5                            @commandline                       240 k
 containerd.io                                x86_64                       1.7.27-3.1.el9                           @commandline                        44 M
 cri-tools                                    x86_64                       1.32.0-150500.1.1                        @commandline                       7.1 M
 haproxy                                      x86_64                       2.4.22-4.el9                             @commandline                       2.2 M
 kubeadm                                      x86_64                       1.32.7-150500.1.1                        @commandline                        12 M
 kubectl                                      x86_64                       1.32.7-150500.1.1                        @commandline                        11 M
 kubelet                                      x86_64                       1.32.7-150500.1.1                        @commandline                        15 M
 kubernetes-cni                               x86_64                       1.6.0-150500.1.1                         @commandline                       8.0 M
 libnetfilter_cthelper                        x86_64                       1.0.0-22.el9                             @commandline                        26 k
 libnetfilter_cttimeout                       x86_64                       1.0.0-19.el9                             @commandline                        25 k
 libnetfilter_queue                           x86_64                       1.0.5-1.el9                              @commandline                        31 k

Transaction Summary
=============================================================================================================================================================
Install  11 Packages

Total size: 99 M
Installed size: 451 M
Is this ok [y/N]: y
Downloading Packages:
Running transaction check
Transaction check succeeded.
Running transaction test
Transaction test succeeded.
Running transaction
  Preparing        :                                                                                                                                     1/1
  Installing       : libnetfilter_queue-1.0.5-1.el9.x86_64                                                                                              1/11
  Installing       : libnetfilter_cttimeout-1.0.0-19.el9.x86_64                                                                                         2/11
  Installing       : libnetfilter_cthelper-1.0.0-22.el9.x86_64                                                                                          3/11
  Installing       : conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               4/11
  Running scriptlet: conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               4/11
  Installing       : kubernetes-cni-1.6.0-150500.1.1.x86_64                                                                                             5/11
  Installing       : cri-tools-1.32.0-150500.1.1.x86_64                                                                                                 6/11
  Installing       : kubeadm-1.32.7-150500.1.1.x86_64                                                                                                   7/11
  Installing       : kubelet-1.32.7-150500.1.1.x86_64                                                                                                   8/11
  Running scriptlet: kubelet-1.32.7-150500.1.1.x86_64                                                                                                   8/11
  Installing       : kubectl-1.32.7-150500.1.1.x86_64                                                                                                   9/11
  Running scriptlet: haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Installing       : haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Running scriptlet: haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Installing       : containerd.io-1.7.27-3.1.el9.x86_64                                                                                               11/11
  Running scriptlet: containerd.io-1.7.27-3.1.el9.x86_64                                                                                               11/11
  Verifying        : conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               1/11
  Verifying        : containerd.io-1.7.27-3.1.el9.x86_64                                                                                                2/11
  Verifying        : cri-tools-1.32.0-150500.1.1.x86_64                                                                                                 3/11
  Verifying        : haproxy-2.4.22-4.el9.x86_64                                                                                                        4/11
  Verifying        : kubeadm-1.32.7-150500.1.1.x86_64                                                                                                   5/11
  Verifying        : kubectl-1.32.7-150500.1.1.x86_64                                                                                                   6/11
  Verifying        : kubelet-1.32.7-150500.1.1.x86_64                                                                                                   7/11
  Verifying        : kubernetes-cni-1.6.0-150500.1.1.x86_64                                                                                             8/11
  Verifying        : libnetfilter_cthelper-1.0.0-22.el9.x86_64                                                                                          9/11
  Verifying        : libnetfilter_cttimeout-1.0.0-19.el9.x86_64                                                                                        10/11
  Verifying        : libnetfilter_queue-1.0.5-1.el9.x86_64                                                                                             11/11
Installed products updated.

Installed:
  conntrack-tools-1.4.7-4.el9_5.x86_64                 containerd.io-1.7.27-3.1.el9.x86_64              cri-tools-1.32.0-150500.1.1.x86_64
  haproxy-2.4.22-4.el9.x86_64                          kubeadm-1.32.7-150500.1.1.x86_64                 kubectl-1.32.7-150500.1.1.x86_64
  kubelet-1.32.7-150500.1.1.x86_64                     kubernetes-cni-1.6.0-150500.1.1.x86_64           libnetfilter_cthelper-1.0.0-22.el9.x86_64
  libnetfilter_cttimeout-1.0.0-19.el9.x86_64           libnetfilter_queue-1.0.5-1.el9.x86_64

Complete!
[root@JCR-vm k8s]# exit
logout
Connection to 192.168.95.70 closed.
[root@localhost k8s]# ssh 192.168.95.72
root@192.168.95.72's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
[root@localhost ~]# ls
anaconda-ks.cfg  k8s
[root@localhost ~]# cd k8s/
[root@localhost k8s]# ls
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm  kubeadm-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm
containerd.io-1.7.27-3.1.el9.x86_64.rpm   kubectl-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm
cri-tools-1.32.0-150500.1.1.x86_64.rpm    kubelet-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_queue-1.0.5-1.el9.x86_64.rpm
haproxy-2.4.22-4.el9.x86_64.rpm           kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm
[root@localhost k8s]#  sudo yum localinstall *.rpm --disablerepo="*"
Updating Subscription Management repositories.
Unable to read consumer identity

This system is not registered with an entitlement server. You can use "rhc" or "subscription-manager" to register.

No repository match: *
Dependencies resolved.
=============================================================================================================================================================
 Package                                      Architecture                 Version                                  Repository                          Size
=============================================================================================================================================================
Installing:
 conntrack-tools                              x86_64                       1.4.7-4.el9_5                            @commandline                       240 k
 containerd.io                                x86_64                       1.7.27-3.1.el9                           @commandline                        44 M
 cri-tools                                    x86_64                       1.32.0-150500.1.1                        @commandline                       7.1 M
 haproxy                                      x86_64                       2.4.22-4.el9                             @commandline                       2.2 M
 kubeadm                                      x86_64                       1.32.7-150500.1.1                        @commandline                        12 M
 kubectl                                      x86_64                       1.32.7-150500.1.1                        @commandline                        11 M
 kubelet                                      x86_64                       1.32.7-150500.1.1                        @commandline                        15 M
 kubernetes-cni                               x86_64                       1.6.0-150500.1.1                         @commandline                       8.0 M
 libnetfilter_cthelper                        x86_64                       1.0.0-22.el9                             @commandline                        26 k
 libnetfilter_cttimeout                       x86_64                       1.0.0-19.el9                             @commandline                        25 k
 libnetfilter_queue                           x86_64                       1.0.5-1.el9                              @commandline                        31 k

Transaction Summary
=============================================================================================================================================================
Install  11 Packages

Total size: 99 M
Installed size: 451 M
Is this ok [y/N]: yes
Downloading Packages:
Running transaction check
Transaction check succeeded.
Running transaction test
Transaction test succeeded.
Running transaction
  Preparing        :                                                                                                                                     1/1
  Installing       : libnetfilter_queue-1.0.5-1.el9.x86_64                                                                                              1/11
  Installing       : libnetfilter_cttimeout-1.0.0-19.el9.x86_64                                                                                         2/11
  Installing       : libnetfilter_cthelper-1.0.0-22.el9.x86_64                                                                                          3/11
  Installing       : conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               4/11
  Running scriptlet: conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               4/11
  Installing       : kubernetes-cni-1.6.0-150500.1.1.x86_64                                                                                             5/11
  Installing       : cri-tools-1.32.0-150500.1.1.x86_64                                                                                                 6/11
  Installing       : kubeadm-1.32.7-150500.1.1.x86_64                                                                                                   7/11
  Installing       : kubelet-1.32.7-150500.1.1.x86_64                                                                                                   8/11
  Running scriptlet: kubelet-1.32.7-150500.1.1.x86_64                                                                                                   8/11
  Installing       : kubectl-1.32.7-150500.1.1.x86_64                                                                                                   9/11
  Running scriptlet: haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Installing       : haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Running scriptlet: haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Installing       : containerd.io-1.7.27-3.1.el9.x86_64                                                                                               11/11
  Running scriptlet: containerd.io-1.7.27-3.1.el9.x86_64                                                                                               11/11
  Verifying        : conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               1/11
  Verifying        : containerd.io-1.7.27-3.1.el9.x86_64                                                                                                2/11
  Verifying        : cri-tools-1.32.0-150500.1.1.x86_64                                                                                                 3/11
  Verifying        : haproxy-2.4.22-4.el9.x86_64                                                                                                        4/11
  Verifying        : kubeadm-1.32.7-150500.1.1.x86_64                                                                                                   5/11
  Verifying        : kubectl-1.32.7-150500.1.1.x86_64                                                                                                   6/11
  Verifying        : kubelet-1.32.7-150500.1.1.x86_64                                                                                                   7/11
  Verifying        : kubernetes-cni-1.6.0-150500.1.1.x86_64                                                                                             8/11
  Verifying        : libnetfilter_cthelper-1.0.0-22.el9.x86_64                                                                                          9/11
  Verifying        : libnetfilter_cttimeout-1.0.0-19.el9.x86_64                                                                                        10/11
  Verifying        : libnetfilter_queue-1.0.5-1.el9.x86_64                                                                                             11/11
Installed products updated.

Installed:
  conntrack-tools-1.4.7-4.el9_5.x86_64                 containerd.io-1.7.27-3.1.el9.x86_64              cri-tools-1.32.0-150500.1.1.x86_64
  haproxy-2.4.22-4.el9.x86_64                          kubeadm-1.32.7-150500.1.1.x86_64                 kubectl-1.32.7-150500.1.1.x86_64
  kubelet-1.32.7-150500.1.1.x86_64                     kubernetes-cni-1.6.0-150500.1.1.x86_64           libnetfilter_cthelper-1.0.0-22.el9.x86_64
  libnetfilter_cttimeout-1.0.0-19.el9.x86_64           libnetfilter_queue-1.0.5-1.el9.x86_64

Complete!
[root@localhost k8s]# exit
logout
Connection to 192.168.95.72 closed.
[root@localhost k8s]# ssh 192.168.95.73
root@192.168.95.73's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 17:49:48 2025 from 10.9.0.30
[root@localhost ~]# cd k8s/
[root@localhost k8s]#  sudo yum localinstall *.rpm --disablerepo="*"
Updating Subscription Management repositories.
Unable to read consumer identity

This system is not registered with an entitlement server. You can use "rhc" or "subscription-manager" to register.

No repository match: *
Dependencies resolved.
=============================================================================================================================================================
 Package                                      Architecture                 Version                                  Repository                          Size
=============================================================================================================================================================
Installing:
 conntrack-tools                              x86_64                       1.4.7-4.el9_5                            @commandline                       240 k
 containerd.io                                x86_64                       1.7.27-3.1.el9                           @commandline                        44 M
 cri-tools                                    x86_64                       1.32.0-150500.1.1                        @commandline                       7.1 M
 haproxy                                      x86_64                       2.4.22-4.el9                             @commandline                       2.2 M
 kubeadm                                      x86_64                       1.32.7-150500.1.1                        @commandline                        12 M
 kubectl                                      x86_64                       1.32.7-150500.1.1                        @commandline                        11 M
 kubelet                                      x86_64                       1.32.7-150500.1.1                        @commandline                        15 M
 kubernetes-cni                               x86_64                       1.6.0-150500.1.1                         @commandline                       8.0 M
 libnetfilter_cthelper                        x86_64                       1.0.0-22.el9                             @commandline                        26 k
 libnetfilter_cttimeout                       x86_64                       1.0.0-19.el9                             @commandline                        25 k
 libnetfilter_queue                           x86_64                       1.0.5-1.el9                              @commandline                        31 k

Transaction Summary
=============================================================================================================================================================
Install  11 Packages

Total size: 99 M
Installed size: 451 M
Is this ok [y/N]: y
Downloading Packages:
Running transaction check
Transaction check succeeded.
Running transaction test
Transaction test succeeded.
Running transaction
  Preparing        :                                                                                                                                     1/1
  Installing       : libnetfilter_queue-1.0.5-1.el9.x86_64                                                                                              1/11
  Installing       : libnetfilter_cttimeout-1.0.0-19.el9.x86_64                                                                                         2/11
  Installing       : libnetfilter_cthelper-1.0.0-22.el9.x86_64                                                                                          3/11
  Installing       : conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               4/11
  Running scriptlet: conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               4/11
  Installing       : kubernetes-cni-1.6.0-150500.1.1.x86_64                                                                                             5/11
  Installing       : cri-tools-1.32.0-150500.1.1.x86_64                                                                                                 6/11
  Installing       : kubeadm-1.32.7-150500.1.1.x86_64                                                                                                   7/11
  Installing       : kubelet-1.32.7-150500.1.1.x86_64                                                                                                   8/11
  Running scriptlet: kubelet-1.32.7-150500.1.1.x86_64                                                                                                   8/11
  Installing       : kubectl-1.32.7-150500.1.1.x86_64                                                                                                   9/11
  Running scriptlet: haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Installing       : haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Running scriptlet: haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Installing       : containerd.io-1.7.27-3.1.el9.x86_64                                                                                               11/11
  Running scriptlet: containerd.io-1.7.27-3.1.el9.x86_64                                                                                               11/11
  Verifying        : conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               1/11
  Verifying        : containerd.io-1.7.27-3.1.el9.x86_64                                                                                                2/11
  Verifying        : cri-tools-1.32.0-150500.1.1.x86_64                                                                                                 3/11
  Verifying        : haproxy-2.4.22-4.el9.x86_64                                                                                                        4/11
  Verifying        : kubeadm-1.32.7-150500.1.1.x86_64                                                                                                   5/11
  Verifying        : kubectl-1.32.7-150500.1.1.x86_64                                                                                                   6/11
  Verifying        : kubelet-1.32.7-150500.1.1.x86_64                                                                                                   7/11
  Verifying        : kubernetes-cni-1.6.0-150500.1.1.x86_64                                                                                             8/11
  Verifying        : libnetfilter_cthelper-1.0.0-22.el9.x86_64                                                                                          9/11
  Verifying        : libnetfilter_cttimeout-1.0.0-19.el9.x86_64                                                                                        10/11
  Verifying        : libnetfilter_queue-1.0.5-1.el9.x86_64                                                                                             11/11
Installed products updated.

Installed:
  conntrack-tools-1.4.7-4.el9_5.x86_64                 containerd.io-1.7.27-3.1.el9.x86_64              cri-tools-1.32.0-150500.1.1.x86_64
  haproxy-2.4.22-4.el9.x86_64                          kubeadm-1.32.7-150500.1.1.x86_64                 kubectl-1.32.7-150500.1.1.x86_64
  kubelet-1.32.7-150500.1.1.x86_64                     kubernetes-cni-1.6.0-150500.1.1.x86_64           libnetfilter_cthelper-1.0.0-22.el9.x86_64
  libnetfilter_cttimeout-1.0.0-19.el9.x86_64           libnetfilter_queue-1.0.5-1.el9.x86_64

Complete!
[root@localhost k8s]# exit
logout
Connection to 192.168.95.73 closed.
[root@localhost k8s]# ip r
default via 192.168.10.1 dev ens192 proto static metric 100
192.168.0.0/16 dev ens192 proto kernel scope link src 192.168.95.71 metric 100
[root@localhost k8s]# rpm -qa | grep kube
[root@localhost k8s]#  sudo yum localinstall *.rpm --disablerepo="*"
Updating Subscription Management repositories.
Repository kubernetes is listed more than once in the configuration
Dependencies resolved.
=============================================================================================================================================================
 Package                                      Architecture                 Version                                  Repository                          Size
=============================================================================================================================================================
Installing:
 conntrack-tools                              x86_64                       1.4.7-4.el9_5                            @commandline                       240 k
 containerd.io                                x86_64                       1.7.27-3.1.el9                           @commandline                        44 M
 cri-tools                                    x86_64                       1.32.0-150500.1.1                        @commandline                       7.1 M
 haproxy                                      x86_64                       2.4.22-4.el9                             @commandline                       2.2 M
 kubeadm                                      x86_64                       1.32.7-150500.1.1                        @commandline                        12 M
 kubectl                                      x86_64                       1.32.7-150500.1.1                        @commandline                        11 M
 kubelet                                      x86_64                       1.32.7-150500.1.1                        @commandline                        15 M
 kubernetes-cni                               x86_64                       1.6.0-150500.1.1                         @commandline                       8.0 M
 libnetfilter_cthelper                        x86_64                       1.0.0-22.el9                             @commandline                        26 k
 libnetfilter_cttimeout                       x86_64                       1.0.0-19.el9                             @commandline                        25 k
 libnetfilter_queue                           x86_64                       1.0.5-1.el9                              @commandline                        31 k

Transaction Summary
=============================================================================================================================================================
Install  11 Packages

Total size: 99 M
Installed size: 451 M
Is this ok [y/N]: y
Downloading Packages:
Running transaction check
Transaction check succeeded.
Running transaction test
Transaction test succeeded.
Running transaction
  Preparing        :                                                                                                                                     1/1
  Installing       : libnetfilter_queue-1.0.5-1.el9.x86_64                                                                                              1/11
  Installing       : libnetfilter_cttimeout-1.0.0-19.el9.x86_64                                                                                         2/11
  Installing       : libnetfilter_cthelper-1.0.0-22.el9.x86_64                                                                                          3/11
  Installing       : conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               4/11
  Running scriptlet: conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               4/11
  Installing       : kubernetes-cni-1.6.0-150500.1.1.x86_64                                                                                             5/11
  Installing       : cri-tools-1.32.0-150500.1.1.x86_64                                                                                                 6/11
  Installing       : kubeadm-1.32.7-150500.1.1.x86_64                                                                                                   7/11
  Installing       : kubelet-1.32.7-150500.1.1.x86_64                                                                                                   8/11
  Running scriptlet: kubelet-1.32.7-150500.1.1.x86_64                                                                                                   8/11
  Installing       : kubectl-1.32.7-150500.1.1.x86_64                                                                                                   9/11
  Running scriptlet: haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Installing       : haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Running scriptlet: haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Installing       : containerd.io-1.7.27-3.1.el9.x86_64                                                                                               11/11
  Running scriptlet: containerd.io-1.7.27-3.1.el9.x86_64                                                                                               11/11
  Verifying        : conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               1/11
  Verifying        : containerd.io-1.7.27-3.1.el9.x86_64                                                                                                2/11
  Verifying        : cri-tools-1.32.0-150500.1.1.x86_64                                                                                                 3/11
  Verifying        : haproxy-2.4.22-4.el9.x86_64                                                                                                        4/11
  Verifying        : kubeadm-1.32.7-150500.1.1.x86_64                                                                                                   5/11
  Verifying        : kubectl-1.32.7-150500.1.1.x86_64                                                                                                   6/11
  Verifying        : kubelet-1.32.7-150500.1.1.x86_64                                                                                                   7/11
  Verifying        : kubernetes-cni-1.6.0-150500.1.1.x86_64                                                                                             8/11
  Verifying        : libnetfilter_cthelper-1.0.0-22.el9.x86_64                                                                                          9/11
  Verifying        : libnetfilter_cttimeout-1.0.0-19.el9.x86_64                                                                                        10/11
  Verifying        : libnetfilter_queue-1.0.5-1.el9.x86_64                                                                                             11/11
Installed products updated.

Installed:
  conntrack-tools-1.4.7-4.el9_5.x86_64                 containerd.io-1.7.27-3.1.el9.x86_64              cri-tools-1.32.0-150500.1.1.x86_64
  haproxy-2.4.22-4.el9.x86_64                          kubeadm-1.32.7-150500.1.1.x86_64                 kubectl-1.32.7-150500.1.1.x86_64
  kubelet-1.32.7-150500.1.1.x86_64                     kubernetes-cni-1.6.0-150500.1.1.x86_64           libnetfilter_cthelper-1.0.0-22.el9.x86_64
  libnetfilter_cttimeout-1.0.0-19.el9.x86_64           libnetfilter_queue-1.0.5-1.el9.x86_64

Complete!
[root@localhost k8s]# ls
1
[root@localhost k8s]# rpm -qa | grep kube
kubernetes-cni-1.6.0-150500.1.1.x86_64
kubeadm-1.32.7-150500.1.1.x86_64
kubelet-1.32.7-150500.1.1.x86_64
kubectl-1.32.7-150500.1.1.x86_64
[root@localhost k8s]# sudo hostnamectl set-hostname <hostname>
sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab
-bash: syntax error near unexpected token `newline'
^C
[root@localhost k8s]#
sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab
^C
[root@localhost k8s]# sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab
[root@localhost k8s]# ssh 192.168.95.69
root@192.168.95.69's password:
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Thu Jul 31 17:52:27 2025 from 192.168.95.71
[root@internet-vm ~]# sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab
[root@internet-vm ~]# ^C
[root@internet-vm ~]# exit
logout
Connection to 192.168.95.69 closed.
[root@localhost k8s]# ssh 192.168.95.70
root@192.168.95.70's password:
Permission denied, please try again.
root@192.168.95.70's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last failed login: Thu Jul 31 18:06:17 IST 2025 from 192.168.95.71 on ssh:notty
There was 1 failed login attempt since the last successful login.
Last login: Thu Jul 31 17:55:55 2025 from 192.168.95.71
[root@JCR-vm ~]# sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab
[root@JCR-vm ~]# exit
logout
Connection to 192.168.95.70 closed.
[root@localhost k8s]# ssh 192.168.95.72
root@192.168.95.72's password:
Permission denied, please try again.
root@192.168.95.72's password:
Permission denied, please try again.
root@192.168.95.72's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last failed login: Thu Jul 31 18:07:42 IST 2025 from 192.168.95.71 on ssh:notty
There were 2 failed login attempts since the last successful login.
Last login: Thu Jul 31 18:00:28 2025 from 192.168.95.71
[root@localhost ~]# sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab
[root@localhost ~]# exit
logout
Connection to 192.168.95.72 closed.
[root@localhost k8s]# ssh 192.168.95.73
root@192.168.95.73's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 18:01:29 2025 from 192.168.95.71
[root@localhost ~]# sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab
[root@localhost ~]# exit
logout
Connection to 192.168.95.73 closed.
[root@localhost k8s]# free -h
               total        used        free      shared  buff/cache   available
Mem:           3.6Gi       3.5Gi       136Mi        54Mi       237Mi       110Mi
Swap:             0B          0B          0B
[root@localhost k8s]# swap -l
bash: swap: command not found...
Similar command is: 'swapon'
[root@localhost k8s]# cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF
br_netfilter
[root@localhost k8s]# ip r
default via 192.168.10.1 dev ens192 proto static metric 100
192.168.0.0/16 dev ens192 proto kernel scope link src 192.168.95.71 metric 100
[root@localhost k8s]# cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
[root@localhost k8s]# sudo sysctl --system
* Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...
* Applying /usr/lib/sysctl.d/50-coredump.conf ...
* Applying /usr/lib/sysctl.d/50-default.conf ...
* Applying /usr/lib/sysctl.d/50-libkcapi-optmem_max.conf ...
* Applying /usr/lib/sysctl.d/50-pid-max.conf ...
* Applying /usr/lib/sysctl.d/50-redhat.conf ...
* Applying /etc/sysctl.d/99-sysctl.conf ...
* Applying /etc/sysctl.d/k8s.conf ...
* Applying /etc/sysctl.conf ...
kernel.yama.ptrace_scope = 0
kernel.core_pattern = |/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h
kernel.core_pipe_limit = 16
fs.suid_dumpable = 2
kernel.sysrq = 16
kernel.core_uses_pid = 1
net.ipv4.conf.default.rp_filter = 2
net.ipv4.conf.ens192.rp_filter = 2
net.ipv4.conf.lo.rp_filter = 2
net.ipv4.conf.default.accept_source_route = 0
net.ipv4.conf.ens192.accept_source_route = 0
net.ipv4.conf.lo.accept_source_route = 0
net.ipv4.conf.default.promote_secondaries = 1
net.ipv4.conf.ens192.promote_secondaries = 1
net.ipv4.conf.lo.promote_secondaries = 1
net.ipv4.ping_group_range = 0 2147483647
net.core.default_qdisc = fq_codel
fs.protected_hardlinks = 1
fs.protected_symlinks = 1
fs.protected_regular = 1
fs.protected_fifos = 1
net.core.optmem_max = 81920
kernel.pid_max = 4194304
kernel.kptr_restrict = 1
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.ens192.rp_filter = 1
net.ipv4.conf.lo.rp_filter = 1
net.ipv4.ip_forward = 1
[root@localhost k8s]# ssh 192.168.95.69
root@192.168.95.69's password:
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Thu Jul 31 18:05:48 2025 from 192.168.95.71
[root@internet-vm ~]# cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF
br_netfilter
[root@internet-vm ~]# cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
[root@internet-vm ~]# exit
logout
Connection to 192.168.95.69 closed.
[root@localhost k8s]# ssh 192.168.95.70
root@192.168.95.70's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 18:06:21 2025 from 192.168.95.71
[root@JCR-vm ~]# cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF
br_netfilter
[root@JCR-vm ~]#
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
[root@JCR-vm ~]# exit
logout
Connection to 192.168.95.70 closed.
[root@localhost k8s]# ssh 192.168.95.72
root@192.168.95.72's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 18:07:45 2025 from 192.168.95.71
[root@localhost ~]#
[root@localhost ~]# cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF
br_netfilter
[root@localhost ~]# cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
[root@localhost ~]# exit
logout
Connection to 192.168.95.72 closed.
[root@localhost k8s]# ssh 192.168.95.73
root@192.168.95.73's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 18:08:01 2025 from 192.168.95.71
[root@localhost ~]# cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF
br_netfilter
[root@localhost ~]# cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
[root@localhost ~]# exit
logout
Connection to 192.168.95.73 closed.
[root@localhost k8s]# cat /etc/containerd/config.toml
#   Copyright 2018-2022 Docker Inc.

#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at

#       http://www.apache.org/licenses/LICENSE-2.0

#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.

disabled_plugins = ["cri"]

#root = "/var/lib/containerd"
#state = "/run/containerd"
#subreaper = true
#oom_score = 0

#[grpc]
#  address = "/run/containerd/containerd.sock"
#  uid = 0
#  gid = 0

#[debug]
#  address = "/run/containerd/debug.sock"
#  uid = 0
#  gid = 0
#  level = "info"
[root@localhost k8s]# cat /etc/containerd/config.toml | grep cgrou
[root@localhost k8s]# cat /etc/containerd/config.toml | grep cgroup
[root@localhost k8s]# cat /etc/containerd/config.toml | grep -i C
#   Copyright 2018-2022 Docker Inc.
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#       http://www.apache.org/licenses/LICENSE-2.0
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.
disabled_plugins = ["cri"]
#root = "/var/lib/containerd"
#state = "/run/containerd"
#oom_score = 0
#[grpc]
#  address = "/run/containerd/containerd.sock"
#  address = "/run/containerd/debug.sock"
[root@localhost k8s]# cat /etc/containerd/config.toml | grep -i Cg
[root@localhost k8s]# cat /etc/containerd/config.toml | grep -i Cgroup
[root@localhost k8s]# systemctl enable --now containerd.service
Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /usr/lib/systemd/system/containerd.service.
[root@localhost k8s]# systemctl enable --now kubelet
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /usr/lib/systemd/system/kubelet.service.
[root@localhost k8s]# setenforce 0
[root@localhost k8s]# ^C
[root@localhost k8s]# ssh 192.168.95.69
root@192.168.95.69's password:
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Thu Jul 31 18:10:06 2025 from 192.168.95.71
[root@internet-vm ~]# systemctl enable --now containerd.service
Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /usr/lib/systemd/system/containerd.service.
[root@internet-vm ~]# systemctl enable --now kubelet
setenforce 0
systemctl stop firewalld
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /usr/lib/systemd/system/kubelet.service.
[root@internet-vm ~]# exit
logout
Connection to 192.168.95.69 closed.
[root@localhost k8s]# ssh 192.168.95.70
root@192.168.95.70's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 18:10:58 2025 from 192.168.95.71
[root@JCR-vm ~]# systemctl enable --now containerd.service
systemctl enable --now kubelet
setenforce 0
systemctl stop firewalld


Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /usr/lib/systemd/system/containerd.service.
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /usr/lib/systemd/system/kubelet.service.
[root@JCR-vm ~]# exit
logout
Connection to 192.168.95.70 closed.
[root@localhost k8s]# ssh 192.168.95.72
root@192.168.95.72's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 18:11:23 2025 from 192.168.95.71
[root@localhost ~]# systemctl enable --now containerd.service
systemctl enable --now kubelet
setenforce 0
systemctl stop firewalld


Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /usr/lib/systemd/system/containerd.service.
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /usr/lib/systemd/system/kubelet.service.
[root@localhost ~]# exit
logout
Connection to 192.168.95.72 closed.
[root@localhost k8s]# ssh 192.168.95.73
root@192.168.95.73's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 18:11:54 2025 from 192.168.95.71
[root@localhost ~]# systemctl enable --now containerd.service
systemctl enable --now kubelet
setenforce 0
systemctl stop firewalld


Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /usr/lib/systemd/system/containerd.service.
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /usr/lib/systemd/system/kubelet.service.
[root@localhost ~]# exit
logout
Connection to 192.168.95.73 closed.
[root@localhost k8s]# systemctl enable --now containerd.service
systemctl enable --now kubelet
setenforce 0
systemctl stop firewalld


[root@localhost k8s]# ip r
default via 192.168.10.1 dev ens192 proto static metric 100
192.168.0.0/16 dev ens192 proto kernel scope link src 192.168.95.71 metric 100
[root@localhost k8s]# ssh 192.168.95.69
root@192.168.95.69's password:
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Thu Jul 31 18:16:57 2025 from 192.168.95.71
[root@internet-vm ~]# exit
logout
Connection to 192.168.95.69 closed.
[root@localhost k8s]# ls
1
[root@localhost k8s]# ll
total 4
-rw-r--r--. 1 root root 236 Jul 31 17:28 1
[root@localhost k8s]# ip r
default via 192.168.10.1 dev ens192 proto static metric 100
192.168.0.0/16 dev ens192 proto kernel scope link src 192.168.95.71 metric 100
[root@localhost k8s]# ls
1
[root@localhost k8s]# cd ..
[root@localhost ~]# ls
anaconda-ks.cfg  jfrog-artifactory-jcr-7.98.19.rpm  k8s
[root@localhost ~]# cd k8s/
[root@localhost k8s]# ls
1
[root@localhost k8s]# ssh 192.168.95.69
root@192.168.95.69's password:
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Thu Jul 31 18:19:19 2025 from 192.168.95.71
[root@internet-vm ~]# cd k8s/
[root@internet-vm k8s]# ls
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm  kubeadm-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm
containerd.io-1.7.27-3.1.el9.x86_64.rpm   kubectl-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm
cri-tools-1.32.0-150500.1.1.x86_64.rpm    kubelet-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_queue-1.0.5-1.el9.x86_64.rpm
haproxy-2.4.22-4.el9.x86_64.rpm           kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm
[root@internet-vm k8s]# exit
logout
Connection to 192.168.95.69 closed.
[root@localhost k8s]# yum install haproxy
Updating Subscription Management repositories.
Repository kubernetes is listed more than once in the configuration
Last metadata expiration check: 0:42:19 ago on Thursday 31 July 2025 05:38:24 PM.
^C^C^C^C^CKeyboardInterrupt: Terminated.
[root@localhost k8s]#


==================





bash: l: command not found...
[postgres@localhost jfrog]$ ls
artifactory
[postgres@localhost jfrog]$ cd artifactory/
[postgres@localhost artifactory]$ ls
app  var
[postgres@localhost artifactory]$ cd var/
[postgres@localhost var]$ ls
backup  bootstrap  data  etc  log  work
[postgres@localhost var]$ cd etc/
[postgres@localhost etc]$ ls
access       filebeat.yaml  logrotate  observability  router    system.basic-template.yaml  system.yaml
artifactory  jfconnect      metadata   replicator     security  system.full-template.yaml
[postgres@localhost etc]$ vi system.yaml
[postgres@localhost etc]$ exit
logout
[root@localhost ~]# ls
anaconda-ks.cfg  jfrog-artifactory-jcr-7.98.19.rpm
[root@localhost ~]# cd /opt/jfrog/
[root@localhost jfrog]# ls
artifactory
[root@localhost jfrog]# cd artifactory/
[root@localhost artifactory]# ls
app  var
[root@localhost artifactory]# cd app/
[root@localhost app]# ls
access  artifactory  artifactory.product.version.properties  bin  doc  event  frontend  metadata  misc  observability  router  run  third-party
[root@localhost app]# cd ..
[root@localhost artifactory]# cd var/etc/
[root@localhost etc]# ls
access       filebeat.yaml  logrotate  observability  router    system.basic-template.yaml  system.yaml
artifactory  jfconnect      metadata   replicator     security  system.full-template.yaml
[root@localhost etc]# vi system.yaml
[root@localhost etc]# systemctl start artifactory.service
[root@localhost etc]# systemctl status artifactory.service
● artifactory.service - Artifactory service
     Loaded: loaded (/usr/lib/systemd/system/artifactory.service; enabled; preset: disabled)
     Active: active (running) since Thu 2025-07-31 16:25:03 IST; 6min ago
   Main PID: 52128 (java)
      Tasks: 0 (limit: 23020)
     Memory: 20.0K
        CPU: 4.386s
     CGroup: /system.slice/artifactory.service
             ‣ 52128 /opt/jfrog/artifactory/app/third-party/java/bin/java -Djava.util.logging.config.file=/opt/jfrog/artifactory/app/artifactory/tomcat/conf>

Jul 31 16:25:00 localhost.localdomain artifactoryManage.sh[46387]: /usr/bin/netstat
Jul 31 16:25:01 localhost.localdomain artifactoryManage.sh[46387]: /usr/bin/netstat
Jul 31 16:25:02 localhost.localdomain artifactoryManage.sh[46387]: /usr/bin/netstat
Jul 31 16:25:03 localhost.localdomain artifactoryManage.sh[46387]: /usr/bin/netstat
Jul 31 16:25:03 localhost.localdomain artifactoryManage.sh[46387]: 2025-07-31T10:55:03.443Z [shell] [INFO ] [] [artifactoryManage.sh:135      ] [main] - Art>
Jul 31 16:25:03 localhost.localdomain artifactoryManage.sh[46387]: 2025-07-31T10:55:03.482Z [shell] [INFO ] [] [artifactoryCommon.sh:1248     ] [main] - Art>
Jul 31 16:25:03 localhost.localdomain systemd[1]: Started Artifactory service.
Jul 31 16:27:54 localhost.localdomain systemd[1]: /usr/lib/systemd/system/artifactory.service:11: PIDFile= references a path below legacy directory /var/run>
Jul 31 16:27:55 localhost.localdomain systemd[1]: /usr/lib/systemd/system/artifactory.service:11: PIDFile= references a path below legacy directory /var/run>
Jul 31 16:28:13 localhost.localdomain systemd[1]: /usr/lib/systemd/system/artifactory.service:11: PIDFile= references a path below legacy directory /var/run>

[root@localhost etc]# systemctl restart artifactory.service
[root@localhost etc]# systemctl status artifactory.service
● artifactory.service - Artifactory service
     Loaded: loaded (/usr/lib/systemd/system/artifactory.service; enabled; preset: disabled)
     Active: active (running) since Thu 2025-07-31 16:32:54 IST; 54s ago
    Process: 54684 ExecStart=/opt/jfrog/artifactory/app/bin/artifactoryManage.sh start (code=exited, status=0/SUCCESS)
   Main PID: 60433 (java)
      Tasks: 0 (limit: 23020)
     Memory: 3.3M
        CPU: 4.582s
     CGroup: /system.slice/artifactory.service
             ‣ 60433 /opt/jfrog/artifactory/app/third-party/java/bin/java -Djava.util.logging.config.file=/opt/jfrog/artifactory/app/artifactory/tomcat/conf>

Jul 31 16:32:47 localhost.localdomain artifactoryManage.sh[54684]: /usr/bin/netstat
Jul 31 16:32:48 localhost.localdomain artifactoryManage.sh[54684]: /usr/bin/netstat
Jul 31 16:32:49 localhost.localdomain artifactoryManage.sh[54684]: /usr/bin/netstat
Jul 31 16:32:50 localhost.localdomain artifactoryManage.sh[54684]: /usr/bin/netstat
Jul 31 16:32:51 localhost.localdomain artifactoryManage.sh[54684]: /usr/bin/netstat
Jul 31 16:32:52 localhost.localdomain artifactoryManage.sh[54684]: /usr/bin/netstat
Jul 31 16:32:54 localhost.localdomain artifactoryManage.sh[54684]: /usr/bin/netstat
Jul 31 16:32:54 localhost.localdomain artifactoryManage.sh[54684]: 2025-07-31T11:02:54.071Z [shell] [INFO ] [] [artifactoryManage.sh:135      ] [main] - Art>
Jul 31 16:32:54 localhost.localdomain artifactoryManage.sh[54684]: 2025-07-31T11:02:54.122Z [shell] [INFO ] [] [artifactoryCommon.sh:1248     ] [main] - Art>
Jul 31 16:32:54 localhost.localdomain systemd[1]: Started Artifactory service.

[root@localhost etc]# yum install nginx
Updating Subscription Management repositories.
Last metadata expiration check: 0:10:07 ago on Thursday 31 July 2025 04:26:42 PM.
Dependencies resolved.
=============================================================================================================================================================
 Package                             Architecture            Version                                 Repository                                         Size
=============================================================================================================================================================
Installing:
 nginx                               x86_64                  2:1.20.1-22.el9_6.3                     rhel-9-for-x86_64-appstream-rpms                   37 k
Installing dependencies:
 nginx-core                          x86_64                  2:1.20.1-22.el9_6.3                     rhel-9-for-x86_64-appstream-rpms                  571 k
 nginx-filesystem                    noarch                  2:1.20.1-22.el9_6.3                     rhel-9-for-x86_64-appstream-rpms                   10 k
 redhat-logos-httpd                  noarch                  90.5-1.el9_6.1                          rhel-9-for-x86_64-appstream-rpms                   16 k

Transaction Summary
=============================================================================================================================================================
Install  4 Packages

Total download size: 635 k
Installed size: 1.8 M
Is this ok [y/N]: y
Downloading Packages:
(1/4): nginx-1.20.1-22.el9_6.3.x86_64.rpm                                                                                     81 kB/s |  37 kB     00:00
(2/4): nginx-filesystem-1.20.1-22.el9_6.3.noarch.rpm                                                                          18 kB/s |  10 kB     00:00
(3/4): nginx-core-1.20.1-22.el9_6.3.x86_64.rpm                                                                               879 kB/s | 571 kB     00:00
(4/4): redhat-logos-httpd-90.5-1.el9_6.1.noarch.rpm                                                                           55 kB/s |  16 kB     00:00
-------------------------------------------------------------------------------------------------------------------------------------------------------------
Total                                                                                                                        837 kB/s | 635 kB     00:00
Red Hat Enterprise Linux 9 for x86_64 - AppStream (RPMs)                                                                     1.0 MB/s | 3.6 kB     00:00
Importing GPG key 0xFD431D51:
 Userid     : "Red Hat, Inc. (release key 2) <security@redhat.com>"
 Fingerprint: 567E 347A D004 4ADE 55BA 8A5F 199E 2F91 FD43 1D51
 From       : /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release
Is this ok [y/N]: y
Key imported successfully
Importing GPG key 0x5A6340B3:
 Userid     : "Red Hat, Inc. (auxiliary key 3) <security@redhat.com>"
 Fingerprint: 7E46 2425 8C40 6535 D56D 6F13 5054 E4A4 5A63 40B3
 From       : /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release
Is this ok [y/N]: y
Key imported successfully
Running transaction check
yTransaction check succeeded.
Running transaction test
Transaction test succeeded.
Running transaction
  Preparing        :                                                                                                                                     1/1
  Running scriptlet: nginx-filesystem-2:1.20.1-22.el9_6.3.noarch                                                                                         1/4
  Installing       : nginx-filesystem-2:1.20.1-22.el9_6.3.noarch                                                                                         1/4
  Installing       : nginx-core-2:1.20.1-22.el9_6.3.x86_64                                                                                               2/4
  Installing       : redhat-logos-httpd-90.5-1.el9_6.1.noarch                                                                                            3/4
  Installing       : nginx-2:1.20.1-22.el9_6.3.x86_64                                                                                                    4/4
  Running scriptlet: nginx-2:1.20.1-22.el9_6.3.x86_64                                                                                                    4/4
  Verifying        : nginx-2:1.20.1-22.el9_6.3.x86_64                                                                                                    1/4
  Verifying        : nginx-core-2:1.20.1-22.el9_6.3.x86_64                                                                                               2/4
  Verifying        : nginx-filesystem-2:1.20.1-22.el9_6.3.noarch                                                                                         3/4
  Verifying        : redhat-logos-httpd-90.5-1.el9_6.1.noarch                                                                                            4/4
Installed products updated.

Installed:
  nginx-2:1.20.1-22.el9_6.3.x86_64                    nginx-core-2:1.20.1-22.el9_6.3.x86_64            nginx-filesystem-2:1.20.1-22.el9_6.3.noarch
  redhat-logos-httpd-90.5-1.el9_6.1.noarch

Complete!
[root@localhost etc]# cd /etc/nginx/
conf.d/    default.d/
[root@localhost etc]# cd /etc/nginx/
conf.d/    default.d/
[root@localhost etc]# cd /etc/nginx/
[root@localhost nginx]# ll
total 68
drwxr-xr-x. 2 root root    6 May 15 01:52 conf.d
drwxr-xr-x. 2 root root    6 May 15 01:52 default.d
-rw-r--r--. 1 root root 1077 May 15 01:51 fastcgi.conf
-rw-r--r--. 1 root root 1077 May 15 01:51 fastcgi.conf.default
-rw-r--r--. 1 root root 1007 May 15 01:51 fastcgi_params
-rw-r--r--. 1 root root 1007 May 15 01:51 fastcgi_params.default
-rw-r--r--. 1 root root 2837 May 15 01:51 koi-utf
-rw-r--r--. 1 root root 2223 May 15 01:51 koi-win
-rw-r--r--. 1 root root 5231 May 15 01:51 mime.types
-rw-r--r--. 1 root root 5231 May 15 01:51 mime.types.default
-rw-r--r--. 1 root root 2334 May 15 01:50 nginx.conf
-rw-r--r--. 1 root root 2656 May 15 01:51 nginx.conf.default
-rw-r--r--. 1 root root  636 May 15 01:51 scgi_params
-rw-r--r--. 1 root root  636 May 15 01:51 scgi_params.default
-rw-r--r--. 1 root root  664 May 15 01:51 uwsgi_params
-rw-r--r--. 1 root root  664 May 15 01:51 uwsgi_params.default
-rw-r--r--. 1 root root 3610 May 15 01:51 win-utf
[root@localhost nginx]# podman login 192.168.95.71:8081
Username: admin
Password:
Error: authenticating creds for "192.168.95.71:8081": pinging container registry 192.168.95.71:8081: Get "https://192.168.95.71:8081/v2/": http: server gave HTTP response to HTTPS client
[root@localhost nginx]# podman login http://192.168.95.71:8081
Username: admin
Password:
Error: authenticating creds for "192.168.95.71:8081": pinging container registry 192.168.95.71:8081: Get "https://192.168.95.71:8081/v2/": http: server gave HTTP response to HTTPS client
[root@localhost nginx]# podman login http://192.168.95.71:8081
Username: admin
Password:
Error: authenticating creds for "192.168.95.71:8081": pinging container registry 192.168.95.71:8081: Get "https://192.168.95.71:8081/v2/": http: server gave HTTP response to HTTPS client
[root@localhost nginx]# podman login http://192.168.95.71:8081
Username: admin
Password:
Error: authenticating creds for "192.168.95.71:8081": pinging container registry 192.168.95.71:8081: Get "https://192.168.95.71:8081/v2/": http: server gave HTTP response to HTTPS client
[root@localhost nginx]#
[root@localhost nginx]#
[root@localhost nginx]#
[root@localhost nginx]#
[root@localhost nginx]# pwd
/etc/nginx
[root@localhost nginx]# ll
total 68
drwxr-xr-x. 2 root root    6 May 15 01:52 conf.d
drwxr-xr-x. 2 root root    6 May 15 01:52 default.d
-rw-r--r--. 1 root root 1077 May 15 01:51 fastcgi.conf
-rw-r--r--. 1 root root 1077 May 15 01:51 fastcgi.conf.default
-rw-r--r--. 1 root root 1007 May 15 01:51 fastcgi_params
-rw-r--r--. 1 root root 1007 May 15 01:51 fastcgi_params.default
-rw-r--r--. 1 root root 2837 May 15 01:51 koi-utf
-rw-r--r--. 1 root root 2223 May 15 01:51 koi-win
-rw-r--r--. 1 root root 5231 May 15 01:51 mime.types
-rw-r--r--. 1 root root 5231 May 15 01:51 mime.types.default
-rw-r--r--. 1 root root 2334 May 15 01:50 nginx.conf
-rw-r--r--. 1 root root 2656 May 15 01:51 nginx.conf.default
-rw-r--r--. 1 root root  636 May 15 01:51 scgi_params
-rw-r--r--. 1 root root  636 May 15 01:51 scgi_params.default
-rw-r--r--. 1 root root  664 May 15 01:51 uwsgi_params
-rw-r--r--. 1 root root  664 May 15 01:51 uwsgi_params.default
-rw-r--r--. 1 root root 3610 May 15 01:51 win-utf
[root@localhost nginx]# ip r l
default via 192.168.10.1 dev ens192 proto static metric 100
192.168.0.0/16 dev ens192 proto kernel scope link src 192.168.95.71 metric 100
[root@localhost nginx]# mkdir -p ssl
[root@localhost nginx]# vim nginx.conf
[root@localhost nginx]# pwd
/etc/nginx
[root@localhost nginx]# vim nginx.conf
[root@localhost nginx]#
[root@localhost nginx]#
[root@localhost nginx]# systemctl retart nginx
Unknown command verb retart.
[root@localhost nginx]# systemctl restart nginx
[root@localhost nginx]# systemctl status nginx
● nginx.service - The nginx HTTP and reverse proxy server
     Loaded: loaded (/usr/lib/systemd/system/nginx.service; disabled; preset: disabled)
     Active: active (running) since Thu 2025-07-31 16:48:49 IST; 5s ago
    Process: 92849 ExecStartPre=/usr/bin/rm -f /run/nginx.pid (code=exited, status=0/SUCCESS)
    Process: 92850 ExecStartPre=/usr/sbin/nginx -t (code=exited, status=0/SUCCESS)
    Process: 92851 ExecStart=/usr/sbin/nginx (code=exited, status=0/SUCCESS)
   Main PID: 92853 (nginx)
      Tasks: 3 (limit: 23020)
     Memory: 3.3M
        CPU: 44ms
     CGroup: /system.slice/nginx.service
             ├─92853 "nginx: master process /usr/sbin/nginx"
             ├─92854 "nginx: worker process"
             └─92855 "nginx: worker process"

Jul 31 16:48:49 localhost.localdomain systemd[1]: Starting The nginx HTTP and reverse proxy server...
Jul 31 16:48:49 localhost.localdomain nginx[92850]: nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
Jul 31 16:48:49 localhost.localdomain nginx[92850]: nginx: configuration file /etc/nginx/nginx.conf test is successful
Jul 31 16:48:49 localhost.localdomain systemd[1]: Started The nginx HTTP and reverse proxy server.
[root@localhost nginx]# ip r l
default via 192.168.10.1 dev ens192 proto static metric 100
192.168.0.0/16 dev ens192 proto kernel scope link src 192.168.95.71 metric 100
[root@localhost nginx]#
[root@localhost nginx]#
[root@localhost nginx]# podman login -u admin 192.168.95.71:9443
Password:
Error: authenticating creds for "192.168.95.71:9443": pinging container registry 192.168.95.71:9443: Get "https://192.168.95.71:9443/v2/": tls: failed to verify certificate: x509: certificate signed by unknown authority
[root@localhost nginx]#
[root@localhost nginx]#
[root@localhost nginx]#
[root@localhost nginx]# openssl s_client -connect 192.168.95.71:9443
CONNECTED(00000003)
Can't use SSL_get_servername
depth=0 C = US, ST = TestState, L = TestCity, O = MyOrg, OU = MyUnit, CN = jfrog.prodevans.com
verify error:num=18:self-signed certificate
verify return:1
depth=0 C = US, ST = TestState, L = TestCity, O = MyOrg, OU = MyUnit, CN = jfrog.prodevans.com
verify return:1
---
Certificate chain
 0 s:C = US, ST = TestState, L = TestCity, O = MyOrg, OU = MyUnit, CN = jfrog.prodevans.com
   i:C = US, ST = TestState, L = TestCity, O = MyOrg, OU = MyUnit, CN = jfrog.prodevans.com
   a:PKEY: rsaEncryption, 2048 (bit); sigalg: RSA-SHA256
   v:NotBefore: Jul 31 11:15:24 2025 GMT; NotAfter: Jul 31 11:15:24 2026 GMT
---
Server certificate
-----BEGIN CERTIFICATE-----
MIIDvjCCAqagAwIBAgIUZ/sbWZPF+VZ6hYWEHQ6eBhgdfOgwDQYJKoZIhvcNAQEL
BQAwczELMAkGA1UEBhMCVVMxEjAQBgNVBAgMCVRlc3RTdGF0ZTERMA8GA1UEBwwI
VGVzdENpdHkxDjAMBgNVBAoMBU15T3JnMQ8wDQYDVQQLDAZNeVVuaXQxHDAaBgNV
BAMME2pmcm9nLnByb2RldmFucy5jb20wHhcNMjUwNzMxMTExNTI0WhcNMjYwNzMx
MTExNTI0WjBzMQswCQYDVQQGEwJVUzESMBAGA1UECAwJVGVzdFN0YXRlMREwDwYD
VQQHDAhUZXN0Q2l0eTEOMAwGA1UECgwFTXlPcmcxDzANBgNVBAsMBk15VW5pdDEc
MBoGA1UEAwwTamZyb2cucHJvZGV2YW5zLmNvbTCCASIwDQYJKoZIhvcNAQEBBQAD
ggEPADCCAQoCggEBAOOPCBr1oVDkX877JkERIRJIDuIPIrahAMs6KSW+1ERoYEw7
WRTdheHdeXqIxdEubfOuN0stKjWkV1/zzVWryoP9uYz0dho/M9xsGeClJqIw3bcz
NHqinpNoBV0r8OLmGNwEaMY+Gd6PjWkxLex3IaWRNBiJUI/3DW2q8W4jwo9axEWe
xMOqWGaU5zwUrKSmC+T6ehTQ3kihohxl7rWRFh++TboMwvT7GzIc6qhAfiKhIESV
DyvlmfUeBk/hPWgK7KZZP2tEPJfljJe7HM1AWZ+hd2rhlNVbFCQlqVD/73A0/a3P
dVYVLosLsu9N7tobn+O2GaOogsRMbmdEC86EkHsCAwEAAaNKMEgwJwYDVR0RBCAw
HocEwKhfRYcEwKhfRocEwKhfR4cEwKhfSIcEwKhfSTAdBgNVHQ4EFgQUkS+yMhkE
1cWfMsmf7+xfOYr08NEwDQYJKoZIhvcNAQELBQADggEBADAKioARB53aXDUPBGGt
AJUtV15HN4NF570hRX9df5NMCz580bhAjv6XQFAlYSN2rdxCqKGnq4vu47KnvSbZ
we0arOpxmZqvp0VuL9dFllvURrIS25Xi/wx6E30wtp3t+ubS1Md7ZzxsK3sC82VP
NMSU7PsKah3qAolvKY4PUvJgreUA0haF4nHyqrAPWTnumfAuKH2Wx9v6lT5A343j
EGITEmnMdIl0vsPeU2DizjibFLwW0kEiK2gK+I+TSxdib70U4z3KyX1K8Z161aIG
W0wpAdnbpJ4Q4XzyiqCyyqB71l8xyNbExkLWWgshBTdUiu/Hsmtnbv0Ebw9mHv66
9Z8=
-----END CERTIFICATE-----
subject=C = US, ST = TestState, L = TestCity, O = MyOrg, OU = MyUnit, CN = jfrog.prodevans.com
issuer=C = US, ST = TestState, L = TestCity, O = MyOrg, OU = MyUnit, CN = jfrog.prodevans.com
---
No client certificate CA names sent
Peer signing digest: SHA256
Peer signature type: RSA-PSS
Server Temp Key: X25519, 253 bits
---
SSL handshake has read 1518 bytes and written 375 bytes
Verification error: self-signed certificate
---
New, TLSv1.3, Cipher is TLS_AES_256_GCM_SHA384
Server public key is 2048 bit
Secure Renegotiation IS NOT supported
Compression: NONE
Expansion: NONE
No ALPN negotiated
Early data was not sent
Verify return code: 18 (self-signed certificate)
---
---
Post-Handshake New Session Ticket arrived:
SSL-Session:
    Protocol  : TLSv1.3
    Cipher    : TLS_AES_256_GCM_SHA384
    Session-ID: 73A5B083D0D05431AFE6C635402A02C7E6A552C7A537978A16C4700E1C62A360
    Session-ID-ctx:
    Resumption PSK: 9086F96958F185678D210C84F30EEDB137B2B9A2BF1987C4D29CAD03389DFB5719359F2C4BE05ABB80A3BE892C595CFF
    PSK identity: None
    PSK identity hint: None
    SRP username: None
    TLS session ticket lifetime hint: 600 (seconds)
    TLS session ticket:
    0000 - 61 ce b3 3f f8 e9 3a 59-24 c4 b8 de bd b9 2a 42   a..?..:Y$.....*B
    0010 - 7f e6 49 bb 6b 47 15 8e-f8 4e 41 92 d5 39 d0 65   ..I.kG...NA..9.e
    0020 - 67 e5 8b 43 13 78 e4 86-50 dc 4a 36 d9 91 15 75   g..C.x..P.J6...u
    0030 - cd 94 97 52 cf 51 e0 26-94 a0 be 89 48 6a 7e 2e   ...R.Q.&....Hj~.
    0040 - 42 ca 5f 3c 26 c8 36 67-d3 4f b6 c7 28 6c d5 f5   B._<&.6g.O..(l..
    0050 - d9 80 74 c5 7f 0f ae ba-74 f5 da 4b 72 a5 15 48   ..t.....t..Kr..H
    0060 - 79 c2 ce 5b a2 33 30 4d-57 fa 00 0e 59 da f4 e7   y..[.30MW...Y...
    0070 - 56 2d a2 80 09 4d 10 34-dd 3e 73 1e 8c d7 a5 c0   V-...M.4.>s.....
    0080 - 88 cd b1 47 7b ce b7 d5-50 ec 59 08 23 9a e7 27   ...G{...P.Y.#..'
    0090 - 01 28 d7 ba c6 1e 30 64-c7 79 0e f4 8e 81 cc b8   .(....0d.y......
    00a0 - 75 44 26 f3 95 e9 ec 12-32 f9 dd 6f 9c 6d e0 4c   uD&.....2..o.m.L
    00b0 - b1 65 91 be 13 e3 35 19-d6 2c 57 70 8e 9e b7 a5   .e....5..,Wp....
    00c0 - 34 49 86 2a e6 0d 46 e0-fa 1f a7 60 63 ce 34 2e   4I.*..F....`c.4.
    00d0 - cc e2 a1 e3 9b 82 0c ae-74 42 f9 bf f1 f7 f8 4b   ........tB.....K

    Start Time: 1753960874
    Timeout   : 7200 (sec)
    Verify return code: 18 (self-signed certificate)
    Extended master secret: no
    Max Early Data: 0
---
read R BLOCK
---
Post-Handshake New Session Ticket arrived:
SSL-Session:
    Protocol  : TLSv1.3
    Cipher    : TLS_AES_256_GCM_SHA384
    Session-ID: 4768515611B63E43173A10C186B6FF3066F3D6552D23F359F1E0253BB04B3D24
    Session-ID-ctx:
    Resumption PSK: 8A3F17E960B9BD92A22582485B2F7510A083910B1D51717594662165A05E283D9DD5EEF77F4584F2FF64644E8B8D7BE1
    PSK identity: None
    PSK identity hint: None
    SRP username: None
    TLS session ticket lifetime hint: 600 (seconds)
    TLS session ticket:
    0000 - 61 ce b3 3f f8 e9 3a 59-24 c4 b8 de bd b9 2a 42   a..?..:Y$.....*B
    0010 - f1 83 29 84 a4 52 01 f9-1a 0b 95 65 ff d1 9e c0   ..)..R.....e....
    0020 - 33 65 2d 82 2a b1 5f 08-96 64 d8 74 65 2c a4 d9   3e-.*._..d.te,..
    0030 - 3a 1f 01 c2 fa cf 27 f8-4d 53 42 e0 0c 62 e4 ad   :.....'.MSB..b..
    0040 - fc 4c 47 97 f4 84 0b ee-33 87 57 88 35 7b be 96   .LG.....3.W.5{..
    0050 - 6a e4 ec c9 3b 92 11 0e-11 1b 20 06 b1 98 d3 74   j...;..... ....t
    0060 - 18 f2 44 b6 c7 8c 20 f0-4d 47 6a d3 bf 4c ac 31   ..D... .MGj..L.1
    0070 - 64 13 0e f9 b9 ba 77 9e-0c d1 a2 ca b1 ac b1 46   d.....w........F
    0080 - 90 13 c7 c8 a5 ab 5a 16-59 77 93 94 e0 5e ac 4c   ......Z.Yw...^.L
    0090 - f4 d9 b4 53 15 10 76 8a-bc 2c 01 94 49 42 bc 95   ...S..v..,..IB..
    00a0 - 26 e3 4a 1c 96 7e 0d 29-a6 f1 e1 a2 d2 01 aa f9   &.J..~.)........
    00b0 - 77 a5 02 c3 80 e1 28 4e-02 30 97 fd 11 ed 85 14   w.....(N.0......
    00c0 - 27 6e 49 32 10 3a d6 b9-28 d5 cf 2f 1a 50 a0 4f   'nI2.:..(../.P.O
    00d0 - f3 55 7e 8b a5 44 c6 44-f9 a1 c1 4d 11 c2 48 e7   .U~..D.D...M..H.

    Start Time: 1753960874
    Timeout   : 7200 (sec)
    Verify return code: 18 (self-signed certificate)
    Extended master secret: no
    Max Early Data: 0
---
read R BLOCK
^C
[root@localhost nginx]# pwd
/etc/nginx
[root@localhost nginx]# cd /etc/containers/certs.d/
[root@localhost certs.d]# mkdir 192.168.95.71:9443
[root@localhost certs.d]# cd 192.168.95.71\:9443/
[root@localhost 192.168.95.71:9443]# vi jfrog.cert
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]# podman login -u admin 192.168.95.71:9443
Password:
Error: authenticating creds for "192.168.95.71:9443": creating new docker client: missing key jfrog.key for client certificate jfrog.cert. Note that CA certificates should use the extension .crt
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]# ll
total 4
-rw-r--r--. 1 root root 1359 Jul 31 16:52 jfrog.cert
[root@localhost 192.168.95.71:9443]# cp jfrog.cert ca.crt
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]# podman login -u admin 192.168.95.71:9443
Password:
Error: authenticating creds for "192.168.95.71:9443": creating new docker client: missing key jfrog.key for client certificate jfrog.cert. Note that CA certificates should use the extension .crt
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]# ll
total 8
-rw-r--r--. 1 root root 1359 Jul 31 16:52 ca.crt
-rw-r--r--. 1 root root 1359 Jul 31 16:52 jfrog.cert
[root@localhost 192.168.95.71:9443]# rm -rf jfrog.cert
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]# podman login -u admin 192.168.95.71:9443
Password:
Error: authenticating creds for "192.168.95.71:9443": Get "https://192.168.95.71/v2/token?account=admin&service=192.168.95.71": dial tcp 192.168.95.71:443: connect: connection refused
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]#
[root@localhost 192.168.95.71:9443]# cd /etc/nginx/ssl
[root@localhost ssl]# ll
total 12
-rw-r--r--. 1 root root 2296 Jul 31 16:47 nginx.conf
-rw-r--r--. 1 root root 1359 Jul 31 16:46 server.crt
-rw-------. 1 root root 1708 Jul 31 16:46 server.key
[root@localhost ssl]# podman login -u admin 192.168.95.71:9443
Password:
Error: authenticating creds for "192.168.95.71:9443": Get "https://192.168.95.71/v2/token?account=admin&service=192.168.95.71": dial tcp 192.168.95.71:443: connect: connection refused
[root@localhost ssl]#
[root@localhost ssl]#
[root@localhost ssl]#
[root@localhost ssl]# pwd
/etc/nginx/ssl
[root@localhost ssl]# ip r l
default via 192.168.10.1 dev ens192 proto static metric 100
192.168.0.0/16 dev ens192 proto kernel scope link src 192.168.95.71 metric 100
[root@localhost ssl]# cd /etc/nginx/
[root@localhost nginx]# cp nginx.conf old_nginx.conf
[root@localhost nginx]# > nginx.conf
[root@localhost nginx]# vim nginx.conf
[root@localhost nginx]#
[root@localhost nginx]#
[root@localhost nginx]# systemctl restart nginx.service
[root@localhost nginx]# systemctl status nginx.service
● nginx.service - The nginx HTTP and reverse proxy server
     Loaded: loaded (/usr/lib/systemd/system/nginx.service; disabled; preset: disabled)
     Active: active (running) since Thu 2025-07-31 16:59:04 IST; 4s ago
    Process: 93295 ExecStartPre=/usr/bin/rm -f /run/nginx.pid (code=exited, status=0/SUCCESS)
    Process: 93296 ExecStartPre=/usr/sbin/nginx -t (code=exited, status=0/SUCCESS)
    Process: 93297 ExecStart=/usr/sbin/nginx (code=exited, status=0/SUCCESS)
   Main PID: 93299 (nginx)
      Tasks: 3 (limit: 23020)
     Memory: 3.6M
        CPU: 41ms
     CGroup: /system.slice/nginx.service
             ├─93299 "nginx: master process /usr/sbin/nginx"
             ├─93300 "nginx: worker process"
             └─93301 "nginx: worker process"

Jul 31 16:59:04 localhost.localdomain systemd[1]: Starting The nginx HTTP and reverse proxy server...
Jul 31 16:59:04 localhost.localdomain nginx[93296]: nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
Jul 31 16:59:04 localhost.localdomain nginx[93296]: nginx: configuration file /etc/nginx/nginx.conf test is successful
Jul 31 16:59:04 localhost.localdomain systemd[1]: Started The nginx HTTP and reverse proxy server.
[root@localhost nginx]#
[root@localhost nginx]# netstat -tunlpe
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       User       Inode      PID/Program name
tcp        0      0 127.0.0.1:5432          0.0.0.0:*               LISTEN      26         289031     53027/postgres
tcp        0      0 0.0.0.0:9443            0.0.0.0:*               LISTEN      0          346011     93299/nginx: master
tcp        0      0 127.0.0.1:8049          0.0.0.0:*               LISTEN      980        300278     59691/jf-router
tcp        0      0 127.0.0.1:8047          0.0.0.0:*               LISTEN      980        301084     59691/jf-router
tcp        0      0 127.0.0.1:8046          0.0.0.0:*               LISTEN      980        300282     59691/jf-router
tcp        0      0 127.0.0.1:8036          0.0.0.0:*               LISTEN      980        301248     60248/jf-observabil
tcp        0      0 127.0.0.1:8086          0.0.0.0:*               LISTEN      980        301165     59888/jf-metadata
tcp        0      0 127.0.0.1:8070          0.0.0.0:*               LISTEN      980        301414     60082/node
tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      0          21855      1079/cupsd
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      0          22789      1081/sshd: /usr/sbi
tcp6       0      0 :::8081                 :::*                    LISTEN      980        299544     60433/java
tcp6       0      0 :::8082                 :::*                    LISTEN      980        300283     59691/jf-router
tcp6       0      0 ::1:631                 :::*                    LISTEN      0          21854      1079/cupsd
tcp6       0      0 127.0.0.1:8040          :::*                    LISTEN      980        298573     59523/java
tcp6       0      0 127.0.0.1:8045          :::*                    LISTEN      980        299881     59523/java
tcp6       0      0 127.0.0.1:8016          :::*                    LISTEN      980        299932     59523/java
tcp6       0      0 127.0.0.1:8015          :::*                    LISTEN      980        300072     60433/java
tcp6       0      0 127.0.0.1:8091          :::*                    LISTEN      980        299545     60433/java
tcp6       0      0 ::1:5432                :::*                    LISTEN      26         289030     53027/postgres
tcp6       0      0 :::22                   :::*                    LISTEN      0          22791      1081/sshd: /usr/sbi
udp        0      0 127.0.0.1:323           0.0.0.0:*                           0          20317      923/chronyd
udp        0      0 0.0.0.0:44235           0.0.0.0:*                           70         21165      855/avahi-daemon: r
udp        0      0 0.0.0.0:5353            0.0.0.0:*                           70         21163      855/avahi-daemon: r
udp6       0      0 :::34988                :::*                                70         21166      855/avahi-daemon: r
udp6       0      0 ::1:323                 :::*                                0          20318      923/chronyd
udp6       0      0 :::5353                 :::*                                70         21164      855/avahi-daemon: r
[root@localhost nginx]# podman login -u admin 192.168.95.71:9443
Password:
Login Succeeded!
[root@localhost nginx]# ip r
default via 192.168.10.1 dev ens192 proto static metric 100
192.168.0.0/16 dev ens192 proto kernel scope link src 192.168.95.71 metric 100
[root@localhost nginx]# cd
[root@localhost ~]# podman pull registry.k8s.io/kube-apiserver:v1.32.0
Trying to pull registry.k8s.io/kube-apiserver:v1.32.0...
Getting image source signatures
Copying blob 2e4cf50eeb92 done   |
Copying blob d82bc7a76a83 done   |
Copying blob 0674a6f58b64 done   |
Copying blob b6824ed73363 done   |
Copying blob 6f4cfee9177b done   |
Copying blob 7c12895b777b done   |
Copying blob 5664b15f108b done   |
Copying blob 27be814a09eb done   |
Copying blob 4aa0ea1413d3 done   |
Copying blob 3f4e2c586348 done   |
Copying blob 9aee425378d2 done   |
Copying blob a88274b8bba7 done   |
Copying blob 890390ebdc76 done   |
Copying config c2e17b8d0f done   |
Writing manifest to image destination
c2e17b8d0f4a39ed32f1c1fd4eb408627c94111ae9a46c2034758e4ced4f79c4
[root@localhost ~]#  podman pull registry.k8s.io/kube-controller-manager:v1.32.0
 podman pull registry.k8s.io/kube-scheduler:v1.32.0
 podman pull registry.k8s.io/kube-proxy:v1.32.0
 podman pull registry.k8s.io/pause:3.9
 podman pull registry.k8s.io/etcd:3.5.9-0
 podman pull registry.k8s.io/coredns/coredns:v1.10.1
Trying to pull registry.k8s.io/kube-controller-manager:v1.32.0...
Getting image source signatures
Copying blob 2e4cf50eeb92 skipped: already exists
Copying blob d82bc7a76a83 skipped: already exists
Copying blob 6f4cfee9177b skipped: already exists
Copying blob b6824ed73363 skipped: already exists
Copying blob 7c12895b777b skipped: already exists
Copying blob 5664b15f108b skipped: already exists
Copying blob 27be814a09eb skipped: already exists
Copying blob 0674a6f58b64 skipped: already exists
Copying blob 3f4e2c586348 skipped: already exists
Copying blob 9aee425378d2 skipped: already exists
Copying blob a88274b8bba7 skipped: already exists
Copying blob 9b34c506a565 done   |
Copying blob 4aa0ea1413d3 skipped: already exists
Copying config 8cab3d2a8b done   |
Writing manifest to image destination
8cab3d2a8bd0fe4127810f35afe0ffd42bfe75b2a4712a84da5595d4bde617d3
Trying to pull registry.k8s.io/kube-scheduler:v1.32.0...
Getting image source signatures
Copying blob d82bc7a76a83 skipped: already exists
Copying blob 6f4cfee9177b skipped: already exists
Copying blob 2e4cf50eeb92 skipped: already exists
Copying blob b6824ed73363 skipped: already exists
Copying blob 7c12895b777b skipped: already exists
Copying blob 5664b15f108b skipped: already exists
Copying blob a88274b8bba7 skipped: already exists
Copying blob 0674a6f58b64 skipped: already exists
Copying blob 4aa0ea1413d3 skipped: already exists
Copying blob 3f4e2c586348 skipped: already exists
Copying blob 9aee425378d2 skipped: already exists
Copying blob 27be814a09eb skipped: already exists
Copying blob 72b3eca693e1 done   |
Copying config a389e107f4 done   |
Writing manifest to image destination
a389e107f4ff1130c69849f0af08cbce9a1dfe3b7c39874012587d233807cfc5
Trying to pull registry.k8s.io/kube-proxy:v1.32.0...
Getting image source signatures
Copying blob 4d7aa327eef4 done   |
Copying blob 4d90f05edc6a done   |
Copying config 040f9f8aac done   |
Writing manifest to image destination
040f9f8aac8cd21d78f05ebfa9621ffb84e3257300c3cb1f72b539a3c3a2cd08
Trying to pull registry.k8s.io/pause:3.9...
Getting image source signatures
Copying blob 61fec91190a0 done   |
Copying config e6f1816883 done   |
Writing manifest to image destination
e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c
Trying to pull registry.k8s.io/etcd:3.5.9-0...
Getting image source signatures
Copying blob 1e3d9b7d1452 done   |
Copying blob fe5ca62666f0 done   |
Copying blob a7ca0d9ba68f done   |
Copying blob fcb6f6d2c998 done   |
Copying blob b02a7525f878 done   |
Copying blob e8c73c638ae9 done   |
Copying blob 4aa0ea1413d3 skipped: already exists
Copying blob 7c881f9ab25e done   |
Copying blob 5627a970d25e done   |
Copying blob 3f4a72e37652 done   |
Copying blob 93182a730d98 done   |
Copying blob 5198587edd6d done   |
Copying config 73deb9a3f7 done   |
Writing manifest to image destination
73deb9a3f702532592a4167455f8bf2e5f5d900bcc959ba2fd2d35c321de1af9
Trying to pull registry.k8s.io/coredns/coredns:v1.10.1...
Getting image source signatures
Copying blob 3799eae1a077 done   |
Copying blob 25b7032c281a done   |
Copying config ead0a4a53d done   |
Writing manifest to image destination
ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc
[root@localhost ~]# podman images
REPOSITORY                               TAG         IMAGE ID      CREATED       SIZE
registry.k8s.io/kube-apiserver           v1.32.0     c2e17b8d0f4a  7 months ago  98.1 MB
registry.k8s.io/kube-controller-manager  v1.32.0     8cab3d2a8bd0  7 months ago  90.8 MB
registry.k8s.io/kube-scheduler           v1.32.0     a389e107f4ff  7 months ago  70.6 MB
registry.k8s.io/kube-proxy               v1.32.0     040f9f8aac8c  7 months ago  95.3 MB
registry.k8s.io/etcd                     3.5.9-0     73deb9a3f702  2 years ago   295 MB
registry.k8s.io/coredns/coredns          v1.10.1     ead0a4a53df8  2 years ago   53.6 MB
registry.k8s.io/pause                    3.9         e6f181688397  2 years ago   750 kB
[root@localhost ~]# podman tag registry.k8s.io/kube-apiserver:v1.32.0 192.168.95.71:9443/k8s/kube-apiserver:v1.32.0
[root@localhost ~]# podman push 192.168.95.71:9443/k8s/kube-apiserver:v1.32.0
Getting image source signatures
Copying blob 6f1cdceb6a31 done   |
Copying blob af5aa97ebe6c done   |
Copying blob d37950ece3d3 done   |
Copying blob 8fa10c0194df done   |
Copying blob 4d049f83d9cf done   |
Copying blob ddc6e550070c done   |
Copying blob bbb6cacb8c82 done   |
Copying blob 1a73b54f556b done   |
Copying blob f4aee9e53c42 done   |
Copying blob b336e209998f done   |
Copying blob 2a92d6ac9e4f done   |
Copying blob b0934aa74a46 done   |
Copying blob 94fbc2e8456a done   |
Copying config c2e17b8d0f done   |
Writing manifest to image destination
[root@localhost ~]# podman tag registry.k8s.io/kube-controller-manager:v1.32.0 192.168.95.71:9443/k8s/kube-controller-manager:v1.32.0
[root@localhost ~]# podman push 192.168.95.71:9443/k8s/kube-controller-manager:v1.32.0
Getting image source signatures
Copying blob 2dedfe187a19 skipped: already exists
Copying blob 02c91f6b395a skipped: already exists
Copying blob ffdf8cfdbafe skipped: already exists
Copying blob 818beb2a6ac5 skipped: already exists
Copying blob 70f76ff64431 skipped: already exists
Copying blob 0b2215fb0760 skipped: already exists
Copying blob 1e45aa110bce skipped: already exists
Copying blob 36e6687b1ede skipped: already exists
Copying blob 12b98db1f9a3 skipped: already exists
Copying blob 2bc97598cf09 skipped: already exists
Copying blob b4aa04aa577f skipped: already exists
Copying blob fffa1ce2a781 skipped: already exists
Copying blob 4d408fe32c6b done   |
Copying config 8cab3d2a8b done   |
Writing manifest to image destination
[root@localhost ~]# podman tag registry.k8s.io/kube-scheduler:v1.32.0 192.168.95.71:9443/k8s/kube-scheduler:v1.32.0
[root@localhost ~]# podman push 192.168.95.71:9443/k8s/kube-scheduler:v1.32.0
Getting image source signatures
Copying blob 818beb2a6ac5 skipped: already exists
Copying blob 02c91f6b395a skipped: already exists
Copying blob 70f76ff64431 skipped: already exists
Copying blob 12b98db1f9a3 skipped: already exists
Copying blob 2dedfe187a19 skipped: already exists
Copying blob b4aa04aa577f skipped: already exists
Copying blob 36e6687b1ede skipped: already exists
Copying blob fffa1ce2a781 skipped: already exists
Copying blob 0b2215fb0760 skipped: already exists
Copying blob 3cc147f35962 done   |
Copying blob ffdf8cfdbafe skipped: already exists
Copying blob 1e45aa110bce skipped: already exists
Copying blob 2bc97598cf09 skipped: already exists
Copying config a389e107f4 done   |
Writing manifest to image destination
[root@localhost ~]# podman tag registry.k8s.io/kube-proxy:v1.32.0 192.168.95.71:9443/k8s/kube-proxy:v1.32.0
[root@localhost ~]# podman push 192.168.95.71:9443/k8s/kube-proxy:v1.32.0
Getting image source signatures
Copying blob f3d4d2f2afaf done   |
Copying blob aeda145147aa done   |
Copying config 040f9f8aac done   |
Writing manifest to image destination
[root@localhost ~]# podman images
REPOSITORY                                      TAG         IMAGE ID      CREATED       SIZE
192.168.95.71:9443/k8s/kube-apiserver           v1.32.0     c2e17b8d0f4a  7 months ago  98.1 MB
registry.k8s.io/kube-apiserver                  v1.32.0     c2e17b8d0f4a  7 months ago  98.1 MB
192.168.95.71:9443/k8s/kube-controller-manager  v1.32.0     8cab3d2a8bd0  7 months ago  90.8 MB
registry.k8s.io/kube-controller-manager         v1.32.0     8cab3d2a8bd0  7 months ago  90.8 MB
192.168.95.71:9443/k8s/kube-scheduler           v1.32.0     a389e107f4ff  7 months ago  70.6 MB
registry.k8s.io/kube-scheduler                  v1.32.0     a389e107f4ff  7 months ago  70.6 MB
192.168.95.71:9443/k8s/kube-proxy               v1.32.0     040f9f8aac8c  7 months ago  95.3 MB
registry.k8s.io/kube-proxy                      v1.32.0     040f9f8aac8c  7 months ago  95.3 MB
registry.k8s.io/etcd                            3.5.9-0     73deb9a3f702  2 years ago   295 MB
registry.k8s.io/coredns/coredns                 v1.10.1     ead0a4a53df8  2 years ago   53.6 MB
registry.k8s.io/pause                           3.9         e6f181688397  2 years ago   750 kB
[root@localhost ~]# podman images | grep reg
registry.k8s.io/kube-apiserver                  v1.32.0     c2e17b8d0f4a  7 months ago  98.1 MB
registry.k8s.io/kube-controller-manager         v1.32.0     8cab3d2a8bd0  7 months ago  90.8 MB
registry.k8s.io/kube-scheduler                  v1.32.0     a389e107f4ff  7 months ago  70.6 MB
registry.k8s.io/kube-proxy                      v1.32.0     040f9f8aac8c  7 months ago  95.3 MB
registry.k8s.io/etcd                            3.5.9-0     73deb9a3f702  2 years ago   295 MB
registry.k8s.io/coredns/coredns                 v1.10.1     ead0a4a53df8  2 years ago   53.6 MB
registry.k8s.io/pause                           3.9         e6f181688397  2 years ago   750 kB
[root@localhost ~]# podman tag registry.k8s.io/etcd:3.5.9-0 192.168.95.71:9443/k8s/etcd:3.5.9-0
[root@localhost ~]# podman push 192.168.95.71:9443/k8s/etcd:3.5.9-0
Getting image source signatures
Copying blob e624a5370eca done   |
Copying blob ff5700ec5418 done   |
Copying blob e023e0e48e6e done   |
Copying blob d52f02c6501c done   |
Copying blob 7bea6b893187 done   |
Copying blob 6fbdf253bbc2 done   |
Copying blob d2d7ec0f6756 done   |
Copying blob 4cb10dd2545b done   |
Copying blob b4aa04aa577f skipped: already exists
Copying blob ba9afb2b3e0c done   |
Copying blob 22bba3da6b0d done   |
Copying blob a4563151d59b done   |
Copying config 73deb9a3f7 done   |
Writing manifest to image destination
[root@localhost ~]# podman tag registry.k8s.io/coredns/coredns:v1.10.1 192.168.95.71:9443/k8s/coredns/coredns:v1.10.1
[root@localhost ~]# podman push 192.168.95.71:9443/k8s/coredns/coredns:v1.10.1
Getting image source signatures
Copying blob 398c9baff0ce done   |
Copying blob 6a4a177e62f3 done   |
Copying config ead0a4a53d done   |
Writing manifest to image destination
[root@localhost ~]# podman tag registry.k8s.io/pause:3.9 192.168.95.71:9443/k8s/pause:3.9
[root@localhost ~]# podman push 192.168.95.71:9443/k8s/pause:3.9
Getting image source signatures
Copying blob e3e5579ddd43 done   |
Copying config e6f1816883 done   |
Writing manifest to image destination
[root@localhost ~]# mkdir k8s
[root@localhost ~]# ls
anaconda-ks.cfg  jfrog-artifactory-jcr-7.98.19.rpm  k8s
[root@localhost ~]# cd k8s/
[root@localhost k8s]# ls
[root@localhost k8s]# cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=0
EOF
[root@localhost k8s]# yum install --downloadonly --downloaddir=. kubeadm kubelet kubectl
Updating Subscription Management repositories.
Kubernetes                                                                                                                   1.9 kB/s | 1.4 kB     00:00
Errors during downloading metadata for repository 'kubernetes':
  - Status code: 404 for https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64/repodata/repomd.xml (IP: 142.251.222.174)
Error: Failed to download metadata for repo 'kubernetes': Cannot download repomd.xml: Cannot download repodata/repomd.xml: All mirrors were tried
[root@localhost k8s]# cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.32/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.32/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
EOF
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.32/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.32/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
[root@localhost k8s]# ll
total 0
[root@localhost k8s]# cd /etc/yum.repos.d/
[root@localhost yum.repos.d]# ls
kubernetes.repo  pgdg-redhat-all.repo  redhat.repo
[root@localhost yum.repos.d]# yum repo list
Updating Subscription Management repositories.
No such command: repo. Please use /usr/bin/yum --help
It could be a YUM plugin command, try: "yum install 'dnf-command(repo)'"
[root@localhost yum.repos.d]# yum repolist
Updating Subscription Management repositories.
repo id                                                         repo name
kubernetes                                                      Kubernetes
pgdg-common                                                     PostgreSQL common RPMs for RHEL / Rocky / AlmaLinux 9 - x86_64
pgdg13                                                          PostgreSQL 13 for RHEL / Rocky / AlmaLinux 9 - x86_64
pgdg14                                                          PostgreSQL 14 for RHEL / Rocky / AlmaLinux 9 - x86_64
pgdg15                                                          PostgreSQL 15 for RHEL / Rocky / AlmaLinux 9 - x86_64
pgdg16                                                          PostgreSQL 16 for RHEL / Rocky / AlmaLinux 9 - x86_64
pgdg17                                                          PostgreSQL 17 for RHEL / Rocky / AlmaLinux 9 - x86_64
rhel-9-for-x86_64-appstream-rpms                                Red Hat Enterprise Linux 9 for x86_64 - AppStream (RPMs)
rhel-9-for-x86_64-baseos-rpms                                   Red Hat Enterprise Linux 9 for x86_64 - BaseOS (RPMs)
[root@localhost yum.repos.d]# cd ~/k8s
[root@localhost k8s]# ll
total 0
[root@localhost k8s]# yum install --downloadonly --downloaddir=. kubeadm kubelet kubectl
Updating Subscription Management repositories.
Kubernetes                                                                                                                    16 kB/s |  20 kB     00:01
All matches were filtered out by exclude filtering for argument: kubeadm
All matches were filtered out by exclude filtering for argument: kubelet
All matches were filtered out by exclude filtering for argument: kubectl
Error: Unable to find a match: kubeadm kubelet kubectl
[root@localhost k8s]# yum search kubedam
Updating Subscription Management repositories.
^CKeyboardInterrupt: Terminated.
[root@localhost k8s]# yum search kubeadm
Updating Subscription Management repositories.
Last metadata expiration check: 0:00:25 ago on Thursday 31 July 2025 05:26:10 PM.
No matches found.
[root@localhost k8s]# pwd
/root/k8s
[root@localhost k8s]# vim /etc/yum.repos.d/kubernetes.repo
[root@localhost k8s]#
[root@localhost k8s]#
[root@localhost k8s]# yum search kubeadm
Updating Subscription Management repositories.
Last metadata expiration check: 0:02:41 ago on Thursday 31 July 2025 05:26:10 PM.
=============================================================== Name Exactly Matched: kubeadm ===============================================================
kubeadm.aarch64 : Command-line utility for administering a Kubernetes cluster
kubeadm.ppc64le : Command-line utility for administering a Kubernetes cluster
kubeadm.s390x : Command-line utility for administering a Kubernetes cluster
kubeadm.src : Command-line utility for administering a Kubernetes cluster
kubeadm.x86_64 : Command-line utility for administering a Kubernetes cluster
[root@localhost k8s]#
[root@localhost k8s]#
[root@localhost k8s]#
[root@localhost k8s]# pwd
/root/k8s
[root@localhost k8s]# yum install --downloadonly --downloaddir=. kubeadm kubelet kubectl
Updating Subscription Management repositories.
Kubernetes                                                                                                                   3.7 kB/s | 1.7 kB     00:00
Dependencies resolved.
=============================================================================================================================================================
 Package                                 Architecture            Version                             Repository                                         Size
=============================================================================================================================================================
Installing:
 kubeadm                                 x86_64                  1.32.7-150500.1.1                   kubernetes                                         12 M
 kubectl                                 x86_64                  1.32.7-150500.1.1                   kubernetes                                         11 M
 kubelet                                 x86_64                  1.32.7-150500.1.1                   kubernetes                                         15 M
Installing dependencies:
 conntrack-tools                         x86_64                  1.4.7-4.el9_5                       rhel-9-for-x86_64-appstream-rpms                  240 k
 cri-tools                               x86_64                  1.32.0-150500.1.1                   kubernetes                                        7.1 M
 kubernetes-cni                          x86_64                  1.6.0-150500.1.1                    kubernetes                                        8.0 M
 libnetfilter_cthelper                   x86_64                  1.0.0-22.el9                        rhel-9-for-x86_64-appstream-rpms                   26 k
 libnetfilter_cttimeout                  x86_64                  1.0.0-19.el9                        rhel-9-for-x86_64-appstream-rpms                   25 k
 libnetfilter_queue                      x86_64                  1.0.5-1.el9                         rhel-9-for-x86_64-appstream-rpms                   31 k

Transaction Summary
=============================================================================================================================================================
Install  9 Packages

Total download size: 53 M
Installed size: 289 M
YUM will only download packages for the transaction.
Is this ok [y/N]: y
Downloading Packages:
(1/9): kubeadm-1.32.7-150500.1.1.x86_64.rpm                                                                                   13 MB/s |  12 MB     00:00
(2/9): kubectl-1.32.7-150500.1.1.x86_64.rpm                                                                                  9.4 MB/s |  11 MB     00:01
(3/9): cri-tools-1.32.0-150500.1.1.x86_64.rpm                                                                                6.0 MB/s | 7.1 MB     00:01
(4/9): kubelet-1.32.7-150500.1.1.x86_64.rpm                                                                                   22 MB/s |  15 MB     00:00
(5/9): libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm                                                                          45 kB/s |  26 kB     00:00
(6/9): libnetfilter_queue-1.0.5-1.el9.x86_64.rpm                                                                              98 kB/s |  31 kB     00:00
(7/9): libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm                                                                         46 kB/s |  25 kB     00:00
(8/9): conntrack-tools-1.4.7-4.el9_5.x86_64.rpm                                                                              609 kB/s | 240 kB     00:00
(9/9): kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm                                                                            4.2 MB/s | 8.0 MB     00:01
-------------------------------------------------------------------------------------------------------------------------------------------------------------
Total                                                                                                                         17 MB/s |  53 MB     00:03
Kubernetes                                                                                                                   3.9 kB/s | 1.7 kB     00:00
Importing GPG key 0x9A296436:
 Userid     : "isv:kubernetes OBS Project <isv:kubernetes@build.opensuse.org>"
 Fingerprint: DE15 B144 86CD 377B 9E87 6E1A 2346 54DA 9A29 6436
 From       : https://pkgs.k8s.io/core:/stable:/v1.32/rpm/repodata/repomd.xml.key
Is this ok [y/N]: y
Key imported successfully
Complete!
The downloaded packages were saved in cache until the next successful transaction.
You can remove cached packages by executing 'yum clean packages'.
[root@localhost k8s]# ls
1                                         kubectl-1.32.7-150500.1.1.x86_64.rpm           libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm  kubelet-1.32.7-150500.1.1.x86_64.rpm           libnetfilter_queue-1.0.5-1.el9.x86_64.rpm
cri-tools-1.32.0-150500.1.1.x86_64.rpm    kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm
kubeadm-1.32.7-150500.1.1.x86_64.rpm      libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm
[root@localhost k8s]# yum install --downloadonly --downloaddir=. kubeadm kubelet kubectlyum install --downloadonly --downloaddir=. containerd^C
[root@localhost k8s]# ^C
[root@localhost k8s]#
[root@localhost k8s]#
[root@localhost k8s]#
[root@localhost k8s]# yum install --downloadonly --downloaddir=. containerd
Updating Subscription Management repositories.
Last metadata expiration check: 0:00:53 ago on Thursday 31 July 2025 05:29:56 PM.
No match for argument: containerd
Error: Unable to find a match: containerd
[root@localhost k8s]# cat <<EOF > /etc/yum.repos.d/kubernetes1.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=0
EOF
[root@localhost k8s]# cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=0
EOF^C
[root@localhost k8s]# yum install --downloadonly --downloaddir=. containerd
Updating Subscription Management repositories.
Repository kubernetes is listed more than once in the configuration
Last metadata expiration check: 0:01:40 ago on Thursday 31 July 2025 05:29:56 PM.
No match for argument: containerd
Error: Unable to find a match: containerd
[root@localhost k8s]# yum search containerd
Updating Subscription Management repositories.
Repository kubernetes is listed more than once in the configuration
Last metadata expiration check: 0:02:04 ago on Thursday 31 July 2025 05:29:56 PM.
No matches found.
[root@localhost k8s]# sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
Updating Subscription Management repositories.
Repository kubernetes is listed more than once in the configuration
Adding repo from: https://download.docker.com/linux/centos/docker-ce.repo
[root@localhost k8s]# cat <<EOF > /etc/yum.repos.d/kubernetes1.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=0
EOF
^C
[root@localhost k8s]# yum search containerd
Updating Subscription Management repositories.
Repository kubernetes is listed more than once in the configuration
Docker CE Stable - x86_64                                                                                                    290 kB/s |  78 kB     00:00
Last metadata expiration check: 0:00:01 ago on Thursday 31 July 2025 05:38:24 PM.
================================================================= Name Matched: containerd ==================================================================
containerd.io.x86_64 : An industry-standard container runtime
[root@localhost k8s]# cat <<EOF > /etc/yum.repos.d/kubernetes1.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=0
EOF
^C
[root@localhost k8s]# yum info containerd
Updating Subscription Management repositories.
Repository kubernetes is listed more than once in the configuration
Last metadata expiration check: 0:00:16 ago on Thursday 31 July 2025 05:38:24 PM.
Error: No matching Packages to list
[root@localhost k8s]# yum info containerd.io.x86_64
Updating Subscription Management repositories.
Repository kubernetes is listed more than once in the configuration
Last metadata expiration check: 0:00:35 ago on Thursday 31 July 2025 05:38:24 PM.
Available Packages
Name         : containerd.io
Version      : 1.7.27
Release      : 3.1.el9
Architecture : x86_64
Size         : 44 M
Source       : containerd.io-1.7.27-3.1.el9.src.rpm
Repository   : docker-ce-stable
Summary      : An industry-standard container runtime
URL          : https://containerd.io
License      : Apache-2.0
Description  : containerd is an industry-standard container runtime with an emphasis on
             : simplicity, robustness and portability. It is available as a daemon for Linux
             : and Windows, which can manage the complete container lifecycle of its host
             : system: image transfer and storage, container execution and supervision,
             : low-level storage and network attachments, etc.

[root@localhost k8s]#
[root@localhost k8s]#
[root@localhost k8s]#
[root@localhost k8s]# yum install --downloadonly --downloaddir=. containerd
Updating Subscription Management repositories.
Repository kubernetes is listed more than once in the configuration
Last metadata expiration check: 0:00:50 ago on Thursday 31 July 2025 05:38:24 PM.
Dependencies resolved.
=============================================================================================================================================================
 Package                               Architecture                   Version                                 Repository                                Size
=============================================================================================================================================================
Installing:
 containerd.io                         x86_64                         1.7.27-3.1.el9                          docker-ce-stable                          44 M

Transaction Summary
=============================================================================================================================================================
Install  1 Package

Total download size: 44 M
Installed size: 155 M
YUM will only download packages for the transaction.
Is this ok [y/N]: y
Downloading Packages:
containerd.io-1.7.27-3.1.el9.x86_64.rpm                                                                                       46 MB/s |  44 MB     00:00
-------------------------------------------------------------------------------------------------------------------------------------------------------------
Total                                                                                                                         46 MB/s |  44 MB     00:00
Docker CE Stable - x86_64                                                                                                     14 kB/s | 1.6 kB     00:00
Importing GPG key 0x621E9F35:
 Userid     : "Docker Release (CE rpm) <docker@docker.com>"
 Fingerprint: 060A 61C5 1B55 8A7F 742B 77AA C52F EB6B 621E 9F35
 From       : https://download.docker.com/linux/centos/gpg
Is this ok [y/N]: y
Key imported successfully
Complete!
The downloaded packages were saved in cache until the next successful transaction.
You can remove cached packages by executing 'yum clean packages'.
[root@localhost k8s]# yum install --downloadonly --downloaddir=. haproxy
Updating Subscription Management repositories.
Repository kubernetes is listed more than once in the configuration
Last metadata expiration check: 0:01:21 ago on Thursday 31 July 2025 05:38:24 PM.
Dependencies resolved.
=============================================================================================================================================================
 Package                       Architecture                 Version                             Repository                                              Size
=============================================================================================================================================================
Installing:
 haproxy                       x86_64                       2.4.22-4.el9                        rhel-9-for-x86_64-appstream-rpms                       2.2 M

Transaction Summary
=============================================================================================================================================================
Install  1 Package

Total download size: 2.2 M
Installed size: 6.6 M
YUM will only download packages for the transaction.
Is this ok [y/N]: y
Downloading Packages:
haproxy-2.4.22-4.el9.x86_64.rpm                                                                                              3.0 MB/s | 2.2 MB     00:00
-------------------------------------------------------------------------------------------------------------------------------------------------------------
Total                                                                                                                        3.0 MB/s | 2.2 MB     00:00
Complete!
The downloaded packages were saved in cache until the next successful transaction.
You can remove cached packages by executing 'yum clean packages'.
[root@localhost k8s]# ls
1                                         haproxy-2.4.22-4.el9.x86_64.rpm       kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm  kubeadm-1.32.7-150500.1.1.x86_64.rpm  libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm
containerd.io-1.7.27-3.1.el9.x86_64.rpm   kubectl-1.32.7-150500.1.1.x86_64.rpm  libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm
cri-tools-1.32.0-150500.1.1.x86_64.rpm    kubelet-1.32.7-150500.1.1.x86_64.rpm  libnetfilter_queue-1.0.5-1.el9.x86_64.rpm
[root@localhost k8s]# cd ..
[root@localhost ~]# ls
anaconda-ks.cfg  jfrog-artifactory-jcr-7.98.19.rpm  k8s
[root@localhost ~]# cd /etc/containers/certs.d/
[root@localhost certs.d]# ls
192.168.95.71:9443
[root@localhost certs.d]# scp -r 192.168.95.7
192.168.95.70:        192.168.95.71\:9443/
[root@localhost certs.d]# scp -r 192.168.95.7
192.168.95.70:        192.168.95.71\:9443/
[root@localhost certs.d]# scp -r 192.168.95.7:^C

cp: missing destination file operand after '192.168.95.7:'
Try 'cp --help' for more information.
[root@localhost certs.d]#
[root@localhost certs.d]#
[root@localhost certs.d]# scp -r 192.168.95.71\:9443/ 192.168.95.69:/etc/containers/certs.d/
The authenticity of host '192.168.95.69 (192.168.95.69)' can't be established.
ED25519 key fingerprint is SHA256:UMisHyl9B2jg334LzycpJp3DJtnBXbUecdxQaO4qDdQ.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '192.168.95.69' (ED25519) to the list of known hosts.
root@192.168.95.69's password:
The authenticity of host '192.168.95.71 (192.168.95.71)' can't be established.
ED25519 key fingerprint is SHA256:b26YoekEpTFWNdTp6PNtAI14eUMa+XrDICmbeizT4ZQ.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '192.168.95.71' (ED25519) to the list of known hosts.
root@192.168.95.71's password:
File "9443" not found.
Failed to download file '9443'
[root@localhost certs.d]# ls
192.168.95.71:9443
[root@localhost certs.d]# scp -r 192.168.95.71\:9443/ root@192.168.95.69:/etc/containers/certs.d/
root@192.168.95.69's password:
root@192.168.95.71's password:
File "9443" not found.
Failed to download file '9443'
[root@localhost certs.d]# scp -r 192.168.95.71:9443 root@192.168.95.69:/etc/containers/certs.d/
root@192.168.95.69's password:
root@192.168.95.71's password:
File "9443" not found.
Failed to download file '9443'
[root@localhost certs.d]# scp -rp 192.168.95.71:9443 root@192.168.95.69:/etc/containers/certs.d/
root@192.168.95.69's password:
root@192.168.95.71's password:
File "9443" not found.
Failed to download file '9443'
[root@localhost certs.d]# scp -rp 192.168.95.71 root@192.168.95.69:/etc/containers/certs.d/
192.168.95.71:        192.168.95.71\:9443/
[root@localhost certs.d]# scp -rp 192.168.95.71\:9443/ root@192.168.95.69:/etc/containers/certs.d/
root@192.168.95.69's password:
root@192.168.95.71's password:
File "9443" not found.
Failed to download file '9443'
[root@localhost certs.d]# ls -ltrh
total 0
drwxr-xr-x. 2 root root 20 Jul 31 16:52 192.168.95.71:9443
[root@localhost certs.d]# pwd
/etc/containers/certs.d
[root@localhost certs.d]# scp -rp '192.168.95.71\:9443/' root@192.168.95.69:/etc/containers/certs.d/
root@192.168.95.69's password:
Permission denied, please try again.
root@192.168.95.69's password:
hostname contains invalid characters
Connection closed
[root@localhost certs.d]# scp -rp * root@192.168.95.69:/etc/containers/certs.d/
root@192.168.95.69's password:
root@192.168.95.71's password:
File "9443" not found.
Failed to download file '9443'
[root@localhost certs.d]# scp -rp "./192.168.95.71:9443" root@192.168.95.69:/etc/containers/certs.d/
root@192.168.95.69's password:
ca.crt                                                                                                                     100% 1359   140.6KB/s   00:00
[root@localhost certs.d]# scp -rp "./192.168.95.71:9443" root@192.168.95.70:/etc/containers/certs.d/
root@192.168.95.70's password:
ca.crt                                                                                                                     100% 1359     3.2MB/s   00:00
[root@localhost certs.d]# scp -rp "./192.168.95.71:9443" root@192.168.95.72:/etc/containers/certs.d/
The authenticity of host '192.168.95.72 (192.168.95.72)' can't be established.
ED25519 key fingerprint is SHA256:K+H3BFj7PV9Ewe6vouLMCuTzm3JltGbOemEQfBWWvL4.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '192.168.95.72' (ED25519) to the list of known hosts.
root@192.168.95.72's password:
ca.crt                                                                                                                     100% 1359     2.3MB/s   00:00
[root@localhost certs.d]# scp -rp "./192.168.95.71:9443" root@192.168.95.73:/etc/containers/certs.d/
The authenticity of host '192.168.95.73 (192.168.95.73)' can't be established.
ED25519 key fingerprint is SHA256:6AmRdL55F740oBG5S4K+WFjQz9yt6Fhg9Oq9Hj7DZ+k.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '192.168.95.73' (ED25519) to the list of known hosts.
root@192.168.95.73's password:
ca.crt                                                                                                                     100% 1359     2.3MB/s   00:00
[root@localhost certs.d]# scp -rp "./192.168.95.71:9443" root@192.168.95.74:/etc/containers/certs.d/
ssh: connect to host 192.168.95.74 port 22: No route to host
Connection closed
[root@localhost certs.d]# cd
[root@localhost ~]# cd k8s/
[root@localhost k8s]# ls
1                                         haproxy-2.4.22-4.el9.x86_64.rpm       kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm  kubeadm-1.32.7-150500.1.1.x86_64.rpm  libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm
containerd.io-1.7.27-3.1.el9.x86_64.rpm   kubectl-1.32.7-150500.1.1.x86_64.rpm  libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm
cri-tools-1.32.0-150500.1.1.x86_64.rpm    kubelet-1.32.7-150500.1.1.x86_64.rpm  libnetfilter_queue-1.0.5-1.el9.x86_64.rpm
[root@localhost k8s]# scp *.rpm root@192.168.95.74:~/k8s/
ssh: connect to host 192.168.95.74 port 22: No route to host
Connection closed
[root@localhost k8s]# scp *.rpm root@192.168.95.73:~/k8s/
root@192.168.95.73's password:
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm                                                                                   100%  240KB  20.2MB/s   00:00
containerd.io-1.7.27-3.1.el9.x86_64.rpm                                                                                    100%   44MB 155.4MB/s   00:00
cri-tools-1.32.0-150500.1.1.x86_64.rpm                                                                                     100% 7291KB 138.1MB/s   00:00
haproxy-2.4.22-4.el9.x86_64.rpm                                                                                            100% 2239KB 166.4MB/s   00:00
kubeadm-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   12MB 137.9MB/s   00:00
kubectl-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   11MB 174.5MB/s   00:00
kubelet-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   15MB 179.9MB/s   00:00
kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm                                                                                 100% 8216KB 108.8MB/s   00:00
libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm                                                                              100%   26KB  27.6MB/s   00:00
libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm                                                                             100%   25KB  28.4MB/s   00:00
libnetfilter_queue-1.0.5-1.el9.x86_64.rpm                                                                                  100%   31KB   8.8MB/s   00:00
[root@localhost k8s]# scp *.rpm root@192.168.95.72:~/k8s/
root@192.168.95.72's password:
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm                                                                                   100%  240KB  55.8MB/s   00:00
containerd.io-1.7.27-3.1.el9.x86_64.rpm                                                                                    100%   44MB 175.9MB/s   00:00
cri-tools-1.32.0-150500.1.1.x86_64.rpm                                                                                     100% 7291KB 175.9MB/s   00:00
haproxy-2.4.22-4.el9.x86_64.rpm                                                                                            100% 2239KB 159.9MB/s   00:00
kubeadm-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   12MB 161.0MB/s   00:00
kubectl-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   11MB 164.6MB/s   00:00
kubelet-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   15MB 159.1MB/s   00:00
kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm                                                                                 100% 8216KB 161.7MB/s   00:00
libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm                                                                              100%   26KB  53.0MB/s   00:00
libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm                                                                             100%   25KB  52.4MB/s   00:00
libnetfilter_queue-1.0.5-1.el9.x86_64.rpm                                                                                  100%   31KB  66.9MB/s   00:00
[root@localhost k8s]# scp *.rpm root@192.168.95.70:~/k8s/
root@192.168.95.70's password:
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm                                                                                   100%  240KB  92.4MB/s   00:00
containerd.io-1.7.27-3.1.el9.x86_64.rpm                                                                                    100%   44MB 199.5MB/s   00:00
cri-tools-1.32.0-150500.1.1.x86_64.rpm                                                                                     100% 7291KB 190.1MB/s   00:00
haproxy-2.4.22-4.el9.x86_64.rpm                                                                                            100% 2239KB 179.5MB/s   00:00
kubeadm-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   12MB 200.2MB/s   00:00
kubectl-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   11MB 221.9MB/s   00:00
kubelet-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   15MB 234.3MB/s   00:00
kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm                                                                                 100% 8216KB 211.2MB/s   00:00
libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm                                                                              100%   26KB  38.8MB/s   00:00
libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm                                                                             100%   25KB  41.2MB/s   00:00
libnetfilter_queue-1.0.5-1.el9.x86_64.rpm                                                                                  100%   31KB  46.5MB/s   00:00
[root@localhost k8s]# scp *.rpm root@192.168.95.69:~/k8s/
root@192.168.95.69's password:
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm                                                                                   100%  240KB  95.1MB/s   00:00
containerd.io-1.7.27-3.1.el9.x86_64.rpm                                                                                    100%   44MB 208.1MB/s   00:00
cri-tools-1.32.0-150500.1.1.x86_64.rpm                                                                                     100% 7291KB 193.9MB/s   00:00
haproxy-2.4.22-4.el9.x86_64.rpm                                                                                            100% 2239KB 181.4MB/s   00:00
kubeadm-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   12MB 201.2MB/s   00:00
kubectl-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   11MB 202.0MB/s   00:00
kubelet-1.32.7-150500.1.1.x86_64.rpm                                                                                       100%   15MB 198.6MB/s   00:00
kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm                                                                                 100% 8216KB 195.4MB/s   00:00
libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm                                                                              100%   26KB  46.9MB/s   00:00
libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm                                                                             100%   25KB  52.5MB/s   00:00
libnetfilter_queue-1.0.5-1.el9.x86_64.rpm                                                                                  100%   31KB  58.2MB/s   00:00
[root@localhost k8s]# ssh 192.168.95.69
root@192.168.95.69's password:
Permission denied, please try again.
root@192.168.95.69's password:
Activate the web console with: systemctl enable --now cockpit.socket

Last failed login: Thu Jul 31 17:52:25 IST 2025 from 192.168.95.71 on ssh:notty
There were 2 failed login attempts since the last successful login.
Last login: Thu Jul 31 15:10:52 2025 from 10.9.0.22
[root@internet-vm ~]# ls
anaconda-ks.cfg  java  jfrog  k8s  nginx  postgress
[root@internet-vm ~]# cd k8s/
[root@internet-vm k8s]# ls
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm  kubeadm-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm
containerd.io-1.7.27-3.1.el9.x86_64.rpm   kubectl-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm
cri-tools-1.32.0-150500.1.1.x86_64.rpm    kubelet-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_queue-1.0.5-1.el9.x86_64.rpm
haproxy-2.4.22-4.el9.x86_64.rpm           kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm
[root@internet-vm k8s]# rpm -ivh kubeadm-1.32.7-150500.1.1.x86_64.rpm
warning: kubeadm-1.32.7-150500.1.1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID 9a296436: NOKEY
error: Failed dependencies:
        cri-tools >= 1.30.0 is needed by kubeadm-1.32.7-150500.1.1.x86_64
[root@internet-vm k8s]# rpm -ihv cri-tools-1.32.0-150500.1.1.x86_64.rpm
warning: cri-tools-1.32.0-150500.1.1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID 9a296436: NOKEY
Verifying...                          ################################# [100%]
Preparing...                          ################################# [100%]
Updating / installing...
   1:cri-tools-1.32.0-150500.1.1      ################################# [100%]
[root@internet-vm k8s]# sudo yum localinstall *.rpm --disablerepo="*"
Updating Subscription Management repositories.
Package cri-tools-1.32.0-150500.1.1.x86_64 is already installed.
Dependencies resolved.
=============================================================================================================================================================
 Package                                      Architecture                 Version                                  Repository                          Size
=============================================================================================================================================================
Installing:
 conntrack-tools                              x86_64                       1.4.7-4.el9_5                            @commandline                       240 k
 containerd.io                                x86_64                       1.7.27-3.1.el9                           @commandline                        44 M
 haproxy                                      x86_64                       2.4.22-4.el9                             @commandline                       2.2 M
 kubeadm                                      x86_64                       1.32.7-150500.1.1                        @commandline                        12 M
 kubectl                                      x86_64                       1.32.7-150500.1.1                        @commandline                        11 M
 kubelet                                      x86_64                       1.32.7-150500.1.1                        @commandline                        15 M
 kubernetes-cni                               x86_64                       1.6.0-150500.1.1                         @commandline                       8.0 M
 libnetfilter_cthelper                        x86_64                       1.0.0-22.el9                             @commandline                        26 k
 libnetfilter_cttimeout                       x86_64                       1.0.0-19.el9                             @commandline                        25 k
 libnetfilter_queue                           x86_64                       1.0.5-1.el9                              @commandline                        31 k

Transaction Summary
=============================================================================================================================================================
Install  10 Packages

Total size: 92 M
Installed size: 421 M
Is this ok [y/N]: y
Downloading Packages:
Running transaction check
Transaction check succeeded.
Running transaction test
Transaction test succeeded.
Running transaction
  Preparing        :                                                                                                                                     1/1
  Installing       : libnetfilter_queue-1.0.5-1.el9.x86_64                                                                                              1/10
  Installing       : libnetfilter_cttimeout-1.0.0-19.el9.x86_64                                                                                         2/10
  Installing       : libnetfilter_cthelper-1.0.0-22.el9.x86_64                                                                                          3/10
  Installing       : conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               4/10
  Running scriptlet: conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               4/10
  Installing       : kubernetes-cni-1.6.0-150500.1.1.x86_64                                                                                             5/10
  Installing       : kubelet-1.32.7-150500.1.1.x86_64                                                                                                   6/10
  Running scriptlet: kubelet-1.32.7-150500.1.1.x86_64                                                                                                   6/10
  Installing       : kubectl-1.32.7-150500.1.1.x86_64                                                                                                   7/10
  Installing       : kubeadm-1.32.7-150500.1.1.x86_64                                                                                                   8/10
  Running scriptlet: haproxy-2.4.22-4.el9.x86_64                                                                                                        9/10
  Installing       : haproxy-2.4.22-4.el9.x86_64                                                                                                        9/10
  Running scriptlet: haproxy-2.4.22-4.el9.x86_64                                                                                                        9/10
  Installing       : containerd.io-1.7.27-3.1.el9.x86_64                                                                                               10/10
  Running scriptlet: containerd.io-1.7.27-3.1.el9.x86_64                                                                                               10/10
  Verifying        : conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               1/10
  Verifying        : containerd.io-1.7.27-3.1.el9.x86_64                                                                                                2/10
  Verifying        : haproxy-2.4.22-4.el9.x86_64                                                                                                        3/10
  Verifying        : kubeadm-1.32.7-150500.1.1.x86_64                                                                                                   4/10
  Verifying        : kubectl-1.32.7-150500.1.1.x86_64                                                                                                   5/10
  Verifying        : kubelet-1.32.7-150500.1.1.x86_64                                                                                                   6/10
  Verifying        : kubernetes-cni-1.6.0-150500.1.1.x86_64                                                                                             7/10
  Verifying        : libnetfilter_cthelper-1.0.0-22.el9.x86_64                                                                                          8/10
  Verifying        : libnetfilter_cttimeout-1.0.0-19.el9.x86_64                                                                                         9/10
  Verifying        : libnetfilter_queue-1.0.5-1.el9.x86_64                                                                                             10/10
Installed products updated.

Installed:
  conntrack-tools-1.4.7-4.el9_5.x86_64             containerd.io-1.7.27-3.1.el9.x86_64                 haproxy-2.4.22-4.el9.x86_64
  kubeadm-1.32.7-150500.1.1.x86_64                 kubectl-1.32.7-150500.1.1.x86_64                    kubelet-1.32.7-150500.1.1.x86_64
  kubernetes-cni-1.6.0-150500.1.1.x86_64           libnetfilter_cthelper-1.0.0-22.el9.x86_64           libnetfilter_cttimeout-1.0.0-19.el9.x86_64
  libnetfilter_queue-1.0.5-1.el9.x86_64

Complete!
[root@internet-vm k8s]# rpm -qa | grep kube
kubernetes-cni-1.6.0-150500.1.1.x86_64
kubelet-1.32.7-150500.1.1.x86_64
kubectl-1.32.7-150500.1.1.x86_64
kubeadm-1.32.7-150500.1.1.x86_64
[root@internet-vm k8s]# rpm -qa | grep conta
container-selinux-2.237.0-1.el9_6.noarch
containernetworking-plugins-1.6.2-2.el9_6.x86_64
containers-common-1-117.el9_6.x86_64
containers-common-extra-1-117.el9_6.x86_64
containerd.io-1.7.27-3.1.el9.x86_64
[root@internet-vm k8s]# exit
logout
Connection to 192.168.95.69 closed.
[root@localhost k8s]# ssh 192.168.95.70
root@192.168.95.70's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 16:42:55 2025 from 10.9.0.30
[root@JCR-vm ~]# cd k8s/
[root@JCR-vm k8s]# ls
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm  kubeadm-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm
containerd.io-1.7.27-3.1.el9.x86_64.rpm   kubectl-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm
cri-tools-1.32.0-150500.1.1.x86_64.rpm    kubelet-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_queue-1.0.5-1.el9.x86_64.rpm
haproxy-2.4.22-4.el9.x86_64.rpm           kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm
[root@JCR-vm k8s]# IP R
bash: IP: command not found...
Similar command is: 'ip'
[root@JCR-vm k8s]# ipr
bash: ipr: command not found...
[root@JCR-vm k8s]# ^C
[root@JCR-vm k8s]# ip r
default via 192.168.10.1 dev ens192
192.168.0.0/16 dev ens192 proto kernel scope link src 192.168.95.70 metric 100
[root@JCR-vm k8s]#  sudo yum localinstall *.rpm --disablerepo="*"
Updating Subscription Management repositories.
Dependencies resolved.
=============================================================================================================================================================
 Package                                      Architecture                 Version                                  Repository                          Size
=============================================================================================================================================================
Installing:
 conntrack-tools                              x86_64                       1.4.7-4.el9_5                            @commandline                       240 k
 containerd.io                                x86_64                       1.7.27-3.1.el9                           @commandline                        44 M
 cri-tools                                    x86_64                       1.32.0-150500.1.1                        @commandline                       7.1 M
 haproxy                                      x86_64                       2.4.22-4.el9                             @commandline                       2.2 M
 kubeadm                                      x86_64                       1.32.7-150500.1.1                        @commandline                        12 M
 kubectl                                      x86_64                       1.32.7-150500.1.1                        @commandline                        11 M
 kubelet                                      x86_64                       1.32.7-150500.1.1                        @commandline                        15 M
 kubernetes-cni                               x86_64                       1.6.0-150500.1.1                         @commandline                       8.0 M
 libnetfilter_cthelper                        x86_64                       1.0.0-22.el9                             @commandline                        26 k
 libnetfilter_cttimeout                       x86_64                       1.0.0-19.el9                             @commandline                        25 k
 libnetfilter_queue                           x86_64                       1.0.5-1.el9                              @commandline                        31 k

Transaction Summary
=============================================================================================================================================================
Install  11 Packages

Total size: 99 M
Installed size: 451 M
Is this ok [y/N]: y
Downloading Packages:
Running transaction check
Transaction check succeeded.
Running transaction test
Transaction test succeeded.
Running transaction
  Preparing        :                                                                                                                                     1/1
  Installing       : libnetfilter_queue-1.0.5-1.el9.x86_64                                                                                              1/11
  Installing       : libnetfilter_cttimeout-1.0.0-19.el9.x86_64                                                                                         2/11
  Installing       : libnetfilter_cthelper-1.0.0-22.el9.x86_64                                                                                          3/11
  Installing       : conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               4/11
  Running scriptlet: conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               4/11
  Installing       : kubernetes-cni-1.6.0-150500.1.1.x86_64                                                                                             5/11
  Installing       : cri-tools-1.32.0-150500.1.1.x86_64                                                                                                 6/11
  Installing       : kubeadm-1.32.7-150500.1.1.x86_64                                                                                                   7/11
  Installing       : kubelet-1.32.7-150500.1.1.x86_64                                                                                                   8/11
  Running scriptlet: kubelet-1.32.7-150500.1.1.x86_64                                                                                                   8/11
  Installing       : kubectl-1.32.7-150500.1.1.x86_64                                                                                                   9/11
  Running scriptlet: haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Installing       : haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Running scriptlet: haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Installing       : containerd.io-1.7.27-3.1.el9.x86_64                                                                                               11/11
  Running scriptlet: containerd.io-1.7.27-3.1.el9.x86_64                                                                                               11/11
  Verifying        : conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               1/11
  Verifying        : containerd.io-1.7.27-3.1.el9.x86_64                                                                                                2/11
  Verifying        : cri-tools-1.32.0-150500.1.1.x86_64                                                                                                 3/11
  Verifying        : haproxy-2.4.22-4.el9.x86_64                                                                                                        4/11
  Verifying        : kubeadm-1.32.7-150500.1.1.x86_64                                                                                                   5/11
  Verifying        : kubectl-1.32.7-150500.1.1.x86_64                                                                                                   6/11
  Verifying        : kubelet-1.32.7-150500.1.1.x86_64                                                                                                   7/11
  Verifying        : kubernetes-cni-1.6.0-150500.1.1.x86_64                                                                                             8/11
  Verifying        : libnetfilter_cthelper-1.0.0-22.el9.x86_64                                                                                          9/11
  Verifying        : libnetfilter_cttimeout-1.0.0-19.el9.x86_64                                                                                        10/11
  Verifying        : libnetfilter_queue-1.0.5-1.el9.x86_64                                                                                             11/11
Installed products updated.

Installed:
  conntrack-tools-1.4.7-4.el9_5.x86_64                 containerd.io-1.7.27-3.1.el9.x86_64              cri-tools-1.32.0-150500.1.1.x86_64
  haproxy-2.4.22-4.el9.x86_64                          kubeadm-1.32.7-150500.1.1.x86_64                 kubectl-1.32.7-150500.1.1.x86_64
  kubelet-1.32.7-150500.1.1.x86_64                     kubernetes-cni-1.6.0-150500.1.1.x86_64           libnetfilter_cthelper-1.0.0-22.el9.x86_64
  libnetfilter_cttimeout-1.0.0-19.el9.x86_64           libnetfilter_queue-1.0.5-1.el9.x86_64

Complete!
[root@JCR-vm k8s]# exit
logout
Connection to 192.168.95.70 closed.
[root@localhost k8s]# ssh 192.168.95.72
root@192.168.95.72's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
[root@localhost ~]# ls
anaconda-ks.cfg  k8s
[root@localhost ~]# cd k8s/
[root@localhost k8s]# ls
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm  kubeadm-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm
containerd.io-1.7.27-3.1.el9.x86_64.rpm   kubectl-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm
cri-tools-1.32.0-150500.1.1.x86_64.rpm    kubelet-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_queue-1.0.5-1.el9.x86_64.rpm
haproxy-2.4.22-4.el9.x86_64.rpm           kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm
[root@localhost k8s]#  sudo yum localinstall *.rpm --disablerepo="*"
Updating Subscription Management repositories.
Unable to read consumer identity

This system is not registered with an entitlement server. You can use "rhc" or "subscription-manager" to register.

No repository match: *
Dependencies resolved.
=============================================================================================================================================================
 Package                                      Architecture                 Version                                  Repository                          Size
=============================================================================================================================================================
Installing:
 conntrack-tools                              x86_64                       1.4.7-4.el9_5                            @commandline                       240 k
 containerd.io                                x86_64                       1.7.27-3.1.el9                           @commandline                        44 M
 cri-tools                                    x86_64                       1.32.0-150500.1.1                        @commandline                       7.1 M
 haproxy                                      x86_64                       2.4.22-4.el9                             @commandline                       2.2 M
 kubeadm                                      x86_64                       1.32.7-150500.1.1                        @commandline                        12 M
 kubectl                                      x86_64                       1.32.7-150500.1.1                        @commandline                        11 M
 kubelet                                      x86_64                       1.32.7-150500.1.1                        @commandline                        15 M
 kubernetes-cni                               x86_64                       1.6.0-150500.1.1                         @commandline                       8.0 M
 libnetfilter_cthelper                        x86_64                       1.0.0-22.el9                             @commandline                        26 k
 libnetfilter_cttimeout                       x86_64                       1.0.0-19.el9                             @commandline                        25 k
 libnetfilter_queue                           x86_64                       1.0.5-1.el9                              @commandline                        31 k

Transaction Summary
=============================================================================================================================================================
Install  11 Packages

Total size: 99 M
Installed size: 451 M
Is this ok [y/N]: yes
Downloading Packages:
Running transaction check
Transaction check succeeded.
Running transaction test
Transaction test succeeded.
Running transaction
  Preparing        :                                                                                                                                     1/1
  Installing       : libnetfilter_queue-1.0.5-1.el9.x86_64                                                                                              1/11
  Installing       : libnetfilter_cttimeout-1.0.0-19.el9.x86_64                                                                                         2/11
  Installing       : libnetfilter_cthelper-1.0.0-22.el9.x86_64                                                                                          3/11
  Installing       : conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               4/11
  Running scriptlet: conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               4/11
  Installing       : kubernetes-cni-1.6.0-150500.1.1.x86_64                                                                                             5/11
  Installing       : cri-tools-1.32.0-150500.1.1.x86_64                                                                                                 6/11
  Installing       : kubeadm-1.32.7-150500.1.1.x86_64                                                                                                   7/11
  Installing       : kubelet-1.32.7-150500.1.1.x86_64                                                                                                   8/11
  Running scriptlet: kubelet-1.32.7-150500.1.1.x86_64                                                                                                   8/11
  Installing       : kubectl-1.32.7-150500.1.1.x86_64                                                                                                   9/11
  Running scriptlet: haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Installing       : haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Running scriptlet: haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Installing       : containerd.io-1.7.27-3.1.el9.x86_64                                                                                               11/11
  Running scriptlet: containerd.io-1.7.27-3.1.el9.x86_64                                                                                               11/11
  Verifying        : conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               1/11
  Verifying        : containerd.io-1.7.27-3.1.el9.x86_64                                                                                                2/11
  Verifying        : cri-tools-1.32.0-150500.1.1.x86_64                                                                                                 3/11
  Verifying        : haproxy-2.4.22-4.el9.x86_64                                                                                                        4/11
  Verifying        : kubeadm-1.32.7-150500.1.1.x86_64                                                                                                   5/11
  Verifying        : kubectl-1.32.7-150500.1.1.x86_64                                                                                                   6/11
  Verifying        : kubelet-1.32.7-150500.1.1.x86_64                                                                                                   7/11
  Verifying        : kubernetes-cni-1.6.0-150500.1.1.x86_64                                                                                             8/11
  Verifying        : libnetfilter_cthelper-1.0.0-22.el9.x86_64                                                                                          9/11
  Verifying        : libnetfilter_cttimeout-1.0.0-19.el9.x86_64                                                                                        10/11
  Verifying        : libnetfilter_queue-1.0.5-1.el9.x86_64                                                                                             11/11
Installed products updated.

Installed:
  conntrack-tools-1.4.7-4.el9_5.x86_64                 containerd.io-1.7.27-3.1.el9.x86_64              cri-tools-1.32.0-150500.1.1.x86_64
  haproxy-2.4.22-4.el9.x86_64                          kubeadm-1.32.7-150500.1.1.x86_64                 kubectl-1.32.7-150500.1.1.x86_64
  kubelet-1.32.7-150500.1.1.x86_64                     kubernetes-cni-1.6.0-150500.1.1.x86_64           libnetfilter_cthelper-1.0.0-22.el9.x86_64
  libnetfilter_cttimeout-1.0.0-19.el9.x86_64           libnetfilter_queue-1.0.5-1.el9.x86_64

Complete!
[root@localhost k8s]# exit
logout
Connection to 192.168.95.72 closed.
[root@localhost k8s]# ssh 192.168.95.73
root@192.168.95.73's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 17:49:48 2025 from 10.9.0.30
[root@localhost ~]# cd k8s/
[root@localhost k8s]#  sudo yum localinstall *.rpm --disablerepo="*"
Updating Subscription Management repositories.
Unable to read consumer identity

This system is not registered with an entitlement server. You can use "rhc" or "subscription-manager" to register.

No repository match: *
Dependencies resolved.
=============================================================================================================================================================
 Package                                      Architecture                 Version                                  Repository                          Size
=============================================================================================================================================================
Installing:
 conntrack-tools                              x86_64                       1.4.7-4.el9_5                            @commandline                       240 k
 containerd.io                                x86_64                       1.7.27-3.1.el9                           @commandline                        44 M
 cri-tools                                    x86_64                       1.32.0-150500.1.1                        @commandline                       7.1 M
 haproxy                                      x86_64                       2.4.22-4.el9                             @commandline                       2.2 M
 kubeadm                                      x86_64                       1.32.7-150500.1.1                        @commandline                        12 M
 kubectl                                      x86_64                       1.32.7-150500.1.1                        @commandline                        11 M
 kubelet                                      x86_64                       1.32.7-150500.1.1                        @commandline                        15 M
 kubernetes-cni                               x86_64                       1.6.0-150500.1.1                         @commandline                       8.0 M
 libnetfilter_cthelper                        x86_64                       1.0.0-22.el9                             @commandline                        26 k
 libnetfilter_cttimeout                       x86_64                       1.0.0-19.el9                             @commandline                        25 k
 libnetfilter_queue                           x86_64                       1.0.5-1.el9                              @commandline                        31 k

Transaction Summary
=============================================================================================================================================================
Install  11 Packages

Total size: 99 M
Installed size: 451 M
Is this ok [y/N]: y
Downloading Packages:
Running transaction check
Transaction check succeeded.
Running transaction test
Transaction test succeeded.
Running transaction
  Preparing        :                                                                                                                                     1/1
  Installing       : libnetfilter_queue-1.0.5-1.el9.x86_64                                                                                              1/11
  Installing       : libnetfilter_cttimeout-1.0.0-19.el9.x86_64                                                                                         2/11
  Installing       : libnetfilter_cthelper-1.0.0-22.el9.x86_64                                                                                          3/11
  Installing       : conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               4/11
  Running scriptlet: conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               4/11
  Installing       : kubernetes-cni-1.6.0-150500.1.1.x86_64                                                                                             5/11
  Installing       : cri-tools-1.32.0-150500.1.1.x86_64                                                                                                 6/11
  Installing       : kubeadm-1.32.7-150500.1.1.x86_64                                                                                                   7/11
  Installing       : kubelet-1.32.7-150500.1.1.x86_64                                                                                                   8/11
  Running scriptlet: kubelet-1.32.7-150500.1.1.x86_64                                                                                                   8/11
  Installing       : kubectl-1.32.7-150500.1.1.x86_64                                                                                                   9/11
  Running scriptlet: haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Installing       : haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Running scriptlet: haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Installing       : containerd.io-1.7.27-3.1.el9.x86_64                                                                                               11/11
  Running scriptlet: containerd.io-1.7.27-3.1.el9.x86_64                                                                                               11/11
  Verifying        : conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               1/11
  Verifying        : containerd.io-1.7.27-3.1.el9.x86_64                                                                                                2/11
  Verifying        : cri-tools-1.32.0-150500.1.1.x86_64                                                                                                 3/11
  Verifying        : haproxy-2.4.22-4.el9.x86_64                                                                                                        4/11
  Verifying        : kubeadm-1.32.7-150500.1.1.x86_64                                                                                                   5/11
  Verifying        : kubectl-1.32.7-150500.1.1.x86_64                                                                                                   6/11
  Verifying        : kubelet-1.32.7-150500.1.1.x86_64                                                                                                   7/11
  Verifying        : kubernetes-cni-1.6.0-150500.1.1.x86_64                                                                                             8/11
  Verifying        : libnetfilter_cthelper-1.0.0-22.el9.x86_64                                                                                          9/11
  Verifying        : libnetfilter_cttimeout-1.0.0-19.el9.x86_64                                                                                        10/11
  Verifying        : libnetfilter_queue-1.0.5-1.el9.x86_64                                                                                             11/11
Installed products updated.

Installed:
  conntrack-tools-1.4.7-4.el9_5.x86_64                 containerd.io-1.7.27-3.1.el9.x86_64              cri-tools-1.32.0-150500.1.1.x86_64
  haproxy-2.4.22-4.el9.x86_64                          kubeadm-1.32.7-150500.1.1.x86_64                 kubectl-1.32.7-150500.1.1.x86_64
  kubelet-1.32.7-150500.1.1.x86_64                     kubernetes-cni-1.6.0-150500.1.1.x86_64           libnetfilter_cthelper-1.0.0-22.el9.x86_64
  libnetfilter_cttimeout-1.0.0-19.el9.x86_64           libnetfilter_queue-1.0.5-1.el9.x86_64

Complete!
[root@localhost k8s]# exit
logout
Connection to 192.168.95.73 closed.
[root@localhost k8s]# ip r
default via 192.168.10.1 dev ens192 proto static metric 100
192.168.0.0/16 dev ens192 proto kernel scope link src 192.168.95.71 metric 100
[root@localhost k8s]# rpm -qa | grep kube
[root@localhost k8s]#  sudo yum localinstall *.rpm --disablerepo="*"
Updating Subscription Management repositories.
Repository kubernetes is listed more than once in the configuration
Dependencies resolved.
=============================================================================================================================================================
 Package                                      Architecture                 Version                                  Repository                          Size
=============================================================================================================================================================
Installing:
 conntrack-tools                              x86_64                       1.4.7-4.el9_5                            @commandline                       240 k
 containerd.io                                x86_64                       1.7.27-3.1.el9                           @commandline                        44 M
 cri-tools                                    x86_64                       1.32.0-150500.1.1                        @commandline                       7.1 M
 haproxy                                      x86_64                       2.4.22-4.el9                             @commandline                       2.2 M
 kubeadm                                      x86_64                       1.32.7-150500.1.1                        @commandline                        12 M
 kubectl                                      x86_64                       1.32.7-150500.1.1                        @commandline                        11 M
 kubelet                                      x86_64                       1.32.7-150500.1.1                        @commandline                        15 M
 kubernetes-cni                               x86_64                       1.6.0-150500.1.1                         @commandline                       8.0 M
 libnetfilter_cthelper                        x86_64                       1.0.0-22.el9                             @commandline                        26 k
 libnetfilter_cttimeout                       x86_64                       1.0.0-19.el9                             @commandline                        25 k
 libnetfilter_queue                           x86_64                       1.0.5-1.el9                              @commandline                        31 k

Transaction Summary
=============================================================================================================================================================
Install  11 Packages

Total size: 99 M
Installed size: 451 M
Is this ok [y/N]: y
Downloading Packages:
Running transaction check
Transaction check succeeded.
Running transaction test
Transaction test succeeded.
Running transaction
  Preparing        :                                                                                                                                     1/1
  Installing       : libnetfilter_queue-1.0.5-1.el9.x86_64                                                                                              1/11
  Installing       : libnetfilter_cttimeout-1.0.0-19.el9.x86_64                                                                                         2/11
  Installing       : libnetfilter_cthelper-1.0.0-22.el9.x86_64                                                                                          3/11
  Installing       : conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               4/11
  Running scriptlet: conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               4/11
  Installing       : kubernetes-cni-1.6.0-150500.1.1.x86_64                                                                                             5/11
  Installing       : cri-tools-1.32.0-150500.1.1.x86_64                                                                                                 6/11
  Installing       : kubeadm-1.32.7-150500.1.1.x86_64                                                                                                   7/11
  Installing       : kubelet-1.32.7-150500.1.1.x86_64                                                                                                   8/11
  Running scriptlet: kubelet-1.32.7-150500.1.1.x86_64                                                                                                   8/11
  Installing       : kubectl-1.32.7-150500.1.1.x86_64                                                                                                   9/11
  Running scriptlet: haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Installing       : haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Running scriptlet: haproxy-2.4.22-4.el9.x86_64                                                                                                       10/11
  Installing       : containerd.io-1.7.27-3.1.el9.x86_64                                                                                               11/11
  Running scriptlet: containerd.io-1.7.27-3.1.el9.x86_64                                                                                               11/11
  Verifying        : conntrack-tools-1.4.7-4.el9_5.x86_64                                                                                               1/11
  Verifying        : containerd.io-1.7.27-3.1.el9.x86_64                                                                                                2/11
  Verifying        : cri-tools-1.32.0-150500.1.1.x86_64                                                                                                 3/11
  Verifying        : haproxy-2.4.22-4.el9.x86_64                                                                                                        4/11
  Verifying        : kubeadm-1.32.7-150500.1.1.x86_64                                                                                                   5/11
  Verifying        : kubectl-1.32.7-150500.1.1.x86_64                                                                                                   6/11
  Verifying        : kubelet-1.32.7-150500.1.1.x86_64                                                                                                   7/11
  Verifying        : kubernetes-cni-1.6.0-150500.1.1.x86_64                                                                                             8/11
  Verifying        : libnetfilter_cthelper-1.0.0-22.el9.x86_64                                                                                          9/11
  Verifying        : libnetfilter_cttimeout-1.0.0-19.el9.x86_64                                                                                        10/11
  Verifying        : libnetfilter_queue-1.0.5-1.el9.x86_64                                                                                             11/11
Installed products updated.

Installed:
  conntrack-tools-1.4.7-4.el9_5.x86_64                 containerd.io-1.7.27-3.1.el9.x86_64              cri-tools-1.32.0-150500.1.1.x86_64
  haproxy-2.4.22-4.el9.x86_64                          kubeadm-1.32.7-150500.1.1.x86_64                 kubectl-1.32.7-150500.1.1.x86_64
  kubelet-1.32.7-150500.1.1.x86_64                     kubernetes-cni-1.6.0-150500.1.1.x86_64           libnetfilter_cthelper-1.0.0-22.el9.x86_64
  libnetfilter_cttimeout-1.0.0-19.el9.x86_64           libnetfilter_queue-1.0.5-1.el9.x86_64

Complete!
[root@localhost k8s]# ls
1
[root@localhost k8s]# rpm -qa | grep kube
kubernetes-cni-1.6.0-150500.1.1.x86_64
kubeadm-1.32.7-150500.1.1.x86_64
kubelet-1.32.7-150500.1.1.x86_64
kubectl-1.32.7-150500.1.1.x86_64
[root@localhost k8s]# sudo hostnamectl set-hostname <hostname>
sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab
-bash: syntax error near unexpected token `newline'
^C
[root@localhost k8s]#
sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab
^C
[root@localhost k8s]# sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab
[root@localhost k8s]# ssh 192.168.95.69
root@192.168.95.69's password:
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Thu Jul 31 17:52:27 2025 from 192.168.95.71
[root@internet-vm ~]# sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab
[root@internet-vm ~]# ^C
[root@internet-vm ~]# exit
logout
Connection to 192.168.95.69 closed.
[root@localhost k8s]# ssh 192.168.95.70
root@192.168.95.70's password:
Permission denied, please try again.
root@192.168.95.70's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last failed login: Thu Jul 31 18:06:17 IST 2025 from 192.168.95.71 on ssh:notty
There was 1 failed login attempt since the last successful login.
Last login: Thu Jul 31 17:55:55 2025 from 192.168.95.71
[root@JCR-vm ~]# sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab
[root@JCR-vm ~]# exit
logout
Connection to 192.168.95.70 closed.
[root@localhost k8s]# ssh 192.168.95.72
root@192.168.95.72's password:
Permission denied, please try again.
root@192.168.95.72's password:
Permission denied, please try again.
root@192.168.95.72's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last failed login: Thu Jul 31 18:07:42 IST 2025 from 192.168.95.71 on ssh:notty
There were 2 failed login attempts since the last successful login.
Last login: Thu Jul 31 18:00:28 2025 from 192.168.95.71
[root@localhost ~]# sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab
[root@localhost ~]# exit
logout
Connection to 192.168.95.72 closed.
[root@localhost k8s]# ssh 192.168.95.73
root@192.168.95.73's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 18:01:29 2025 from 192.168.95.71
[root@localhost ~]# sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab
[root@localhost ~]# exit
logout
Connection to 192.168.95.73 closed.
[root@localhost k8s]# free -h
               total        used        free      shared  buff/cache   available
Mem:           3.6Gi       3.5Gi       136Mi        54Mi       237Mi       110Mi
Swap:             0B          0B          0B
[root@localhost k8s]# swap -l
bash: swap: command not found...
Similar command is: 'swapon'
[root@localhost k8s]# cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF
br_netfilter
[root@localhost k8s]# ip r
default via 192.168.10.1 dev ens192 proto static metric 100
192.168.0.0/16 dev ens192 proto kernel scope link src 192.168.95.71 metric 100
[root@localhost k8s]# cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
[root@localhost k8s]# sudo sysctl --system
* Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...
* Applying /usr/lib/sysctl.d/50-coredump.conf ...
* Applying /usr/lib/sysctl.d/50-default.conf ...
* Applying /usr/lib/sysctl.d/50-libkcapi-optmem_max.conf ...
* Applying /usr/lib/sysctl.d/50-pid-max.conf ...
* Applying /usr/lib/sysctl.d/50-redhat.conf ...
* Applying /etc/sysctl.d/99-sysctl.conf ...
* Applying /etc/sysctl.d/k8s.conf ...
* Applying /etc/sysctl.conf ...
kernel.yama.ptrace_scope = 0
kernel.core_pattern = |/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h
kernel.core_pipe_limit = 16
fs.suid_dumpable = 2
kernel.sysrq = 16
kernel.core_uses_pid = 1
net.ipv4.conf.default.rp_filter = 2
net.ipv4.conf.ens192.rp_filter = 2
net.ipv4.conf.lo.rp_filter = 2
net.ipv4.conf.default.accept_source_route = 0
net.ipv4.conf.ens192.accept_source_route = 0
net.ipv4.conf.lo.accept_source_route = 0
net.ipv4.conf.default.promote_secondaries = 1
net.ipv4.conf.ens192.promote_secondaries = 1
net.ipv4.conf.lo.promote_secondaries = 1
net.ipv4.ping_group_range = 0 2147483647
net.core.default_qdisc = fq_codel
fs.protected_hardlinks = 1
fs.protected_symlinks = 1
fs.protected_regular = 1
fs.protected_fifos = 1
net.core.optmem_max = 81920
kernel.pid_max = 4194304
kernel.kptr_restrict = 1
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.ens192.rp_filter = 1
net.ipv4.conf.lo.rp_filter = 1
net.ipv4.ip_forward = 1
[root@localhost k8s]# ssh 192.168.95.69
root@192.168.95.69's password:
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Thu Jul 31 18:05:48 2025 from 192.168.95.71
[root@internet-vm ~]# cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF
br_netfilter
[root@internet-vm ~]# cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
[root@internet-vm ~]# exit
logout
Connection to 192.168.95.69 closed.
[root@localhost k8s]# ssh 192.168.95.70
root@192.168.95.70's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 18:06:21 2025 from 192.168.95.71
[root@JCR-vm ~]# cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF
br_netfilter
[root@JCR-vm ~]#
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
[root@JCR-vm ~]# exit
logout
Connection to 192.168.95.70 closed.
[root@localhost k8s]# ssh 192.168.95.72
root@192.168.95.72's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 18:07:45 2025 from 192.168.95.71
[root@localhost ~]#
[root@localhost ~]# cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF
br_netfilter
[root@localhost ~]# cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
[root@localhost ~]# exit
logout
Connection to 192.168.95.72 closed.
[root@localhost k8s]# ssh 192.168.95.73
root@192.168.95.73's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 18:08:01 2025 from 192.168.95.71
[root@localhost ~]# cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF
br_netfilter
[root@localhost ~]# cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
[root@localhost ~]# exit
logout
Connection to 192.168.95.73 closed.
[root@localhost k8s]# cat /etc/containerd/config.toml
#   Copyright 2018-2022 Docker Inc.

#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at

#       http://www.apache.org/licenses/LICENSE-2.0

#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.

disabled_plugins = ["cri"]

#root = "/var/lib/containerd"
#state = "/run/containerd"
#subreaper = true
#oom_score = 0

#[grpc]
#  address = "/run/containerd/containerd.sock"
#  uid = 0
#  gid = 0

#[debug]
#  address = "/run/containerd/debug.sock"
#  uid = 0
#  gid = 0
#  level = "info"
[root@localhost k8s]# cat /etc/containerd/config.toml | grep cgrou
[root@localhost k8s]# cat /etc/containerd/config.toml | grep cgroup
[root@localhost k8s]# cat /etc/containerd/config.toml | grep -i C
#   Copyright 2018-2022 Docker Inc.
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#       http://www.apache.org/licenses/LICENSE-2.0
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.
disabled_plugins = ["cri"]
#root = "/var/lib/containerd"
#state = "/run/containerd"
#oom_score = 0
#[grpc]
#  address = "/run/containerd/containerd.sock"
#  address = "/run/containerd/debug.sock"
[root@localhost k8s]# cat /etc/containerd/config.toml | grep -i Cg
[root@localhost k8s]# cat /etc/containerd/config.toml | grep -i Cgroup
[root@localhost k8s]# systemctl enable --now containerd.service
Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /usr/lib/systemd/system/containerd.service.
[root@localhost k8s]# systemctl enable --now kubelet
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /usr/lib/systemd/system/kubelet.service.
[root@localhost k8s]# setenforce 0
[root@localhost k8s]# ^C
[root@localhost k8s]# ssh 192.168.95.69
root@192.168.95.69's password:
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Thu Jul 31 18:10:06 2025 from 192.168.95.71
[root@internet-vm ~]# systemctl enable --now containerd.service
Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /usr/lib/systemd/system/containerd.service.
[root@internet-vm ~]# systemctl enable --now kubelet
setenforce 0
systemctl stop firewalld
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /usr/lib/systemd/system/kubelet.service.
[root@internet-vm ~]# exit
logout
Connection to 192.168.95.69 closed.
[root@localhost k8s]# ssh 192.168.95.70
root@192.168.95.70's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 18:10:58 2025 from 192.168.95.71
[root@JCR-vm ~]# systemctl enable --now containerd.service
systemctl enable --now kubelet
setenforce 0
systemctl stop firewalld


Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /usr/lib/systemd/system/containerd.service.
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /usr/lib/systemd/system/kubelet.service.
[root@JCR-vm ~]# exit
logout
Connection to 192.168.95.70 closed.
[root@localhost k8s]# ssh 192.168.95.72
root@192.168.95.72's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 18:11:23 2025 from 192.168.95.71
[root@localhost ~]# systemctl enable --now containerd.service
systemctl enable --now kubelet
setenforce 0
systemctl stop firewalld


Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /usr/lib/systemd/system/containerd.service.
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /usr/lib/systemd/system/kubelet.service.
[root@localhost ~]# exit
logout
Connection to 192.168.95.72 closed.
[root@localhost k8s]# ssh 192.168.95.73
root@192.168.95.73's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 18:11:54 2025 from 192.168.95.71
[root@localhost ~]# systemctl enable --now containerd.service
systemctl enable --now kubelet
setenforce 0
systemctl stop firewalld


Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /usr/lib/systemd/system/containerd.service.
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /usr/lib/systemd/system/kubelet.service.
[root@localhost ~]# exit
logout
Connection to 192.168.95.73 closed.
[root@localhost k8s]# systemctl enable --now containerd.service
systemctl enable --now kubelet
setenforce 0
systemctl stop firewalld


[root@localhost k8s]# ip r
default via 192.168.10.1 dev ens192 proto static metric 100
192.168.0.0/16 dev ens192 proto kernel scope link src 192.168.95.71 metric 100
[root@localhost k8s]# ssh 192.168.95.69
root@192.168.95.69's password:
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Thu Jul 31 18:16:57 2025 from 192.168.95.71
[root@internet-vm ~]# exit
logout
Connection to 192.168.95.69 closed.
[root@localhost k8s]# ls
1
[root@localhost k8s]# ll
total 4
-rw-r--r--. 1 root root 236 Jul 31 17:28 1
[root@localhost k8s]# ip r
default via 192.168.10.1 dev ens192 proto static metric 100
192.168.0.0/16 dev ens192 proto kernel scope link src 192.168.95.71 metric 100
[root@localhost k8s]# ls
1
[root@localhost k8s]# cd ..
[root@localhost ~]# ls
anaconda-ks.cfg  jfrog-artifactory-jcr-7.98.19.rpm  k8s
[root@localhost ~]# cd k8s/
[root@localhost k8s]# ls
1
[root@localhost k8s]# ssh 192.168.95.69
root@192.168.95.69's password:
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Thu Jul 31 18:19:19 2025 from 192.168.95.71
[root@internet-vm ~]# cd k8s/
[root@internet-vm k8s]# ls
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm  kubeadm-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm
containerd.io-1.7.27-3.1.el9.x86_64.rpm   kubectl-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm
cri-tools-1.32.0-150500.1.1.x86_64.rpm    kubelet-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_queue-1.0.5-1.el9.x86_64.rpm
haproxy-2.4.22-4.el9.x86_64.rpm           kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm
[root@internet-vm k8s]# exit
logout
Connection to 192.168.95.69 closed.
[root@localhost k8s]# yum install haproxy
Updating Subscription Management repositories.
Repository kubernetes is listed more than once in the configuration
Last metadata expiration check: 0:42:19 ago on Thursday 31 July 2025 05:38:24 PM.
^C^C^C^C^CKeyboardInterrupt: Terminated.
[root@localhost k8s]#




============================================================



login as: root
root@192.168.95.71's password:
Access denied
root@192.168.95.71's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last failed login: Thu Jul 31 16:11:08 IST 2025 from 10.9.0.30 on ssh:notty
There was 1 failed login attempt since the last successful login.
Last login: Thu Jul 31 16:04:57 2025 from 10.9.0.30
[root@localhost ~]# netstat -tunlp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      1079/cupsd
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1081/sshd: /usr/sbi
tcp6       0      0 :::8081                 :::*                    LISTEN      22328/java
tcp6       0      0 ::1:631                 :::*                    LISTEN      1079/cupsd
tcp6       0      0 127.0.0.1:8040          :::*                    LISTEN      21416/java
tcp6       0      0 127.0.0.1:8016          :::*                    LISTEN      21416/java
tcp6       0      0 127.0.0.1:8015          :::*                    LISTEN      22328/java
tcp6       0      0 127.0.0.1:8091          :::*                    LISTEN      22328/java
tcp6       0      0 :::22                   :::*                    LISTEN      1081/sshd: /usr/sbi
udp        0      0 127.0.0.1:323           0.0.0.0:*                           923/chronyd
udp        0      0 0.0.0.0:44235           0.0.0.0:*                           855/avahi-daemon: r
udp        0      0 0.0.0.0:5353            0.0.0.0:*                           855/avahi-daemon: r
udp6       0      0 :::34988                :::*                                855/avahi-daemon: r
udp6       0      0 ::1:323                 :::*                                923/chronyd
udp6       0      0 :::5353                 :::*                                855/avahi-daemon: r
[root@localhost ~]# systemctl stop artifactory.service
[root@localhost ~]#
[root@localhost ~]#
[root@localhost ~]# netstat -tunlp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      1079/cupsd
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1081/sshd: /usr/sbi
tcp6       0      0 ::1:631                 :::*                    LISTEN      1079/cupsd
tcp6       0      0 :::22                   :::*                    LISTEN      1081/sshd: /usr/sbi
udp        0      0 127.0.0.1:323           0.0.0.0:*                           923/chronyd
udp        0      0 0.0.0.0:44235           0.0.0.0:*                           855/avahi-daemon: r
udp        0      0 0.0.0.0:5353            0.0.0.0:*                           855/avahi-daemon: r
udp6       0      0 :::34988                :::*                                855/avahi-daemon: r
udp6       0      0 ::1:323                 :::*                                923/chronyd
udp6       0      0 :::5353                 :::*                                855/avahi-daemon: r
[root@localhost ~]#
[root@localhost ~]#
[root@localhost ~]#
[root@localhost ~]#
[root@localhost ~]#
[root@localhost ~]# systemctl start artifactory.service
[root@localhost ~]# cd /opt/jfrog/artifactory/var/log/
[root@localhost log]# tail -f console.log
2025-07-31T10:46:59.245Z [shell] [INFO ] [] [installerCommon.sh:3518       ] [main] - Found JF_SHARED_NODE_ID set as environment variable with value : localhost.localdomain
2025-07-31T10:46:59.284Z [shell] [INFO ] [] [installerCommon.sh:3518       ] [main] - Found JF_SHARED_NODE_IP set as environment variable with value : [::1]
2025-07-31T10:46:59.318Z [shell] [INFO ] [] [installerCommon.sh:3518       ] [main] - Found JF_SHARED_NODE_NAME set as environment variable with value : localhost
2025-07-31T10:46:59.590Z [shell] [INFO ] [] [nativeCommon.sh:75            ] [main] - Using Tomcat template to generate : /opt/jfrog/artifactory/app/access/tomcat/conf/server.xml
2025-07-31T10:46:59.662Z [shell] [INFO ] [] [systemYamlHelper.sh:981       ] [main] - Resolved ${access.http.port||8040} to default value : 8040
2025-07-31T10:46:59.715Z [shell] [INFO ] [] [systemYamlHelper.sh:981       ] [main] - Resolved ${access.tomcat.connector.sendReasonPhrase||false} to default value : false
2025-07-31T10:46:59.769Z [shell] [INFO ] [] [systemYamlHelper.sh:981       ] [main] - Resolved ${access.tomcat.connector.maxThreads||50} to default value : 50
2025-07-31T10:46:59.875Z [shell] [INFO ] [] [systemYamlHelper.sh:621       ] [main] - Resolved JF_PRODUCT_HOME (/opt/jfrog/artifactory) from environment variable
2025-07-31T10:46:59.966Z [shell] [INFO ] [] [systemYamlHelper.sh:981       ] [main] - Resolved ${access.tomcat.workDir||/opt/jfrog/artifactory/var/work/access/tomcat} to default value : /opt/jfrog/artifactory/var/work/access/tomcat
2025-07-31T16:17:00.728L [tomct] [WARNING] [                ] [org.apache.tomcat.util.digester.Digester] [org.apache.tomcat.util.digester.SetPropertiesRule begin] - Match [Server/Service/Connector] failed to set property [sendReasonPhrase] to [false]
Logging configuration has both console=true and filepath='router-service.log'; ignoring console.
2025-07-31T10:47:01.148Z [jfrou] [INFO ] [6e7e9dc742434a25] [bootstrap.go:91               ] [main                ] [] - Router (jfrou) service initialization started. Version: 7.135.2-1 Revision: 3089f1f73bb276c2f6d51d62c1651ac81baf70d4 PID: 36493 Home: /opt/jfrog/artifactory
2025-07-31T10:47:01.148Z [jfrou] [INFO ] [6e7e9dc742434a25] [bootstrap.go:94               ] [main                ] [] - JFrog Router IP: [::1]
2025-07-31T10:47:01.150Z [jfrou] [INFO ] [6e7e9dc742434a25] [bootstrap.go:95               ] [main                ] [] - Effective configuration:
access (File): <nil>
access.http.port (Default Value): 8040
artifactory.pid (Environment Variable: JF_ARTIFACTORY_PID): /var/run/artifactory.pid
configVersion (File): 1
product.home (Environment Variable: JF_PRODUCT_HOME): /opt/jfrog/artifactory
router.autoMemLimitRatio (Default Value): 0.9
router.availabilityZoneOptimization.loggingIntervalSecs (Default Value): 600
router.encryptSystemConfig (Default Value): true
router.entrypoints.externalHost (Default Value):
router.entrypoints.externalMaxConcurrentStreams (Default Value): 500
router.entrypoints.externalPort (Default Value): 8082
router.entrypoints.grpcPort (Default Value): 8047
router.entrypoints.hotPathLogEnabled (Default Value): true
router.entrypoints.internalMaxConcurrentStreams (Default Value): 500
router.entrypoints.internalPort (Default Value): 8046
router.entrypoints.traefikApiPort (Default Value): 8049
router.httpclient.requestTimeoutSecs (Default Value): 30
router.lifecycle.shutdown.entrypointsGraceTimeout (Default Value): 10s
router.lifecycle.shutdown.internalJobsGraceTimeout (Default Value): 7s
router.logging.application.caller (Default Value): false
router.logging.application.console (Default Value): true
router.logging.application.filePath (Default Value): router-service.log
router.logging.application.format (Default Value): jftext
router.logging.application.level (Default Value): info
router.logging.application.rotation.compress (Default Value): true
router.logging.application.rotation.keepLastDecompressed (Default Value): 1
router.logging.application.rotation.maxAgeDays (Default Value): 0
router.logging.application.rotation.maxFiles (Default Value): 10
router.logging.application.rotation.maxSizeMb (Default Value): 25
router.logging.consoleLog.format (Default Value): jftext
router.logging.request.filePath (Default Value): router-request.log
router.logging.request.forwardedForLogging.enabled (Default Value): false
router.logging.request.forwardedForLogging.header (Default Value): X-Forwarded-For
router.logging.request.rotation.compress (Default Value): true
router.logging.request.rotation.keepLastDecompressed (Default Value): 1
router.logging.request.rotation.maxAgeDays (Default Value): 0
router.logging.request.rotation.maxFiles (Default Value): 10
router.logging.request.rotation.maxSizeMb (Default Value): 100
router.logging.request.tokenIdLoggingEnabled (Default Value): false
router.logging.request.verbose (Default Value): false
router.logging.traefik.caller (Default Value): false
router.logging.traefik.console (Default Value): false
router.logging.traefik.filePath (Default Value): router-traefik.log
router.logging.traefik.format (Default Value): jftext
router.logging.traefik.level (Default Value): info
router.logging.traefik.rotation.compress (Default Value): true
router.logging.traefik.rotation.keepLastDecompressed (Default Value): 1
router.logging.traefik.rotation.maxAgeDays (Default Value): 0
router.logging.traefik.rotation.maxFiles (Default Value): 10
router.logging.traefik.rotation.maxSizeMb (Default Value): 25
router.masterKeyRemoval.heartbeatCount (Default Value): 3
router.masterKeyRemoval.intervalSecs (Default Value): 60
router.masterKeyRemoval.timeoutSecs (Default Value): 600
router.metrics.tokenVerification.accessUrl (Default Value):
router.metrics.tokenVerification.revocableExpiryThresholdSeconds (Default Value): -1
router.mtls.configCacheErrorExpirationSecs (Default Value): 15
router.mtls.configCacheExpirationSecs (Default Value): 600
router.multiTenant.cacheExpirySecs (Default Value): 86400
router.multiTenant.cacheGcPeriodSecs (Default Value): 600
router.multiTenant.transport.idleConnTimeoutSecs (Default Value): 5
router.multiTenant.transport.insecureSkipVerify (Default Value): true
router.probes.liveness.failOnLongFailingReadiness.enabled (Default Value): true
router.probes.liveness.failOnLongFailingReadiness.failureDurationSecs (Default Value): 60
router.profiling.enabled (Default Value): false
router.profiling.pprofServerPort (Default Value): 6060
router.profiling.pprofServerTimeoutSeconds (Default Value): 21600
router.proxy.httpUrl (Default Value):
router.proxy.httpsUrl (Default Value):
router.proxy.ignoredHosts (Default Value): []
router.redirectUnknownPathToArtifactoryEnabled (Default Value): false
router.retry.enabled (Default Value): false
router.retry.enabledOnUnavailable (Default Value): true
router.security.cipherSuites (Default Value): TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_AES_128_GCM_SHA256,TLS_AES_256_GCM_SHA384,TLS_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305
router.serviceRegistry.accessUrl (Applicative Resolution): http://localhost:8040/access
router.serviceRegistry.insecure (Default Value): false
router.serviceRegistry.requestTimeout (Default Value): 15s
router.serviceRegistry.skipJoinConnectivityTest (Default Value): false
router.serviceRegistry.topologyServiceTokenExpirySecs (Default Value): 7200
router.serviceRegistry.topologyUrl (Applicative Resolution): http://localhost:8040/access
router.serviceRegistry.useTopologyService (Default Value): false
router.supportBundle.aggregationTimeout (Default Value): 1h0m0s
router.topology.external.allowH2c (Default Value): true
router.topology.external.healthRequestTimeoutSecs (Default Value): 5
router.topology.external.protocolFinderCacheSecs (Default Value): 600
router.topology.external.refresh.interval (Default Value): 3s
router.topology.external.refresh.maxStaleHeartbeat (Default Value): 30s
router.topology.external.skipIpCheck (Default Value): false
router.topology.local.enableNodeStatusBroadcast (Default Value): true
router.topology.local.healthCheck.healthyThreshold (Default Value): 2
router.topology.local.healthCheck.interval (Default Value): 5s
router.topology.local.healthCheck.requestTimeout (Default Value): 5s
router.topology.local.healthCheck.unhealthyThreshold (Default Value): 2
router.topology.local.ignorePersistedServices (Default Value): false
router.topology.local.requiredServiceTypes (Environment Variable: JF_ROUTER_TOPOLOGY_LOCAL_REQUIREDSERVICETYPES): jfrt,jfac,jfmd,jffe,jfob
shared.database.driver (File): org.postgresql.Driver
shared.database.password (File): *****
shared.database.type (File): postgresql
shared.database.url (File): jdbc:postgresql:/192.168.95.70:5432/artifactory
shared.database.username (File): artifactory
shared.jfrogUrl (Default Value):
shared.logging.consoleLog.format (Default Value): shared.logging.consoleLog.format
shared.logging.enableJsonConsoleLogAppenders (Default Value): false
shared.multiTenant.cellId (Default Value):
shared.multiTenant.enabled (Default Value): false
shared.multiTenant.tenantRegistryClient.caCert (Default Value):
shared.multiTenant.tenantRegistryClient.cacheExpirationSecs (Default Value): 21600
shared.multiTenant.tenantRegistryClient.clientCert (Default Value):
shared.multiTenant.tenantRegistryClient.clientCertKey (Default Value): *****
shared.multiTenant.tenantRegistryClient.dialTimeoutSecs (Default Value): 10
shared.multiTenant.tenantRegistryClient.endpoints (Default Value): []
shared.multiTenant.tenantRegistryClient.requestTimeoutSecs (Default Value): 30
shared.newrelic.appName (Default Value):
shared.newrelic.distributedTracerEnabled (Default Value): true
shared.newrelic.enableDebugLog (Default Value): false
shared.newrelic.enabled (Default Value): false
shared.newrelic.licenseKey (Default Value): *****
shared.node (File): <nil>
shared.node.availabilityZoneId (Default Value):
shared.node.id (Environment Variable: JF_SHARED_NODE_ID): localhost.localdomain
shared.node.ip (Environment Variable: JF_SHARED_NODE_IP): [::1]
shared.node.name (Environment Variable: JF_SHARED_NODE_NAME): localhost
shared.node.port (Default Value): 0
shared.node.tlsEnabled (Default Value): false
shared.security (File): <nil>
shared.security.bootstrapKeysReadTimeoutSecs (Default Value): 300
shared.security.joinKeyFile (Default Value): etc/security/join.key
shared.security.masterKeyExternal (Default Value): false
shared.security.masterKeyFile (Default Value): etc/security/master.key
system.yaml (Environment Variable: JF_SYSTEM_YAML): /opt/jfrog/artifactory/var/etc/system.yaml
topology.grpcPort (Default Value): 8021
topology.port (Default Value): 8020
2025/07/31 16:17:01 maxprocs: Leaving GOMAXPROCS=2: CPU quota undefined
2025-07-31T16:17:01.619L [tomct] [INFO ] [                ] [org.apache.coyote.http11.Http11NioProtocol] [org.apache.coyote.AbstractProtocol init] - Initializing ProtocolHandler ["http-nio-127.0.0.1-8040"]
2025-07-31T10:47:01.629Z [jfmd] [DEBUG] [                ] [file_resolver.go:71           ] [main                ] - Resolved system configuration file path: /opt/jfrog/artifactory/var/etc/system.yaml
2025-07-31T10:47:01.630Z [jfmd] [TRACE] [                ] [aws_secret_resolver.go:76     ] [main                ] - Config key not set for aws secret (metadata.database.secretsManagerAlias)
2025-07-31T10:47:01.630Z [jfmd] [DEBUG] [                ] [load_config.go:192            ] [main                ] - Adding optional resolver failed: <nil>
2025/07/31 16:17:01 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
2025/07/31 16:17:01 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
2025/07/31 16:17:01 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
2025/07/31 16:17:01 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
2025/07/31 16:17:01 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
2025/07/31 16:17:01 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
2025-07-31T16:17:01.713L [tomct] [INFO ] [                ] [org.apache.catalina.core.StandardService] [org.apache.catalina.core.StandardService startInternal] - Starting service [Catalina]
2025-07-31T16:17:01.714L [tomct] [INFO ] [                ] [org.apache.catalina.core.StandardEngine] [org.apache.catalina.core.StandardEngine startInternal] - Starting Servlet engine: [Apache Tomcat/10.1.31]
2025-07-31T16:17:01.744L [tomct] [INFO ] [                ] [org.apache.catalina.startup.HostConfig] [org.apache.catalina.startup.HostConfig deployDescriptor] - Deploying deployment descriptor [/opt/jfrog/artifactory/app/access/tomcat/conf/Catalina/localhost/access.xml]
2025-07-31T16:17:01.772L [tomct] [WARNING] [                ] [org.apache.catalina.startup.HostConfig] [org.apache.catalina.startup.HostConfig deployDescriptor] - The path attribute with value [/access] in deployment descriptor [/opt/jfrog/artifactory/app/access/tomcat/conf/Catalina/localhost/access.xml] has been ignored
2025-07-31T16:17:01.785L [tomct] [WARNING] [                ] [org.apache.catalina.startup.HostConfig] [org.apache.catalina.startup.HostConfig deployDescriptor] - A docBase [/opt/jfrog/artifactory/app/access/tomcat/webapps/access.war] inside the host appBase has been specified, and will be ignored
2025/07/31 16:17:03 maxprocs: Leaving GOMAXPROCS=2: CPU quota undefined
[INFO ] JFrog Observability (jfob) service initialization started. Version: 1.31.11 (revision: a761e9718c, build date: 2024-12-23T14:07:19Z) PID: 37035 Home: /opt/jfrog/artifactory
[DEBUG] Resolved system configuration file path: /opt/jfrog/artifactory/var/etc/system.yaml
2025/07/31 16:17:03 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
2025/07/31 16:17:03 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
2025/07/31 16:17:03 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
2025/07/31 16:17:03 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
2025/07/31 16:17:03 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
Max number of open files: 32000
Using JF_PRODUCT_HOME: /opt/jfrog/artifactory
Using JF_ARTIFACTORY_PID: /var/run/artifactory.pid
Tomcat started.
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:04.927Z","service":"tomcat","loglevel":"WARNING","class":"org.apache.tomcat.util.digester.Digester","message":"Match [Server/Service/Connector] failed to set property [sendReasonPhrase] to [false]"}}
2025-07-31T10:47:05.162Z [jffe ] [INFO ] [] [frontend-service.log] [main                ] - frontend (jffe) service initialization started. Version: 1.98.11 Revision: 10000011 PID: 36880 Home: /opt/jfrog/artifactory
2025-07-31T10:47:05.174Z [jffe ] [INFO ] [] [frontend-service.log] [main                ] - Starting FE server in ST mode
2025-07-31T10:47:05.177Z [jffe ] [INFO ] [] [frontend-service.log] [main                ] - attempting pinging artifactory for 300 retires and 1.0s interval for total of 5 minutes
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:06.031Z","service":"tomcat","loglevel":"INFO","class":"org.apache.coyote.http11.Http11NioProtocol","message":"Initializing ProtocolHandler ["http-nio-8081"]"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:06.144Z","service":"tomcat","loglevel":"INFO","class":"org.apache.coyote.http11.Http11NioProtocol","message":"Initializing ProtocolHandler ["http-nio-127.0.0.1-8091"]"}}
2025-07-31T10:47:06.158Z [jfrou] [INFO ] [6e7e9dc742434a25] [security_keys.go:172          ] [main                ] [] - Master key is missing. Pending for 5 seconds with 5m0s timeout
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:06.166Z","service":"tomcat","loglevel":"INFO","class":"org.apache.catalina.core.StandardService","message":"Starting service [Catalina]"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:06.167Z","service":"tomcat","loglevel":"INFO","class":"org.apache.catalina.core.StandardEngine","message":"Starting Servlet engine: [Apache Tomcat/10.1.34]"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:06.241Z","service":"tomcat","loglevel":"INFO","class":"org.apache.catalina.startup.HostConfig","message":"Deploying deployment descriptor [/opt/jfrog/artifactory/app/artifactory/tomcat/conf/Catalina/localhost/artifactory.xml]"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:06.281Z","service":"tomcat","loglevel":"WARNING","class":"org.apache.catalina.startup.HostConfig","message":"The path attribute with value [/artifactory] in deployment descriptor [/opt/jfrog/artifactory/app/artifactory/tomcat/conf/Catalina/localhost/artifactory.xml] has been ignored"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:06.293Z","service":"tomcat","loglevel":"WARNING","class":"org.apache.catalina.startup.HostConfig","message":"A docBase [/opt/jfrog/artifactory/app/artifactory/tomcat/webapps/artifactory.war] inside the host appBase has been specified, and will be ignored"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:06.438Z","service":"tomcat","loglevel":"WARNING","class":"org.apache.tomcat.util.digester.Digester","message":"Match [Context/CookieProcessor] failed to set property [forwardSlashIsSeparator] to [false]"}}
2025-07-31T10:47:06.645Z [jfmd ] [INFO ] [                ] [security_keys.go:172          ] [main                ] - Master key is missing. Pending for 5 seconds with 2m0s timeout [init]
2025-07-31T10:47:08.238Z 35[jfob ] [INFO ] [273e55260cebb691] [security_keys.go:172          ] [main                ] - Master key is missing. Pending for 5 seconds with 2m0s timeout [startup]
2025-07-31T10:47:09.760Z [jfac ] [INFO ] [9e882fcdb45efd27] [licationContextInitializer:166] [main                ] - Access (jfac) service initialization started. Version: 7.128.13 Revision: 82813900 PID: 36319 Home: /opt/jfrog/artifactory FIPS Mode: none
2025-07-31T10:47:10.028Z [jfac ] [INFO ] [9e882fcdb45efd27] [o.j.a.AccessApplication:50    ] [main                ] - Starting AccessApplication v7.128.13 using Java 17.0.12 with PID 36319 (/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-application-7.128.13.jar started by artifactory in /)
2025-07-31T10:47:10.030Z [jfac ] [INFO ] [9e882fcdb45efd27] [o.j.a.AccessApplication:660   ] [main                ] - The following 1 profile is active: "production"
2025-07-31T10:47:10.545Z [jfrt ] [INFO ] [2582d4292160c48b] [o.a.c.h.ArtifactoryHome:252   ] [Catalina-utility-1  ] - Creating Artifactory home at 'null'
2025-07-31T10:47:11.007Z [jfrt ] [INFO ] [2582d4292160c48b] [o.a.c.h.HaNodeProperties:65   ] [Catalina-utility-1  ] - Artifactory is running in non-clustered mode.
2025-07-31T10:47:11.068Z [jfrt ] [INFO ] [2582d4292160c48b] [tifactoryHomeConfigListener:85] [Catalina-utility-1  ] - Resolved Home: '/opt/jfrog/artifactory
2025-07-31T10:47:11.162Z [jfrou] [INFO ] [6e7e9dc742434a25] [security_keys.go:172          ] [main                ] [] - Master key is missing. Pending for 10 seconds with 5m0s timeout
2025-07-31T10:47:11.292Z [jfrt ] [INFO ] [2582d4292160c48b] [o.a.c.h.ArtifactoryHome:1022  ] [Catalina-utility-1  ] - Setting 'cocoapodsNew,vcsNew,swiftNew,alpineNew,npmNew,condaNew,permissions-cache-legacy' to system property 'spring.profiles.active'
2025-07-31T10:47:11.419Z [jfrt ] [ERROR] [2582d4292160c48b] [d.d.l.DbDistributeLocksDao:357] [Catalina-utility-1  ] - Unable to detect database version Driver org.postgresql.Driver claims to not accept jdbcUrl, jdbc:postgresql:/192.168.95.70:5432/artifactory
2025-07-31T10:47:11.495Z [jfrt ] [INFO ] [2582d4292160c48b] [o.a.c.h.ArtifactoryHome:1022  ] [Catalina-utility-1  ] - Setting 'cocoapodsNew,vcsNew,swiftNew,alpineNew,npmNew,condaNew,permissions-cache-legacy' to system property 'spring.profiles.active'
2025-07-31T10:47:11.547Z [jfrt ] [INFO ] [                ] [o.j.c.w.FileWatcher:146       ] [file-watcher-poller ] - Starting watch of folder configurations
2025-07-31T10:47:11.648Z [jfmd ] [INFO ] [                ] [security_keys.go:172          ] [main                ] - Master key is missing. Pending for 10 seconds with 2m0s timeout [init]
2025-07-31T10:47:11.675Z [jfrt ] [ERROR] [2582d4292160c48b] [o.a.c.VersionProviderImpl:144 ] [Catalina-utility-1  ] - Failed to resolve version information: Driver org.postgresql.Driver claims to not accept jdbcUrl, jdbc:postgresql:/192.168.95.70:5432/artifactory
2025-07-31T10:47:11.676Z [jfrt ] [ERROR] [2582d4292160c48b] [tifactoryHomeConfigListener:55] [Catalina-utility-1  ] - Failed initializing Home. Caught exception:
java.lang.IllegalStateException: Driver org.postgresql.Driver claims to not accept jdbcUrl, jdbc:postgresql:/192.168.95.70:5432/artifactory
        at org.artifactory.converter.VersionProviderImpl.init(VersionProviderImpl.java:145)
        at org.artifactory.lifecycle.webapp.servlet.BasicConfigurationManager.initVersion(BasicConfigurationManager.java:171)
        at org.artifactory.lifecycle.webapp.servlet.BasicConfigurationManager.initArtifactoryInstallation(BasicConfigurationManager.java:148)
        at org.artifactory.lifecycle.webapp.servlet.BasicConfigurationManager.initialize(BasicConfigurationManager.java:135)
        at org.artifactory.lifecycle.webapp.servlet.ArtifactoryHomeConfigListener.initBasicConfigManager(ArtifactoryHomeConfigListener.java:61)
        at org.artifactory.lifecycle.webapp.servlet.ArtifactoryHomeConfigListener.contextInitialized(ArtifactoryHomeConfigListener.java:53)
        at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4008)
        at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4436)
        at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
        at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:599)
        at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:571)
        at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:654)
        at org.apache.catalina.startup.HostConfig.deployDescriptor(HostConfig.java:635)
        at org.apache.catalina.startup.HostConfig$DeployDescriptor.run(HostConfig.java:1889)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:63)
        at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.RuntimeException: Driver org.postgresql.Driver claims to not accept jdbcUrl, jdbc:postgresql:/192.168.95.70:5432/artifactory
        at com.zaxxer.hikari.util.DriverDataSource.<init>(DriverDataSource.java:109)
        at com.zaxxer.hikari.pool.PoolBase.initializeDataSource(PoolBase.java:327)
        at com.zaxxer.hikari.pool.PoolBase.<init>(PoolBase.java:113)
        at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:91)
        at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:111)
        at org.jfrog.storage.wrapper.JFrogDataSourceWrapper.getConnection(JFrogDataSourceWrapper.java:104)
        at org.jfrog.storage.wrapper.JFrogDataSourceWrapper.invoke(JFrogDataSourceWrapper.java:73)
        at jdk.proxy2/jdk.proxy2.$Proxy5.getConnection(Unknown Source)
        at org.jfrog.storage.util.DbUtils.withMetadata(DbUtils.java:765)
        at org.jfrog.storage.util.DbUtils.tableExists(DbUtils.java:275)
        at org.artifactory.common.config.db.DbVersionUtil.isDbPropertiesTableExists(DbVersionUtil.java:50)
        at org.artifactory.common.config.db.DbVersionDao.isDbPropertiesTableExists(DbVersionDao.java:48)
        at org.artifactory.converter.VersionProviderImpl.tryToResolveFromDb(VersionProviderImpl.java:233)
        at org.artifactory.converter.VersionProviderImpl.getCompoundVersionDetailsFromDbIfExists(VersionProviderImpl.java:158)
        at org.artifactory.converter.VersionProviderImpl.loadOriginalDbVersion(VersionProviderImpl.java:179)
        at org.artifactory.converter.VersionProviderImpl.init(VersionProviderImpl.java:140)
        ... 20 common frames omitted
2025-07-31T10:47:11.707Z [jfrt ] [INFO ] [2582d4292160c48b] [actoryContextConfigListener:88] [Catalina-utility-1  ] - Artifactory context initialization started for context: null
2025-07-31T10:47:11.707Z [jfrt ] [ERROR] [2582d4292160c48b] [actoryContextConfigListener:93] [Catalina-utility-1  ] - Failed initializing Artifactory context: Artifactory home not initialized.
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:11.708Z","service":"tomcat","loglevel":"SEVERE","class":"org.apache.catalina.core.StandardContext","message":"One or more listeners failed to start. Full details will be found in the appropriate container log file"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:11.718Z","service":"tomcat","loglevel":"SEVERE","class":"org.apache.catalina.core.StandardContext","message":"Context [/artifactory] startup failed due to previous errors"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:11.794Z","service":"tomcat","loglevel":"INFO","class":"org.apache.catalina.startup.HostConfig","message":"Deployment of deployment descriptor [/opt/jfrog/artifactory/app/artifactory/tomcat/conf/Catalina/localhost/artifactory.xml] has finished in [5,554] ms"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:11.799Z","service":"tomcat","loglevel":"INFO","class":"org.apache.catalina.startup.HostConfig","message":"Deploying web application directory [/opt/jfrog/artifactory/app/artifactory/tomcat/webapps/ROOT]"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:11.806Z","service":"tomcat","loglevel":"WARNING","class":"org.apache.tomcat.util.digester.Digester","message":"Match [Context/CookieProcessor] failed to set property [forwardSlashIsSeparator] to [false]"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:11.890Z","service":"tomcat","loglevel":"INFO","class":"org.apache.catalina.startup.HostConfig","message":"Deployment of web application directory [/opt/jfrog/artifactory/app/artifactory/tomcat/webapps/ROOT] has finished in [91] ms"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:11.893Z","service":"tomcat","loglevel":"INFO","class":"org.apache.coyote.http11.Http11NioProtocol","message":"Starting ProtocolHandler ["http-nio-8081"]"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:11.977Z","service":"tomcat","loglevel":"INFO","class":"org.apache.coyote.http11.Http11NioProtocol","message":"Starting ProtocolHandler ["http-nio-127.0.0.1-8091"]"}}
2025-07-31T10:47:13.059Z [shell] [INFO ] [] [artifactoryManage.sh:135      ] [main] - Artifactory Tomcat started in normal mode
2025-07-31T10:47:13.094Z [shell] [INFO ] [] [artifactoryCommon.sh:1248     ] [main] - Artifactory running with PID 37217
2025-07-31T10:47:13.243Z 35[jfob ] [INFO ] [273e55260cebb691] [security_keys.go:172          ] [main                ] - Master key is missing. Pending for 10 seconds with 2m0s timeout [startup]
2025-07-31T10:47:15.000Z [jffe ] [INFO ] [] [frontend-service.log] [main                ] - pinging artifactory, attempt number 10
2025-07-31T10:47:15.018Z [jffe ] [INFO ] [] [frontend-service.log] [main                ] - pinging artifactory attempt number 10 failed with code : ECONNREFUSED
^C
[root@localhost log]# tail -f console.log
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:40.742Z","service":"tomcat","loglevel":"INFO","class":"org.apache.catalina.core.StandardServer","message":"A valid shutdown command was received via the shutdown port. Stopping the Server instance."}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:40.743Z","service":"tomcat","loglevel":"INFO","class":"org.apache.coyote.http11.Http11NioProtocol","message":"Pausing ProtocolHandler ["http-nio-8081"]"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:40.747Z","service":"tomcat","loglevel":"INFO","class":"org.apache.coyote.http11.Http11NioProtocol","message":"Pausing ProtocolHandler ["http-nio-127.0.0.1-8091"]"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:40.749Z","service":"tomcat","loglevel":"INFO","class":"org.apache.catalina.core.StandardService","message":"Stopping service [Catalina]"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:40.760Z","service":"tomcat","loglevel":"INFO","class":"org.apache.coyote.http11.Http11NioProtocol","message":"Stopping ProtocolHandler ["http-nio-8081"]"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:40.769Z","service":"tomcat","loglevel":"INFO","class":"org.apache.coyote.http11.Http11NioProtocol","message":"Stopping ProtocolHandler ["http-nio-127.0.0.1-8091"]"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:40.782Z","service":"tomcat","loglevel":"INFO","class":"org.apache.coyote.http11.Http11NioProtocol","message":"Destroying ProtocolHandler ["http-nio-8081"]"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:40.783Z","service":"tomcat","loglevel":"INFO","class":"org.apache.coyote.http11.Http11NioProtocol","message":"Destroying ProtocolHandler ["http-nio-127.0.0.1-8091"]"}}
2025-07-31T10:47:41.183Z [jfrou] [INFO ] [6e7e9dc742434a25] [security_keys.go:172          ] [main                ] [] - Master key is missing. Pending for 40 seconds with 5m0s timeout
2025-07-31T10:47:41.664Z [jfmd ] [INFO ] [                ] [security_keys.go:172          ] [main                ] - Master key is missing. Pending for 40 seconds with 2m0s timeout [init]


2025-07-31T10:47:43.261Z 35[jfob ] [INFO ] [273e55260cebb691] [security_keys.go:172          ] [main                ] - Master key is missing. Pending for 40 seconds with 2m0s timeout [startup]
















2025-07-31T10:47:43.811Z [shell] [INFO ] [] [artifactoryManage.sh:161      ] [main] - Artifactory Tomcat stopped















2025-07-31T10:47:44.316Z [shell] [INFO ] [] [systemYamlHelper.sh:621       ] [main] - Resolved .shared.database.type (postgresql) from /opt/jfrog/artifactory/var/etc/system.yaml




2025-07-31T10:47:44.441Z [shell] [INFO ] [] [systemYamlHelper.sh:621       ] [main] - Resolved .shared.database.url (__sensitive_key_hidden___) from /opt/jfrog/artifactory/var/etc/system.yaml
2025-07-31T10:47:44.530Z [shell] [INFO ] [] [systemYamlHelper.sh:621       ] [main] - Resolved .shared.database.password (__sensitive_key_hidden___) from /opt/jfrog/artifactory/var/etc/system.yaml
2025-07-31T16:17:45.082L [tomct] [INFO ] [                ] [org.apache.catalina.core.StandardServer] [org.apache.catalina.core.StandardServer await] - A valid shutdown command was received via the shutdown port. Stopping the Server instance.
2025-07-31T16:17:45.083L [tomct] [INFO ] [                ] [org.apache.coyote.http11.Http11NioProtocol] [org.apache.coyote.AbstractProtocol pause] - Pausing ProtocolHandler ["http-nio-127.0.0.1-8040"]
2025-07-31T16:17:45.087L [tomct] [INFO ] [                ] [org.apache.catalina.core.StandardService] [org.apache.catalina.core.StandardService stopInternal] - Stopping service [Catalina]
2025-07-31T16:17:45.103L [tomct] [INFO ] [                ] [org.apache.coyote.http11.Http11NioProtocol] [org.apache.coyote.AbstractProtocol stop] - Stopping ProtocolHandler ["http-nio-127.0.0.1-8040"]
2025-07-31T16:17:45.109L [tomct] [INFO ] [                ] [org.apache.coyote.http11.Http11NioProtocol] [org.apache.coyote.AbstractProtocol destroy] - Destroying ProtocolHandler ["http-nio-127.0.0.1-8040"]
2025-07-31T10:47:45.149Z [jffe ] [INFO ] [] [frontend-service.log] [main                ] - pinging artifactory, attempt number 40
2025-07-31T10:47:45.153Z [jffe ] [INFO ] [] [frontend-service.log] [main                ] - pinging artifactory attempt number 40 failed with code : ECONNREFUSED
2025-07-31T10:47:46.187Z [jfrou] [INFO ] [6e7e9dc742434a25] [security_keys.go:172          ] [main                ] [] - Master key is missing. Pending for 45 seconds with 5m0s timeout
2025-07-31T10:47:46.668Z [jfmd ] [INFO ] [                ] [security_keys.go:172          ] [main                ] - Master key is missing. Pending for 45 seconds with 2m0s timeout [init]
2025-07-31T10:47:48.265Z 35[jfob ] [INFO ] [273e55260cebb691] [security_keys.go:172          ] [main                ] - Master key is missing. Pending for 45 seconds with 2m0s timeout [startup]
2025-07-31T10:47:48.943Z [shell] [INFO ] [] [artifactoryManage.sh:87       ] [main] - Starting Artifactory tomcat as user artifactory...
2025-07-31T10:47:48.960Z [shell] [INFO ] [] [installerCommon.sh:1632       ] [main] - Checking open files and processes limits
2025-07-31T10:47:48.978Z [shell] [INFO ] [] [installerCommon.sh:1635       ] [main] - Current max open files is 1024
2025-07-31T10:47:48.997Z [shell] [INFO ] [] [installerCommon.sh:1646       ] [main] - Current max open processes is 14387
2025-07-31T10:47:49.070Z [shell] [INFO ] [] [installerCommon.sh:3006       ] [main] - System.yaml validation succeeded
2025-07-31T10:47:49.270Z [shell] [INFO ] [] [installerCommon.sh:3508       ] [main] - Setting JF_SHARED_NODE_ID to localhost.localdomain
2025-07-31T10:47:49.326Z [shell] [INFO ] [] [installerCommon.sh:3508       ] [main] - Setting JF_SHARED_NODE_IP to ::1
2025-07-31T10:47:49.343Z [shell] [INFO ] [] [installerCommon.sh:2018       ] [main] - Found an ipv6 value, enclosing it wih brackets : [::1]
2025-07-31T10:47:49.394Z [shell] [INFO ] [] [installerCommon.sh:3508       ] [main] - Setting JF_SHARED_NODE_NAME to localhost
2025-07-31T10:47:49.627Z [shell] [INFO ] [] [artifactoryCommon.sh:268      ] [main] - Using Tomcat template to generate : /opt/jfrog/artifactory/app/artifactory/tomcat/conf/server.xml
2025-07-31T10:47:49.706Z [shell] [INFO ] [] [systemYamlHelper.sh:981       ] [main] - Resolved ${artifactory.port||8081} to default value : 8081
2025-07-31T10:47:49.759Z [shell] [INFO ] [] [systemYamlHelper.sh:981       ] [main] - Resolved ${artifactory.tomcat.connector.sendReasonPhrase||false} to default value : false
2025-07-31T10:47:49.812Z [shell] [INFO ] [] [systemYamlHelper.sh:981       ] [main] - Resolved ${artifactory.tomcat.connector.relaxedPathChars||_SQUARE_BRACKETS_} to default value : _SQUARE_BRACKETS_
2025-07-31T10:47:49.869Z [shell] [INFO ] [] [systemYamlHelper.sh:981       ] [main] - Resolved ${artifactory.tomcat.connector.relaxedQueryChars||_SQUARE_BRACKETS_} to default value : _SQUARE_BRACKETS_
2025-07-31T10:47:49.924Z [shell] [INFO ] [] [systemYamlHelper.sh:981       ] [main] - Resolved ${artifactory.tomcat.connector.maxThreads||200} to default value : 200
2025-07-31T10:47:50.018Z [shell] [INFO ] [] [systemYamlHelper.sh:981       ] [main] - Resolved ${artifactory.tomcat.maintenanceConnector.port||8091} to default value : 8091
2025-07-31T10:47:50.074Z [shell] [INFO ] [] [systemYamlHelper.sh:981       ] [main] - Resolved ${artifactory.tomcat.maintenanceConnector.maxThreads||5} to default value : 5
2025-07-31T10:47:50.130Z [shell] [INFO ] [] [systemYamlHelper.sh:981       ] [main] - Resolved ${artifactory.tomcat.maintenanceConnector.acceptCount||5} to default value : 5
2025-07-31T10:47:50.241Z [shell] [INFO ] [] [systemYamlHelper.sh:621       ] [main] - Resolved JF_PRODUCT_HOME (/opt/jfrog/artifactory) from environment variable
2025-07-31T10:47:50.333Z [shell] [INFO ] [] [systemYamlHelper.sh:981       ] [main] - Resolved ${shared.tomcat.workDir||/opt/jfrog/artifactory/var/work/artifactory/tomcat} to default value : /opt/jfrog/artifactory/var/work/artifactory/tomcat
2025-07-31T10:47:52.568Z [shell] [INFO ] [] [systemYamlHelper.sh:621       ] [main] - Resolved JF_SHARED_NODE_ID (localhost.localdomain) from environment variable
2025-07-31T10:47:52.609Z [shell] [INFO ] [] [systemYamlHelper.sh:621       ] [main] - Resolved JF_SHARED_NODE_IP ([::1]) from environment variable
2025-07-31T10:47:52.875Z [shell] [INFO ] [] [systemYamlHelper.sh:621       ] [main] - Resolved .shared.database.type (postgresql) from /opt/jfrog/artifactory/var/etc/system.yaml
2025-07-31T10:47:52.997Z [shell] [INFO ] [] [systemYamlHelper.sh:621       ] [main] - Resolved .shared.database.url (__sensitive_key_hidden___) from /opt/jfrog/artifactory/var/etc/system.yaml
2025-07-31T10:47:53.092Z [shell] [INFO ] [] [systemYamlHelper.sh:621       ] [main] - Resolved .shared.database.password (__sensitive_key_hidden___) from /opt/jfrog/artifactory/var/etc/system.yaml
2025-07-31T10:47:53.251Z [shell] [INFO ] [] [installerCommon.sh:1632       ] [main] - Checking open files and processes limits
2025-07-31T10:47:53.270Z [shell] [INFO ] [] [installerCommon.sh:1635       ] [main] - Current max open files is 32000
2025-07-31T10:47:53.289Z [shell] [INFO ] [] [installerCommon.sh:1646       ] [main] - Current max open processes is 14387
2025-07-31T10:47:53.343Z [shell] [INFO ] [] [installerCommon.sh:3006       ] [main] - System.yaml validation succeeded
2025-07-31T10:47:53.416Z [shell] [INFO ] [] [installerCommon.sh:1715       ] [main] - Testing directory /opt/jfrog/artifactory/var has read/write permissions for user id 980
2025-07-31T10:47:53.438Z [shell] [INFO ] [] [installerCommon.sh:1730       ] [main] - Permissions for /opt/jfrog/artifactory/var are good
2025-07-31T10:47:53.521Z [shell] [INFO ] [] [installerCommon.sh:3518       ] [main] - Found JF_SHARED_NODE_ID set as environment variable with value : localhost.localdomain
2025-07-31T10:47:53.565Z [shell] [INFO ] [] [installerCommon.sh:3518       ] [main] - Found JF_SHARED_NODE_IP set as environment variable with value : [::1]
2025-07-31T10:47:53.602Z [shell] [INFO ] [] [installerCommon.sh:3518       ] [main] - Found JF_SHARED_NODE_NAME set as environment variable with value : localhost
2025-07-31T10:47:53.895Z [shell] [INFO ] [] [nativeCommon.sh:75            ] [main] - Using Tomcat template to generate : /opt/jfrog/artifactory/app/access/tomcat/conf/server.xml
2025-07-31T10:47:53.971Z [shell] [INFO ] [] [systemYamlHelper.sh:981       ] [main] - Resolved ${access.http.port||8040} to default value : 8040
2025-07-31T10:47:54.031Z [shell] [INFO ] [] [systemYamlHelper.sh:981       ] [main] - Resolved ${access.tomcat.connector.sendReasonPhrase||false} to default value : false
2025-07-31T10:47:54.090Z [shell] [INFO ] [] [systemYamlHelper.sh:981       ] [main] - Resolved ${access.tomcat.connector.maxThreads||50} to default value : 50
2025-07-31T10:47:54.205Z [shell] [INFO ] [] [systemYamlHelper.sh:621       ] [main] - Resolved JF_PRODUCT_HOME (/opt/jfrog/artifactory) from environment variable
2025-07-31T10:47:54.298Z [shell] [INFO ] [] [systemYamlHelper.sh:981       ] [main] - Resolved ${access.tomcat.workDir||/opt/jfrog/artifactory/var/work/access/tomcat} to default value : /opt/jfrog/artifactory/var/work/access/tomcat
Logging configuration has both console=true and filepath='router-service.log'; ignoring console.
2025-07-31T10:47:55.519Z [jfrou] [INFO ] [62e66a4c99c7d390] [bootstrap.go:91               ] [main                ] [] - Router (jfrou) service initialization started. Version: 7.135.2-1 Revision: 3089f1f73bb276c2f6d51d62c1651ac81baf70d4 PID: 43856 Home: /opt/jfrog/artifactory
2025-07-31T10:47:55.520Z [jfrou] [INFO ] [62e66a4c99c7d390] [bootstrap.go:94               ] [main                ] [] - JFrog Router IP: [::1]
2025-07-31T10:47:55.523Z [jfrou] [INFO ] [62e66a4c99c7d390] [bootstrap.go:95               ] [main                ] [] - Effective configuration:
access (File): <nil>
access.http.port (Default Value): 8040
artifactory.pid (Environment Variable: JF_ARTIFACTORY_PID): /var/run/artifactory.pid
configVersion (File): 1
product.home (Environment Variable: JF_PRODUCT_HOME): /opt/jfrog/artifactory
router.autoMemLimitRatio (Default Value): 0.9
router.availabilityZoneOptimization.loggingIntervalSecs (Default Value): 600
router.encryptSystemConfig (Default Value): true
router.entrypoints.externalHost (Default Value):
router.entrypoints.externalMaxConcurrentStreams (Default Value): 500
router.entrypoints.externalPort (Default Value): 8082
router.entrypoints.grpcPort (Default Value): 8047
router.entrypoints.hotPathLogEnabled (Default Value): true
router.entrypoints.internalMaxConcurrentStreams (Default Value): 500
router.entrypoints.internalPort (Default Value): 8046
router.entrypoints.traefikApiPort (Default Value): 8049
router.httpclient.requestTimeoutSecs (Default Value): 30
router.lifecycle.shutdown.entrypointsGraceTimeout (Default Value): 10s
router.lifecycle.shutdown.internalJobsGraceTimeout (Default Value): 7s
router.logging.application.caller (Default Value): false
router.logging.application.console (Default Value): true
router.logging.application.filePath (Default Value): router-service.log
router.logging.application.format (Default Value): jftext
router.logging.application.level (Default Value): info
router.logging.application.rotation.compress (Default Value): true
router.logging.application.rotation.keepLastDecompressed (Default Value): 1
router.logging.application.rotation.maxAgeDays (Default Value): 0
router.logging.application.rotation.maxFiles (Default Value): 10
router.logging.application.rotation.maxSizeMb (Default Value): 25
router.logging.consoleLog.format (Default Value): jftext
router.logging.request.filePath (Default Value): router-request.log
router.logging.request.forwardedForLogging.enabled (Default Value): false
router.logging.request.forwardedForLogging.header (Default Value): X-Forwarded-For
router.logging.request.rotation.compress (Default Value): true
router.logging.request.rotation.keepLastDecompressed (Default Value): 1
router.logging.request.rotation.maxAgeDays (Default Value): 0
router.logging.request.rotation.maxFiles (Default Value): 10
router.logging.request.rotation.maxSizeMb (Default Value): 100
router.logging.request.tokenIdLoggingEnabled (Default Value): false
router.logging.request.verbose (Default Value): false
router.logging.traefik.caller (Default Value): false
router.logging.traefik.console (Default Value): false
router.logging.traefik.filePath (Default Value): router-traefik.log
router.logging.traefik.format (Default Value): jftext
router.logging.traefik.level (Default Value): info
router.logging.traefik.rotation.compress (Default Value): true
router.logging.traefik.rotation.keepLastDecompressed (Default Value): 1
router.logging.traefik.rotation.maxAgeDays (Default Value): 0
router.logging.traefik.rotation.maxFiles (Default Value): 10
router.logging.traefik.rotation.maxSizeMb (Default Value): 25
router.masterKeyRemoval.heartbeatCount (Default Value): 3
router.masterKeyRemoval.intervalSecs (Default Value): 60
router.masterKeyRemoval.timeoutSecs (Default Value): 600
router.metrics.tokenVerification.accessUrl (Default Value):
router.metrics.tokenVerification.revocableExpiryThresholdSeconds (Default Value): -1
router.mtls.configCacheErrorExpirationSecs (Default Value): 15
router.mtls.configCacheExpirationSecs (Default Value): 600
router.multiTenant.cacheExpirySecs (Default Value): 86400
router.multiTenant.cacheGcPeriodSecs (Default Value): 600
router.multiTenant.transport.idleConnTimeoutSecs (Default Value): 5
router.multiTenant.transport.insecureSkipVerify (Default Value): true
router.probes.liveness.failOnLongFailingReadiness.enabled (Default Value): true
router.probes.liveness.failOnLongFailingReadiness.failureDurationSecs (Default Value): 60
router.profiling.enabled (Default Value): false
router.profiling.pprofServerPort (Default Value): 6060
router.profiling.pprofServerTimeoutSeconds (Default Value): 21600
router.proxy.httpUrl (Default Value):
router.proxy.httpsUrl (Default Value):
router.proxy.ignoredHosts (Default Value): []
router.redirectUnknownPathToArtifactoryEnabled (Default Value): false
router.retry.enabled (Default Value): false
router.retry.enabledOnUnavailable (Default Value): true
router.security.cipherSuites (Default Value): TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_AES_128_GCM_SHA256,TLS_AES_256_GCM_SHA384,TLS_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305
router.serviceRegistry.accessUrl (Applicative Resolution): http://localhost:8040/access
router.serviceRegistry.insecure (Default Value): false
router.serviceRegistry.requestTimeout (Default Value): 15s
router.serviceRegistry.skipJoinConnectivityTest (Default Value): false
router.serviceRegistry.topologyServiceTokenExpirySecs (Default Value): 7200
router.serviceRegistry.topologyUrl (Applicative Resolution): http://localhost:8040/access
router.serviceRegistry.useTopologyService (Default Value): false
router.supportBundle.aggregationTimeout (Default Value): 1h0m0s
router.topology.external.allowH2c (Default Value): true
router.topology.external.healthRequestTimeoutSecs (Default Value): 5
router.topology.external.protocolFinderCacheSecs (Default Value): 600
router.topology.external.refresh.interval (Default Value): 3s
router.topology.external.refresh.maxStaleHeartbeat (Default Value): 30s
router.topology.external.skipIpCheck (Default Value): false
router.topology.local.enableNodeStatusBroadcast (Default Value): true
router.topology.local.healthCheck.healthyThreshold (Default Value): 2
router.topology.local.healthCheck.interval (Default Value): 5s
router.topology.local.healthCheck.requestTimeout (Default Value): 5s
router.topology.local.healthCheck.unhealthyThreshold (Default Value): 2
router.topology.local.ignorePersistedServices (Default Value): false
router.topology.local.requiredServiceTypes (Environment Variable: JF_ROUTER_TOPOLOGY_LOCAL_REQUIREDSERVICETYPES): jfrt,jfac,jfmd,jffe,jfob
shared.database.driver (File): org.postgresql.Driver
shared.database.password (File): *****
shared.database.type (File): postgresql
shared.database.url (File): jdbc:postgresql://192.168.95.70:5432/artifactory
shared.database.username (File): artifactory
shared.jfrogUrl (Default Value):
shared.logging.consoleLog.format (Default Value): shared.logging.consoleLog.format
shared.logging.enableJsonConsoleLogAppenders (Default Value): false
shared.multiTenant.cellId (Default Value):
shared.multiTenant.enabled (Default Value): false
shared.multiTenant.tenantRegistryClient.caCert (Default Value):
shared.multiTenant.tenantRegistryClient.cacheExpirationSecs (Default Value): 21600
shared.multiTenant.tenantRegistryClient.clientCert (Default Value):
shared.multiTenant.tenantRegistryClient.clientCertKey (Default Value): *****
shared.multiTenant.tenantRegistryClient.dialTimeoutSecs (Default Value): 10
shared.multiTenant.tenantRegistryClient.endpoints (Default Value): []
shared.multiTenant.tenantRegistryClient.requestTimeoutSecs (Default Value): 30
shared.newrelic.appName (Default Value):
shared.newrelic.distributedTracerEnabled (Default Value): true
shared.newrelic.enableDebugLog (Default Value): false
shared.newrelic.enabled (Default Value): false
shared.newrelic.licenseKey (Default Value): *****
shared.node (File): <nil>
shared.node.availabilityZoneId (Default Value):
shared.node.id (Environment Variable: JF_SHARED_NODE_ID): localhost.localdomain
shared.node.ip (Environment Variable: JF_SHARED_NODE_IP): [::1]
shared.node.name (Environment Variable: JF_SHARED_NODE_NAME): localhost
shared.node.port (Default Value): 0
shared.node.tlsEnabled (Default Value): false
shared.security (File): <nil>
shared.security.bootstrapKeysReadTimeoutSecs (Default Value): 300
shared.security.joinKeyFile (Default Value): etc/security/join.key
shared.security.masterKeyExternal (Default Value): false
shared.security.masterKeyFile (Default Value): etc/security/master.key
system.yaml (Environment Variable: JF_SYSTEM_YAML): /opt/jfrog/artifactory/var/etc/system.yaml
topology.grpcPort (Default Value): 8021
topology.port (Default Value): 8020
2025-07-31T16:17:55.903L [tomct] [WARNING] [                ] [org.apache.tomcat.util.digester.Digester] [org.apache.tomcat.util.digester.SetPropertiesRule begin] - Match [Server/Service/Connector] failed to set property [sendReasonPhrase] to [false]
2025/07/31 16:17:56 maxprocs: Leaving GOMAXPROCS=2: CPU quota undefined
2025-07-31T10:47:56.328Z [jfmd] [DEBUG] [                ] [file_resolver.go:71           ] [main                ] - Resolved system configuration file path: /opt/jfrog/artifactory/var/etc/system.yaml
2025-07-31T10:47:56.330Z [jfmd] [TRACE] [                ] [aws_secret_resolver.go:76     ] [main                ] - Config key not set for aws secret (metadata.database.secretsManagerAlias)
2025-07-31T10:47:56.330Z [jfmd] [DEBUG] [                ] [load_config.go:192            ] [main                ] - Adding optional resolver failed: <nil>
2025/07/31 16:17:56 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
2025/07/31 16:17:56 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
2025/07/31 16:17:56 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
2025/07/31 16:17:56 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
2025/07/31 16:17:56 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
2025/07/31 16:17:56 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
2025-07-31T16:17:57.248L [tomct] [INFO ] [                ] [org.apache.coyote.http11.Http11NioProtocol] [org.apache.coyote.AbstractProtocol init] - Initializing ProtocolHandler ["http-nio-127.0.0.1-8040"]
2025-07-31T16:17:57.323L [tomct] [INFO ] [                ] [org.apache.catalina.core.StandardService] [org.apache.catalina.core.StandardService startInternal] - Starting service [Catalina]
2025-07-31T16:17:57.324L [tomct] [INFO ] [                ] [org.apache.catalina.core.StandardEngine] [org.apache.catalina.core.StandardEngine startInternal] - Starting Servlet engine: [Apache Tomcat/10.1.31]
2025-07-31T16:17:57.366L [tomct] [INFO ] [                ] [org.apache.catalina.startup.HostConfig] [org.apache.catalina.startup.HostConfig deployDescriptor] - Deploying deployment descriptor [/opt/jfrog/artifactory/app/access/tomcat/conf/Catalina/localhost/access.xml]
2025-07-31T16:17:57.392L [tomct] [WARNING] [                ] [org.apache.catalina.startup.HostConfig] [org.apache.catalina.startup.HostConfig deployDescriptor] - The path attribute with value [/access] in deployment descriptor [/opt/jfrog/artifactory/app/access/tomcat/conf/Catalina/localhost/access.xml] has been ignored
2025-07-31T16:17:57.403L [tomct] [WARNING] [                ] [org.apache.catalina.startup.HostConfig] [org.apache.catalina.startup.HostConfig deployDescriptor] - A docBase [/opt/jfrog/artifactory/app/access/tomcat/webapps/access.war] inside the host appBase has been specified, and will be ignored
2025/07/31 16:17:57 maxprocs: Leaving GOMAXPROCS=2: CPU quota undefined
[INFO ] JFrog Observability (jfob) service initialization started. Version: 1.31.11 (revision: a761e9718c, build date: 2024-12-23T14:07:19Z) PID: 44414 Home: /opt/jfrog/artifactory
[DEBUG] Resolved system configuration file path: /opt/jfrog/artifactory/var/etc/system.yaml
2025/07/31 16:17:57 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
2025/07/31 16:17:57 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
2025/07/31 16:17:57 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
2025/07/31 16:17:57 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
2025/07/31 16:17:57 Interval log rotation should not be zero or a negative integer. Provided : 0. Log rotation interval has been set to 24 hours
Max number of open files: 32000
Using JF_PRODUCT_HOME: /opt/jfrog/artifactory
Using JF_ARTIFACTORY_PID: /var/run/artifactory.pid
Tomcat started.
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:17:59.414Z","service":"tomcat","loglevel":"WARNING","class":"org.apache.tomcat.util.digester.Digester","message":"Match [Server/Service/Connector] failed to set property [sendReasonPhrase] to [false]"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:18:00.468Z","service":"tomcat","loglevel":"INFO","class":"org.apache.coyote.http11.Http11NioProtocol","message":"Initializing ProtocolHandler ["http-nio-8081"]"}}
2025-07-31T10:48:00.527Z [jfrou] [INFO ] [62e66a4c99c7d390] [security_keys.go:172          ] [main                ] [] - Master key is missing. Pending for 5 seconds with 5m0s timeout
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:18:00.538Z","service":"tomcat","loglevel":"INFO","class":"org.apache.coyote.http11.Http11NioProtocol","message":"Initializing ProtocolHandler ["http-nio-127.0.0.1-8091"]"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:18:00.561Z","service":"tomcat","loglevel":"INFO","class":"org.apache.catalina.core.StandardService","message":"Starting service [Catalina]"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:18:00.562Z","service":"tomcat","loglevel":"INFO","class":"org.apache.catalina.core.StandardEngine","message":"Starting Servlet engine: [Apache Tomcat/10.1.34]"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:18:00.638Z","service":"tomcat","loglevel":"INFO","class":"org.apache.catalina.startup.HostConfig","message":"Deploying deployment descriptor [/opt/jfrog/artifactory/app/artifactory/tomcat/conf/Catalina/localhost/artifactory.xml]"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:18:00.698Z","service":"tomcat","loglevel":"WARNING","class":"org.apache.catalina.startup.HostConfig","message":"The path attribute with value [/artifactory] in deployment descriptor [/opt/jfrog/artifactory/app/artifactory/tomcat/conf/Catalina/localhost/artifactory.xml] has been ignored"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:18:00.731Z","service":"tomcat","loglevel":"WARNING","class":"org.apache.catalina.startup.HostConfig","message":"A docBase [/opt/jfrog/artifactory/app/artifactory/tomcat/webapps/artifactory.war] inside the host appBase has been specified, and will be ignored"}}
2025-07-31T10:48:00.764Z [jffe ] [INFO ] [] [frontend-service.log] [main                ] - frontend (jffe) service initialization started. Version: 1.98.11 Revision: 10000011 PID: 44251 Home: /opt/jfrog/artifactory
2025-07-31T10:48:00.776Z [jffe ] [INFO ] [] [frontend-service.log] [main                ] - Starting FE server in ST mode
2025-07-31T10:48:00.777Z [jffe ] [INFO ] [] [frontend-service.log] [main                ] - attempting pinging artifactory for 300 retires and 1.0s interval for total of 5 minutes
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:18:00.959Z","service":"tomcat","loglevel":"WARNING","class":"org.apache.tomcat.util.digester.Digester","message":"Match [Context/CookieProcessor] failed to set property [forwardSlashIsSeparator] to [false]"}}
2025-07-31T10:48:01.378Z [jfmd ] [INFO ] [                ] [security_keys.go:172          ] [main                ] - Master key is missing. Pending for 5 seconds with 2m0s timeout [init]
2025-07-31T10:48:02.844Z 35[jfob ] [INFO ] [77fe94683fb18bf5] [security_keys.go:172          ] [main                ] - Master key is missing. Pending for 5 seconds with 2m0s timeout [startup]
2025-07-31T10:48:04.807Z [jfrt ] [INFO ] [3f1b8f0805cce138] [o.a.c.h.ArtifactoryHome:252   ] [Catalina-utility-1  ] - Creating Artifactory home at 'null'
2025-07-31T10:48:05.087Z [jfrt ] [INFO ] [3f1b8f0805cce138] [o.a.c.h.HaNodeProperties:65   ] [Catalina-utility-1  ] - Artifactory is running in non-clustered mode.
2025-07-31T10:48:05.164Z [jfrt ] [INFO ] [3f1b8f0805cce138] [tifactoryHomeConfigListener:85] [Catalina-utility-1  ] - Resolved Home: '/opt/jfrog/artifactory
2025-07-31T10:48:05.533Z [jfrou] [INFO ] [62e66a4c99c7d390] [security_keys.go:172          ] [main                ] [] - Master key is missing. Pending for 10 seconds with 5m0s timeout
2025-07-31T10:48:05.534Z [jfrt ] [INFO ] [3f1b8f0805cce138] [o.a.c.h.ArtifactoryHome:1022  ] [Catalina-utility-1  ] - Setting 'condaNew,cocoapodsNew,vcsNew,swiftNew,alpineNew,npmNew,permissions-cache-legacy' to system property 'spring.profiles.active'
2025-07-31T10:48:06.055Z [jfac ] [INFO ] [fffa3850f18a016b] [licationContextInitializer:166] [main                ] - Access (jfac) service initialization started. Version: 7.128.13 Revision: 82813900 PID: 43677 Home: /opt/jfrog/artifactory FIPS Mode: none
2025-07-31T10:48:06.274Z [jfac ] [INFO ] [fffa3850f18a016b] [o.j.a.AccessApplication:50    ] [main                ] - Starting AccessApplication v7.128.13 using Java 17.0.12 with PID 43677 (/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-application-7.128.13.jar started by artifactory in /)
2025-07-31T10:48:06.275Z [jfac ] [INFO ] [fffa3850f18a016b] [o.j.a.AccessApplication:660   ] [main                ] - The following 1 profile is active: "production"
2025-07-31T10:48:06.381Z [jfmd ] [INFO ] [                ] [security_keys.go:172          ] [main                ] - Master key is missing. Pending for 10 seconds with 2m0s timeout [init]
2025-07-31T10:48:06.906Z [jfrt ] [ERROR] [3f1b8f0805cce138] [d.d.l.DbDistributeLocksDao:357] [Catalina-utility-1  ] - Unable to detect database version Unable to get connection from unique lock data source
2025-07-31T10:48:06.951Z [jfrt ] [INFO ] [3f1b8f0805cce138] [o.a.c.h.ArtifactoryHome:1022  ] [Catalina-utility-1  ] - Setting 'condaNew,cocoapodsNew,vcsNew,swiftNew,alpineNew,npmNew,permissions-cache-legacy' to system property 'spring.profiles.active'
2025-07-31T10:48:06.995Z [jfrt ] [INFO ] [                ] [o.j.c.w.FileWatcher:146       ] [file-watcher-poller ] - Starting watch of folder configurations
2025-07-31T10:48:07.847Z 35[jfob ] [INFO ] [77fe94683fb18bf5] [security_keys.go:172          ] [main                ] - Master key is missing. Pending for 10 seconds with 2m0s timeout [startup]
2025-07-31T10:48:08.086Z [jfrt ] [ERROR] [3f1b8f0805cce138] [o.a.c.VersionProviderImpl:144 ] [Catalina-utility-1  ] - Failed to resolve version information: Could not find database table: db_properties
2025-07-31T10:48:08.086Z [jfrt ] [ERROR] [3f1b8f0805cce138] [tifactoryHomeConfigListener:55] [Catalina-utility-1  ] - Failed initializing Home. Caught exception:
java.lang.IllegalStateException: Could not find database table: db_properties
        at org.artifactory.converter.VersionProviderImpl.init(VersionProviderImpl.java:145)
        at org.artifactory.lifecycle.webapp.servlet.BasicConfigurationManager.initVersion(BasicConfigurationManager.java:171)
        at org.artifactory.lifecycle.webapp.servlet.BasicConfigurationManager.initArtifactoryInstallation(BasicConfigurationManager.java:148)
        at org.artifactory.lifecycle.webapp.servlet.BasicConfigurationManager.initialize(BasicConfigurationManager.java:135)
        at org.artifactory.lifecycle.webapp.servlet.ArtifactoryHomeConfigListener.initBasicConfigManager(ArtifactoryHomeConfigListener.java:61)
        at org.artifactory.lifecycle.webapp.servlet.ArtifactoryHomeConfigListener.contextInitialized(ArtifactoryHomeConfigListener.java:53)
        at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4008)
        at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4436)
        at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
        at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:599)
        at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:571)
        at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:654)
        at org.apache.catalina.startup.HostConfig.deployDescriptor(HostConfig.java:635)
        at org.apache.catalina.startup.HostConfig$DeployDescriptor.run(HostConfig.java:1889)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:63)
        at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.IllegalStateException: Could not find database table: db_properties
        at org.artifactory.common.config.db.DbVersionDao.isDbPropertiesTableExists(DbVersionDao.java:51)
        at org.artifactory.converter.VersionProviderImpl.tryToResolveFromDb(VersionProviderImpl.java:233)
        at org.artifactory.converter.VersionProviderImpl.getCompoundVersionDetailsFromDbIfExists(VersionProviderImpl.java:158)
        at org.artifactory.converter.VersionProviderImpl.loadOriginalDbVersion(VersionProviderImpl.java:179)
        at org.artifactory.converter.VersionProviderImpl.init(VersionProviderImpl.java:140)
        ... 20 common frames omitted
Caused by: org.postgresql.util.PSQLException: Connection to 192.168.95.70:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
        at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:346)
        at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
        at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)
        at org.postgresql.Driver.makeConnection(Driver.java:446)
        at org.postgresql.Driver.connect(Driver.java:298)
        at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:137)
        at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:360)
        at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:202)
        at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:461)
        at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:550)
        at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:98)
        at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:111)
        at org.jfrog.storage.wrapper.JFrogDataSourceWrapper.getConnection(JFrogDataSourceWrapper.java:104)
        at org.jfrog.storage.wrapper.JFrogDataSourceWrapper.invoke(JFrogDataSourceWrapper.java:73)
        at jdk.proxy2/jdk.proxy2.$Proxy5.getConnection(Unknown Source)
        at org.jfrog.storage.util.DbUtils.withMetadata(DbUtils.java:765)
        at org.jfrog.storage.util.DbUtils.tableExists(DbUtils.java:275)
        at org.artifactory.common.config.db.DbVersionUtil.isDbPropertiesTableExists(DbVersionUtil.java:50)
        at org.artifactory.common.config.db.DbVersionDao.isDbPropertiesTableExists(DbVersionDao.java:48)
        ... 24 common frames omitted
Caused by: java.net.ConnectException: Connection refused
        at java.base/sun.nio.ch.Net.pollConnect(Native Method)
        at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
        at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:554)
        at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)
        at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
        at java.base/java.net.Socket.connect(Socket.java:633)
        at org.postgresql.core.PGStream.createSocket(PGStream.java:243)
        at org.postgresql.core.PGStream.<init>(PGStream.java:98)
        at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:136)
        at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:262)
        ... 42 common frames omitted
2025-07-31T10:48:08.131Z [jfrt ] [INFO ] [3f1b8f0805cce138] [actoryContextConfigListener:88] [Catalina-utility-1  ] - Artifactory context initialization started for context: null
2025-07-31T10:48:08.131Z [jfrt ] [ERROR] [3f1b8f0805cce138] [actoryContextConfigListener:93] [Catalina-utility-1  ] - Failed initializing Artifactory context: Artifactory home not initialized.
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:18:08.132Z","service":"tomcat","loglevel":"SEVERE","class":"org.apache.catalina.core.StandardContext","message":"One or more listeners failed to start. Full details will be found in the appropriate container log file"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:18:08.140Z","service":"tomcat","loglevel":"SEVERE","class":"org.apache.catalina.core.StandardContext","message":"Context [/artifactory] startup failed due to previous errors"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:18:08.229Z","service":"tomcat","loglevel":"INFO","class":"org.apache.catalina.startup.HostConfig","message":"Deployment of deployment descriptor [/opt/jfrog/artifactory/app/artifactory/tomcat/conf/Catalina/localhost/artifactory.xml] has finished in [7,591] ms"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:18:08.238Z","service":"tomcat","loglevel":"INFO","class":"org.apache.catalina.startup.HostConfig","message":"Deploying web application directory [/opt/jfrog/artifactory/app/artifactory/tomcat/webapps/ROOT]"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:18:08.241Z","service":"tomcat","loglevel":"WARNING","class":"org.apache.tomcat.util.digester.Digester","message":"Match [Context/CookieProcessor] failed to set property [forwardSlashIsSeparator] to [false]"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:18:08.335Z","service":"tomcat","loglevel":"INFO","class":"org.apache.catalina.startup.HostConfig","message":"Deployment of web application directory [/opt/jfrog/artifactory/app/artifactory/tomcat/webapps/ROOT] has finished in [97] ms"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:18:08.345Z","service":"tomcat","loglevel":"INFO","class":"org.apache.coyote.http11.Http11NioProtocol","message":"Starting ProtocolHandler ["http-nio-8081"]"}}
{"log_name":"tomcat-catalina.log","app":{"datetime":"2025-07-31T16:18:08.437Z","service":"tomcat","loglevel":"INFO","class":"org.apache.coyote.http11.Http11NioProtocol","message":"Starting ProtocolHandler ["http-nio-127.0.0.1-8091"]"}}
2025-07-31T10:48:09.461Z [shell] [INFO ] [] [artifactoryManage.sh:135      ] [main] - Artifactory Tomcat started in normal mode
2025-07-31T10:48:09.485Z [shell] [INFO ] [] [artifactoryCommon.sh:1248     ] [main] - Artifactory running with PID 44594
2025-07-31T10:48:10.538Z [jfrou] [INFO ] [62e66a4c99c7d390] [security_keys.go:172          ] [main                ] [] - Master key is missing. Pending for 15 seconds with 5m0s timeout
2025-07-31T10:48:10.759Z [jffe ] [INFO ] [] [frontend-service.log] [main                ] - pinging artifactory, attempt number 10
2025-07-31T10:48:10.764Z [jffe ] [INFO ] [] [frontend-service.log] [main                ] - pinging artifactory attempt number 10 failed with code : ECONNREFUSED
2025-07-31T10:48:11.384Z [jfmd ] [INFO ] [                ] [security_keys.go:172          ] [main                ] - Master key is missing. Pending for 15 seconds with 2m0s timeout [init]
2025-07-31T10:48:12.852Z 35[jfob ] [INFO ] [77fe94683fb18bf5] [security_keys.go:172          ] [main                ] - Master key is missing. Pending for 15 seconds with 2m0s timeout [startup]
2025-07-31T10:48:13.809Z [jfac ] [ERROR] [fffa3850f18a016b] [.s.d.u.AccessJdbcHelperImpl:87] [main                ] - Could not initialize database:
org.flywaydb.core.internal.dbsupport.FlywaySqlException:
Unable to obtain Jdbc connection from DataSource (jdbc:postgresql://192.168.95.70:5432/artifactory?autosave=conservative) for user 'artifactory': Connection to 192.168.95.70:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
SQL State  : 08001
Error Code : 0
Message    : Connection to 192.168.95.70:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.

        at org.flywaydb.core.internal.util.jdbc.DriverDataSource.getConnectionFromDriver(DriverDataSource.java:396)
        at org.flywaydb.core.internal.util.jdbc.DriverDataSource.getConnection(DriverDataSource.java:358)
        at org.flywaydb.core.internal.util.jdbc.JdbcUtils.openConnection(JdbcUtils.java:51)
        at org.flywaydb.core.Flyway.execute(Flyway.java:1367)
        at org.flywaydb.core.Flyway.info(Flyway.java:1056)
        at org.jfrog.access.server.db.spring.AccessFlywayConfiguration.shouldAddInitMigration(AccessFlywayConfiguration.java:83)
        at org.jfrog.access.server.db.spring.AccessFlywayConfiguration.addInitLocation(AccessFlywayConfiguration.java:64)
        at org.jfrog.access.server.db.spring.AccessFlywayConfiguration.createFlyway(AccessFlywayConfiguration.java:46)
        at org.jfrog.storage.util.migration.MigrationUtil.initSchemas(MigrationUtil.java:99)
        at org.jfrog.access.server.db.util.AccessJdbcHelperImpl.init(AccessJdbcHelperImpl.java:85)
        at org.jfrog.access.server.db.util.AccessJdbcHelperImpl.<init>(AccessJdbcHelperImpl.java:70)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
        at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:208)
        at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:117)
        at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:315)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:306)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
        at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
        at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
        at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
        at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
        at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeansOfType(DefaultListableBeanFactory.java:665)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeansOfType(DefaultListableBeanFactory.java:653)
        at org.springframework.context.support.AbstractApplicationContext.getBeansOfType(AbstractApplicationContext.java:1389)
        at org.jfrog.access.rest.config.JerseyConfig.registerAdditionalFilters(JerseyConfig.java:80)
        at org.jfrog.access.rest.config.JerseyConfig.<init>(JerseyConfig.java:41)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
        at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:208)
        at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:117)
        at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:315)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:306)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
        at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:409)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:205)
        at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:211)
        at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:202)
        at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addServletContextInitializerBeans(ServletContextInitializerBeans.java:97)
        at org.springframework.boot.web.servlet.ServletContextInitializerBeans.<init>(ServletContextInitializerBeans.java:86)
        at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:266)
        at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:240)
        at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:197)
        at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:162)
        at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:619)
        at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
        at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754)
        at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:456)
        at org.springframework.boot.SpringApplication.run(SpringApplication.java:335)
        at org.springframework.boot.web.servlet.support.SpringBootServletInitializer.run(SpringBootServletInitializer.java:174)
        at org.springframework.boot.web.servlet.support.SpringBootServletInitializer.createRootApplicationContext(SpringBootServletInitializer.java:154)
        at org.springframework.boot.web.servlet.support.SpringBootServletInitializer.onStartup(SpringBootServletInitializer.java:96)
        at org.springframework.web.SpringServletContainerInitializer.onStartup(SpringServletContainerInitializer.java:171)
        at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4412)
        at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
        at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:599)
        at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:571)
        at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:654)
        at org.apache.catalina.startup.HostConfig.deployDescriptor(HostConfig.java:635)
        at org.apache.catalina.startup.HostConfig$DeployDescriptor.run(HostConfig.java:1889)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
        at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:123)
        at org.apache.catalina.startup.HostConfig.deployDescriptors(HostConfig.java:530)
        at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:421)
        at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1629)
        at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:303)
        at org.apache.catalina.util.LifecycleBase.fireLifecycleEvent(LifecycleBase.java:109)
        at org.apache.catalina.util.LifecycleBase.setStateInternal(LifecycleBase.java:385)
        at org.apache.catalina.util.LifecycleBase.setState(LifecycleBase.java:332)
        at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:776)
        at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:772)
        at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
        at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
        at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
        at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
        at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
        at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:203)
        at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
        at org.apache.catalina.core.StandardService.startInternal(StandardService.java:415)
        at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
        at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:870)
        at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
        at org.apache.catalina.startup.Catalina.start(Catalina.java:757)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:345)
        at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:473)
Caused by: org.postgresql.util.PSQLException: Connection to 192.168.95.70:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
        at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:352)
        at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
        at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)
        at org.postgresql.Driver.makeConnection(Driver.java:446)
        at org.postgresql.Driver.connect(Driver.java:298)
        at org.flywaydb.core.internal.util.jdbc.DriverDataSource.getConnectionFromDriver(DriverDataSource.java:393)
        ... 199 common frames omitted
Caused by: java.net.ConnectException: Connection refused
        at java.base/sun.nio.ch.Net.pollConnect(Native Method)
        at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
        at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:554)
        at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)
        at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
        at java.base/java.net.Socket.connect(Socket.java:633)
        at org.postgresql.core.PGStream.createSocket(PGStream.java:260)
        at org.postgresql.core.PGStream.<init>(PGStream.java:121)
        at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)
        at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)
        ... 204 common frames omitted
2025-07-31T10:48:13.823Z [jfac ] [WARN ] [fffa3850f18a016b] [ebServerApplicationContext:633] [main                ] - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.context.ApplicationContextException: Unable to start web server
2025-07-31T10:48:13.893Z [jfac ] [ERROR] [fffa3850f18a016b] [o.s.b.SpringApplication:859   ] [main                ] - Application run failed
org.springframework.context.ApplicationContextException: Unable to start web server
        at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:165)
        at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:619)
        at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
        at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754)
        at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:456)
        at org.springframework.boot.SpringApplication.run(SpringApplication.java:335)
        at org.springframework.boot.web.servlet.support.SpringBootServletInitializer.run(SpringBootServletInitializer.java:174)
        at org.springframework.boot.web.servlet.support.SpringBootServletInitializer.createRootApplicationContext(SpringBootServletInitializer.java:154)
        at org.springframework.boot.web.servlet.support.SpringBootServletInitializer.onStartup(SpringBootServletInitializer.java:96)
        at org.springframework.web.SpringServletContainerInitializer.onStartup(SpringServletContainerInitializer.java:171)
        at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4412)
        at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
        at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:599)
        at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:571)
        at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:654)
        at org.apache.catalina.startup.HostConfig.deployDescriptor(HostConfig.java:635)
        at org.apache.catalina.startup.HostConfig$DeployDescriptor.run(HostConfig.java:1889)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
        at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:123)
        at org.apache.catalina.startup.HostConfig.deployDescriptors(HostConfig.java:530)
        at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:421)
        at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1629)
        at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:303)
        at org.apache.catalina.util.LifecycleBase.fireLifecycleEvent(LifecycleBase.java:109)
        at org.apache.catalina.util.LifecycleBase.setStateInternal(LifecycleBase.java:385)
        at org.apache.catalina.util.LifecycleBase.setState(LifecycleBase.java:332)
        at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:776)
        at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:772)
        at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
        at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
        at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
        at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
        at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
        at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:203)
        at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
        at org.apache.catalina.core.StandardService.startInternal(StandardService.java:415)
        at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
        at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:870)
        at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
        at org.apache.catalina.startup.Catalina.start(Catalina.java:757)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:345)
        at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:473)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'org.springframework.boot.autoconfigure.jersey.JerseyAutoConfiguration': Unsatisfied dependency expressed through constructor parameter 1: Error creating bean with name 'jerseyConfig' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-application-7.128.13.jar!/org/jfrog/access/rest/config/JerseyConfig.class]: Failed to instantiate [org.jfrog.access.rest.config.JerseyConfig]: Constructor threw exception
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:409)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:205)
        at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:211)
        at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:202)
        at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addServletContextInitializerBeans(ServletContextInitializerBeans.java:97)
        at org.springframework.boot.web.servlet.ServletContextInitializerBeans.<init>(ServletContextInitializerBeans.java:86)
        at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:266)
        at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:240)
        at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:197)
        at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:162)
        ... 49 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'jerseyConfig' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-application-7.128.13.jar!/org/jfrog/access/rest/config/JerseyConfig.class]: Failed to instantiate [org.jfrog.access.rest.config.JerseyConfig]: Constructor threw exception
        at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:318)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:306)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
        at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
        ... 75 common frames omitted
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.jfrog.access.rest.config.JerseyConfig]: Constructor threw exception
        at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:221)
        at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:117)
        at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:315)
        ... 89 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'authenticationFilter' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-rest-7.128.13.jar!/org/jfrog/access/server/rest/filter/AuthenticationFilter.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'authenticationServiceImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/service/auth/AuthenticationServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userStorageServiceImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/service/db/UserStorageServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'accessUsersDao' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/dao/AccessUsersDao.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'sqlDaoHelperFactory' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/dao/SqlDaoHelperFactory.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'accessJdbcHelperImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/util/AccessJdbcHelperImpl.class]: Failed to instantiate [org.jfrog.access.server.db.util.AccessJdbcHelperImpl]: Constructor threw exception
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeansOfType(DefaultListableBeanFactory.java:665)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeansOfType(DefaultListableBeanFactory.java:653)
        at org.springframework.context.support.AbstractApplicationContext.getBeansOfType(AbstractApplicationContext.java:1389)
        at org.jfrog.access.rest.config.JerseyConfig.registerAdditionalFilters(JerseyConfig.java:80)
        at org.jfrog.access.rest.config.JerseyConfig.<init>(JerseyConfig.java:41)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
        at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:208)
        ... 91 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'authenticationServiceImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/service/auth/AuthenticationServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userStorageServiceImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/service/db/UserStorageServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'accessUsersDao' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/dao/AccessUsersDao.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'sqlDaoHelperFactory' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/dao/SqlDaoHelperFactory.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'accessJdbcHelperImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/util/AccessJdbcHelperImpl.class]: Failed to instantiate [org.jfrog.access.server.db.util.AccessJdbcHelperImpl]: Constructor threw exception
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
        at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
        ... 111 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'userStorageServiceImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/service/db/UserStorageServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'accessUsersDao' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/dao/AccessUsersDao.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'sqlDaoHelperFactory' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/dao/SqlDaoHelperFactory.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'accessJdbcHelperImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/util/AccessJdbcHelperImpl.class]: Failed to instantiate [org.jfrog.access.server.db.util.AccessJdbcHelperImpl]: Constructor threw exception
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
        at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
        ... 125 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'accessUsersDao' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/dao/AccessUsersDao.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'sqlDaoHelperFactory' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/dao/SqlDaoHelperFactory.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'accessJdbcHelperImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/util/AccessJdbcHelperImpl.class]: Failed to instantiate [org.jfrog.access.server.db.util.AccessJdbcHelperImpl]: Constructor threw exception
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
        at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
        ... 139 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'sqlDaoHelperFactory' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/dao/SqlDaoHelperFactory.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'accessJdbcHelperImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/util/AccessJdbcHelperImpl.class]: Failed to instantiate [org.jfrog.access.server.db.util.AccessJdbcHelperImpl]: Constructor threw exception
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
        at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
        ... 153 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'accessJdbcHelperImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/util/AccessJdbcHelperImpl.class]: Failed to instantiate [org.jfrog.access.server.db.util.AccessJdbcHelperImpl]: Constructor threw exception
        at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:318)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:306)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
        at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
        ... 167 common frames omitted
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.jfrog.access.server.db.util.AccessJdbcHelperImpl]: Constructor threw exception
        at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:221)
        at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:117)
        at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:315)
        ... 181 common frames omitted
Caused by: org.flywaydb.core.internal.dbsupport.FlywaySqlException:
Unable to obtain Jdbc connection from DataSource (jdbc:postgresql://192.168.95.70:5432/artifactory?autosave=conservative) for user 'artifactory': Connection to 192.168.95.70:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
SQL State  : 08001
Error Code : 0
Message    : Connection to 192.168.95.70:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.

        at org.flywaydb.core.internal.util.jdbc.DriverDataSource.getConnectionFromDriver(DriverDataSource.java:396)
        at org.flywaydb.core.internal.util.jdbc.DriverDataSource.getConnection(DriverDataSource.java:358)
        at org.flywaydb.core.internal.util.jdbc.JdbcUtils.openConnection(JdbcUtils.java:51)
        at org.flywaydb.core.Flyway.execute(Flyway.java:1367)
        at org.flywaydb.core.Flyway.info(Flyway.java:1056)
        at org.jfrog.access.server.db.spring.AccessFlywayConfiguration.shouldAddInitMigration(AccessFlywayConfiguration.java:83)
        at org.jfrog.access.server.db.spring.AccessFlywayConfiguration.addInitLocation(AccessFlywayConfiguration.java:64)
        at org.jfrog.access.server.db.spring.AccessFlywayConfiguration.createFlyway(AccessFlywayConfiguration.java:46)
        at org.jfrog.storage.util.migration.MigrationUtil.initSchemas(MigrationUtil.java:99)
        at org.jfrog.access.server.db.util.AccessJdbcHelperImpl.init(AccessJdbcHelperImpl.java:85)
        at org.jfrog.access.server.db.util.AccessJdbcHelperImpl.<init>(AccessJdbcHelperImpl.java:70)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
        at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:208)
        ... 183 common frames omitted
Caused by: org.postgresql.util.PSQLException: Connection to 192.168.95.70:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
        at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:352)
        at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
        at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)
        at org.postgresql.Driver.makeConnection(Driver.java:446)
        at org.postgresql.Driver.connect(Driver.java:298)
        at org.flywaydb.core.internal.util.jdbc.DriverDataSource.getConnectionFromDriver(DriverDataSource.java:393)
        ... 199 common frames omitted
Caused by: java.net.ConnectException: Connection refused
        at java.base/sun.nio.ch.Net.pollConnect(Native Method)
        at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
        at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:554)
        at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)
        at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
        at java.base/java.net.Socket.connect(Socket.java:633)
        at org.postgresql.core.PGStream.createSocket(PGStream.java:260)
        at org.postgresql.core.PGStream.<init>(PGStream.java:121)
        at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)
        at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)
        ... 204 common frames omitted
2025-07-31T16:18:13.900L [tomct] [SEVERE] [                ] [org.apache.catalina.startup.HostConfig] [org.apache.catalina.startup.HostConfig deployDescriptor] - Error deploying deployment descriptor [/opt/jfrog/artifactory/app/access/tomcat/conf/Catalina/localhost/access.xml]
java.lang.IllegalStateException: Error starting child
        at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:602)
        at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:571)
        at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:654)
        at org.apache.catalina.startup.HostConfig.deployDescriptor(HostConfig.java:635)
        at org.apache.catalina.startup.HostConfig$DeployDescriptor.run(HostConfig.java:1889)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
        at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:123)
        at org.apache.catalina.startup.HostConfig.deployDescriptors(HostConfig.java:530)
        at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:421)
        at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1629)
        at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:303)
        at org.apache.catalina.util.LifecycleBase.fireLifecycleEvent(LifecycleBase.java:109)
        at org.apache.catalina.util.LifecycleBase.setStateInternal(LifecycleBase.java:385)
        at org.apache.catalina.util.LifecycleBase.setState(LifecycleBase.java:332)
        at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:776)
        at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:772)
        at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
        at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
        at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
        at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
        at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
        at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:203)
        at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
        at org.apache.catalina.core.StandardService.startInternal(StandardService.java:415)
        at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
        at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:870)
        at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
        at org.apache.catalina.startup.Catalina.start(Catalina.java:757)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:345)
        at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:473)
Caused by: org.apache.catalina.LifecycleException: Failed to start component [StandardEngine[Catalina].StandardHost[localhost].StandardContext[/access]]
        at org.apache.catalina.util.LifecycleBase.handleSubClassException(LifecycleBase.java:402)
        at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:179)
        at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:599)
        ... 37 more
Caused by: org.springframework.context.ApplicationContextException: Unable to start web server
        at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:165)
        at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:619)
        at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
        at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754)
        at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:456)
        at org.springframework.boot.SpringApplication.run(SpringApplication.java:335)
        at org.springframework.boot.web.servlet.support.SpringBootServletInitializer.run(SpringBootServletInitializer.java:174)
        at org.springframework.boot.web.servlet.support.SpringBootServletInitializer.createRootApplicationContext(SpringBootServletInitializer.java:154)
        at org.springframework.boot.web.servlet.support.SpringBootServletInitializer.onStartup(SpringBootServletInitializer.java:96)
        at org.springframework.web.SpringServletContainerInitializer.onStartup(SpringServletContainerInitializer.java:171)
        at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4412)
        at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
        ... 38 more
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'org.springframework.boot.autoconfigure.jersey.JerseyAutoConfiguration': Unsatisfied dependency expressed through constructor parameter 1: Error creating bean with name 'jerseyConfig' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-application-7.128.13.jar!/org/jfrog/access/rest/config/JerseyConfig.class]: Failed to instantiate [org.jfrog.access.rest.config.JerseyConfig]: Constructor threw exception
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:409)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:205)
        at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:211)
        at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:202)
        at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addServletContextInitializerBeans(ServletContextInitializerBeans.java:97)
        at org.springframework.boot.web.servlet.ServletContextInitializerBeans.<init>(ServletContextInitializerBeans.java:86)
        at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:266)
        at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:240)
        at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:197)
        at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:162)
        ... 49 more
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'jerseyConfig' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-application-7.128.13.jar!/org/jfrog/access/rest/config/JerseyConfig.class]: Failed to instantiate [org.jfrog.access.rest.config.JerseyConfig]: Constructor threw exception
        at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:318)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:306)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
        at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
        ... 75 more
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.jfrog.access.rest.config.JerseyConfig]: Constructor threw exception
        at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:221)
        at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:117)
        at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:315)
        ... 89 more
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'authenticationFilter' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-rest-7.128.13.jar!/org/jfrog/access/server/rest/filter/AuthenticationFilter.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'authenticationServiceImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/service/auth/AuthenticationServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userStorageServiceImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/service/db/UserStorageServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'accessUsersDao' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/dao/AccessUsersDao.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'sqlDaoHelperFactory' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/dao/SqlDaoHelperFactory.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'accessJdbcHelperImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/util/AccessJdbcHelperImpl.class]: Failed to instantiate [org.jfrog.access.server.db.util.AccessJdbcHelperImpl]: Constructor threw exception
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeansOfType(DefaultListableBeanFactory.java:665)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeansOfType(DefaultListableBeanFactory.java:653)
        at org.springframework.context.support.AbstractApplicationContext.getBeansOfType(AbstractApplicationContext.java:1389)
        at org.jfrog.access.rest.config.JerseyConfig.registerAdditionalFilters(JerseyConfig.java:80)
        at org.jfrog.access.rest.config.JerseyConfig.<init>(JerseyConfig.java:41)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
        at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:208)
        ... 91 more
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'authenticationServiceImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/service/auth/AuthenticationServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userStorageServiceImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/service/db/UserStorageServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'accessUsersDao' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/dao/AccessUsersDao.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'sqlDaoHelperFactory' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/dao/SqlDaoHelperFactory.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'accessJdbcHelperImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/util/AccessJdbcHelperImpl.class]: Failed to instantiate [org.jfrog.access.server.db.util.AccessJdbcHelperImpl]: Constructor threw exception
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
        at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
        ... 111 more
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'userStorageServiceImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/service/db/UserStorageServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'accessUsersDao' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/dao/AccessUsersDao.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'sqlDaoHelperFactory' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/dao/SqlDaoHelperFactory.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'accessJdbcHelperImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/util/AccessJdbcHelperImpl.class]: Failed to instantiate [org.jfrog.access.server.db.util.AccessJdbcHelperImpl]: Constructor threw exception
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
        at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
        ... 125 more
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'accessUsersDao' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/dao/AccessUsersDao.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'sqlDaoHelperFactory' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/dao/SqlDaoHelperFactory.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'accessJdbcHelperImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/util/AccessJdbcHelperImpl.class]: Failed to instantiate [org.jfrog.access.server.db.util.AccessJdbcHelperImpl]: Constructor threw exception
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
        at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
        ... 139 more
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'sqlDaoHelperFactory' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/dao/SqlDaoHelperFactory.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'accessJdbcHelperImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/util/AccessJdbcHelperImpl.class]: Failed to instantiate [org.jfrog.access.server.db.util.AccessJdbcHelperImpl]: Constructor threw exception
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
        at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
        ... 153 more
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'accessJdbcHelperImpl' defined in URL [jar:file:/opt/jfrog/artifactory/app/access/tomcat/webapps/access/WEB-INF/lib/access-server-core-7.128.13.jar!/org/jfrog/access/server/db/util/AccessJdbcHelperImpl.class]: Failed to instantiate [org.jfrog.access.server.db.util.AccessJdbcHelperImpl]: Constructor threw exception
        at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:318)
        at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:306)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
        at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
        at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
        at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
        ... 167 more
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.jfrog.access.server.db.util.AccessJdbcHelperImpl]: Constructor threw exception
        at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:221)
        at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:117)
        at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:315)
        ... 181 more
Caused by: org.flywaydb.core.internal.dbsupport.FlywaySqlException:
Unable to obtain Jdbc connection from DataSource (jdbc:postgresql://192.168.95.70:5432/artifactory?autosave=conservative) for user 'artifactory': Connection to 192.168.95.70:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
SQL State  : 08001
Error Code : 0
Message    : Connection to 192.168.95.70:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.

        at org.flywaydb.core.internal.util.jdbc.DriverDataSource.getConnectionFromDriver(DriverDataSource.java:396)
        at org.flywaydb.core.internal.util.jdbc.DriverDataSource.getConnection(DriverDataSource.java:358)
        at org.flywaydb.core.internal.util.jdbc.JdbcUtils.openConnection(JdbcUtils.java:51)
        at org.flywaydb.core.Flyway.execute(Flyway.java:1367)
        at org.flywaydb.core.Flyway.info(Flyway.java:1056)
        at org.jfrog.access.server.db.spring.AccessFlywayConfiguration.shouldAddInitMigration(AccessFlywayConfiguration.java:83)
        at org.jfrog.access.server.db.spring.AccessFlywayConfiguration.addInitLocation(AccessFlywayConfiguration.java:64)
        at org.jfrog.access.server.db.spring.AccessFlywayConfiguration.createFlyway(AccessFlywayConfiguration.java:46)
        at org.jfrog.storage.util.migration.MigrationUtil.initSchemas(MigrationUtil.java:99)
        at org.jfrog.access.server.db.util.AccessJdbcHelperImpl.init(AccessJdbcHelperImpl.java:85)
        at org.jfrog.access.server.db.util.AccessJdbcHelperImpl.<init>(AccessJdbcHelperImpl.java:70)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
        at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:208)
        ... 183 more
Caused by: org.postgresql.util.PSQLException: Connection to 192.168.95.70:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
        at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:352)
        at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
        at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)
        at org.postgresql.Driver.makeConnection(Driver.java:446)
        at org.postgresql.Driver.connect(Driver.java:298)
        at org.flywaydb.core.internal.util.jdbc.DriverDataSource.getConnectionFromDriver(DriverDataSource.java:393)
        ... 199 more
Caused by: java.net.ConnectException: Connection refused
        at java.base/sun.nio.ch.Net.pollConnect(Native Method)
        at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
        at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:554)
        at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)
        at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
        at java.base/java.net.Socket.connect(Socket.java:633)
        at org.postgresql.core.PGStream.createSocket(PGStream.java:260)
        at org.postgresql.core.PGStream.<init>(PGStream.java:121)
        at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)
        at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)
        ... 204 more

2025-07-31T16:18:13.909L [tomct] [INFO ] [                ] [org.apache.catalina.startup.HostConfig] [org.apache.catalina.startup.HostConfig deployDescriptor] - Deployment of deployment descriptor [/opt/jfrog/artifactory/app/access/tomcat/conf/Catalina/localhost/access.xml] has finished in [16,544] ms
2025-07-31T16:18:13.911L [tomct] [INFO ] [                ] [org.apache.catalina.startup.HostConfig] [org.apache.catalina.startup.HostConfig deployDirectory] - Deploying web application directory [/opt/jfrog/artifactory/app/access/tomcat/webapps/ROOT]
2025-07-31T16:18:13.945L [tomct] [INFO ] [                ] [org.apache.catalina.startup.HostConfig] [org.apache.catalina.startup.HostConfig deployDirectory] - Deployment of web application directory [/opt/jfrog/artifactory/app/access/tomcat/webapps/ROOT] has finished in [34] ms
2025-07-31T16:18:13.949L [tomct] [INFO ] [                ] [org.apache.coyote.http11.Http11NioProtocol] [org.apache.coyote.AbstractProtocol start] - Starting ProtocolHandler ["http-nio-127.0.0.1-8040"]
2025-07-31T10:48:15.542Z [jfrou] [INFO ] [62e66a4c99c7d390] [security_keys.go:172          ] [main                ] [] - Master key is missing. Pending for 20 seconds with 5m0s timeout
^C
[root@localhost log]# nc -zv 192.168.95.70 5432
Ncat: Version 7.92 ( https://nmap.org/ncat )
Ncat: Connection refused.
[root@localhost log]# getenforce
Permissive
[root@localhost log]# systemctl status firewalld
○ firewalld.service - firewalld - dynamic firewall daemon
     Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; preset: enabled)
     Active: inactive (dead)
       Docs: man:firewalld(1)

Jul 29 19:35:21 localhost systemd[1]: Starting firewalld - dynamic firewall daemon...
Jul 29 19:35:23 localhost systemd[1]: Started firewalld - dynamic firewall daemon.
Jul 31 16:09:22 localhost.localdomain systemd[1]: Stopping firewalld - dynamic firewall daemon...
Jul 31 16:09:22 localhost.localdomain systemd[1]: firewalld.service: Deactivated successfully.
Jul 31 16:09:22 localhost.localdomain systemd[1]: Stopped firewalld - dynamic firewall daemon.
[root@localhost log]# nc -zv 192.168.95.69 5432
Ncat: Version 7.92 ( https://nmap.org/ncat )
Ncat: Connection refused.
[root@localhost log]# cd
[root@localhost ~]# cd /usr/
bin/     games/   include/ lib/     lib64/   libexec/ local/   sbin/    share/   src/     tmp/
[root@localhost ~]# cd /usr/
bin/     games/   include/ lib/     lib64/   libexec/ local/   sbin/    share/   src/     tmp/
[root@localhost ~]# cd /usr/
[root@localhost usr]# kk
bash: kk: command not found...
ls
^C
[root@localhost usr]# ll
total 220
dr-xr-xr-x.   2 root root 45056 Jul 29 19:16 bin
drwxr-xr-x.   2 root root     6 Aug 10  2021 games
drwxr-xr-x.   3 root root    23 Jul 29 19:13 include
dr-xr-xr-x.  38 root root  4096 Jul 29 19:21 lib
dr-xr-xr-x. 113 root root 61440 Jul 29 19:16 lib64
drwxr-xr-x.  46 root root  8192 Jul 29 19:16 libexec
drwxr-xr-x.  12 root root   131 Jul 29 19:10 local
dr-xr-xr-x.   2 root root 16384 Jul 29 19:16 sbin
drwxr-xr-x. 200 root root  8192 Jul 29 19:16 share
drwxr-xr-x.   4 root root    34 Jul 29 19:10 src
lrwxrwxrwx.   1 root root    10 Aug 10  2021 tmp -> ../var/tmp
[root@localhost usr]# ip r l
default via 192.168.10.1 dev ens192 proto static metric 100
192.168.0.0/16 dev ens192 proto kernel scope link src 192.168.95.71 metric 100
[root@localhost usr]# nc -zv 192.168.95.70 5432
Ncat: Version 7.92 ( https://nmap.org/ncat )
Ncat: Connected to 192.168.95.70:5432.
Ncat: 0 bytes sent, 0 bytes received in 0.05 seconds.
[root@localhost usr]# systemctl restart artifactory.service
[root@localhost usr]# ip r
default via 192.168.10.1 dev ens192 proto static metric 100
192.168.0.0/16 dev ens192 proto kernel scope link src 192.168.95.71 metric 100
[root@localhost usr]#











=======================




login as: root
root@192.168.95.70's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 16:19:38 2025 from 192.168.95.71
[root@JCR-vm ~]# cd /etc/nginx/
[root@JCR-vm nginx]# cd ssl/
[root@JCR-vm ssl]# ls -ltrh
total 20K
-rw-------. 1 root root 1.7K Jul 31 15:35 server.key_old
-rw-r--r--. 1 root root 1.2K Jul 31 15:35 server.crt_old
-rw-r--r--. 1 root root  314 Jul 31 15:41 openssl-san.cnf
-rw-------. 1 root root 1.7K Jul 31 15:41 server.key
-rw-r--r--. 1 root root 1.3K Jul 31 15:41 server.crt
[root@JCR-vm ssl]# vim openssl-san.cnf
[root@JCR-vm ssl]#
[root@JCR-vm ssl]#
[root@JCR-vm ssl]#
(reverse-i-search)`openssl': vim ^Censsl-san.cnf
[root@JCR-vm ssl]# history
    1  sudo yum update
    2  cd /root/jfrog/artifactory-jcr-7.117.8/var/log
    3  ip r l
    4  exit
    5  ls
    6  clear
    7  l
    8  ll
    9  sudo yum update
   10  ping google.com
   11  ping www.google.com
   12  ping google.com
   13  ping
   14  ping http://google.com
   15  ping google.com
   16  curl www.google.com
   17  nslookup google.com
   18  clear
   19  ls
   20  ll
   21  cd java/
   22  ls
   23  sudo yum localinstall *.rpm --disablerepo="*" --skip-broken
   24  java -version
   25  ckear'clear
   26  clear
   27  ls
   28  nslookup google.com
   29  ip show
   30  nslookup google.com
   31  cd /etc/resolv.conf
   32  cat /etc/resolv.conf
   33  ip -br add
   34  history
   35  ls
   36  clear
   37  ls
   38  java -version
   39  ls
   40  cd postgres/
   41  ls
   42  sudo yum localinstall *.rpm --disablerepo="*" --skip-broken
   43  ls
   44  sudo /usr/pgsql-16/bin/postgresql-16-setup initdb
   45  sudo systemctl enable postgresql-16
   46  sudo systemctl start postgresql-16
   47  sudo systemctl status postgresql-16
   48  sudo passwd postgres
   49  sudo su - postgres
   50  ls
   51  cd
   52  ls
   53  rm -rf jfrog
   54  ls
   55  mv jfrog-artifactory-jcr.tar.gz /opt/
   56  ls
   57  cd /opt/
   58  ls
   59  export JFROG_HOME=/opt/
   60  ls
   61  tar -xvzf jfrog-artifactory-jcr.tar.gz
   62  ls
   63  mv artifactory-jcr-7.117.8/ artifactory
   64  ls
   65  cd artifactory/
   66  ls
   67  cd app/
   68  ls
   69  cd bin/
   70  ls
   71  ./installService.sh
   72  vo /opt/artifactory/var/etc/system.yaml
   73  vi /opt/artifactory/var/etc/system.yamla
   74  vi /opt/artifactory/var/etc/system.yaml
   75  systemctl start artifactory.service
   76  systemctl status artifactory.service
   77  systemctl status artifactory.service
   78  curl http://192.168.95.70:8082/
   79  getenforce
   80  setenforce 0
   81  systemctl status firewalld
   82  systemctl stop firewalld
   83  systemctl disable firewalld
   84  ip r l
   85  exit
   86  cd nginx/
   87  ls -ltrh
   88  exit
   89  ll
   90  systemctl status artifactory.service
   91  cd /opt/artifactory/
   92  ls
   93  cd var/etc
   94  ls
   95  vim system.yaml
   96  exit
   97  netstat -tulnp
   98  cd /usr/pgsql-16/
   99  ls
  100  cd bin/
  101  ls
  102  cd ..
  103  ls
  104  cd lib/
  105  ls
  106  cd ..
  107  ls
  108  cd share/
  109  ls
  110  systemctl status postgresql-16.service
  111  cat /usr/lib/systemd/system/postgresql-16.service
  112  cd /var/lib/pgsql/16/data/
  113  ls -ltr
  114  vim postgresql.conf
  115  systemctl restart postgresql-16.service
  116  systemctl status postgresql-16.service
  117  netstat -tunlp
  118  exit
  119  cd /etc/nginx/
  120  cd ssl/
  121  ls -ltrh
  122  vim openssl-san.cnf
  123  history
[root@JCR-vm ssl]# pwd
/etc/nginx/ssl
[root@JCR-vm ssl]# ls -ltrh
total 20K
-rw-------. 1 root root 1.7K Jul 31 15:35 server.key_old
-rw-r--r--. 1 root root 1.2K Jul 31 15:35 server.crt_old
-rw-------. 1 root root 1.7K Jul 31 15:41 server.key
-rw-r--r--. 1 root root 1.3K Jul 31 15:41 server.crt
-rw-r--r--. 1 root root  404 Jul 31 16:44 openssl-san.cnf
[root@JCR-vm ssl]# openssl req -x509 -nodes -days 365 \
  -newkey rsa:2048 \
  -keyout /etc/nginx/ssl/server.key \
  -out /etc/nginx/ssl/server.crt \
  -config openssl-san.cnf
......+.+...+++++++++++++++++++++++++++++++++++++++*.+......+...+++++++++++++++++++++++++++++++++++++++*....+.......+.....+.........+.+..+.............+...+.........+..+.+.....+.........+.............+........+....+........+..................+...++++++
...+...+...+..+...+.............+.....+.+.....+......+...+.+++++++++++++++++++++++++++++++++++++++*......+...+.+...........+....+..+...+....+...+++++++++++++++++++++++++++++++++++++++*..+......+......+........+.+..+..........+..............+.........+......+.........+.+...+..........................+.........+...+.......+..+...+.+..+.......+...+...+.....+.+.....+.+........+......+...+......+.......+.........+......+.....+..........+..+............+...+.+.........+.....+.+..+....+......+.........+......+.....+.......+.....+......+.+...+......+........................+..+.........+....+.....+.+......+......+.................+.+..+....+........+...+...+.+.....++++++
-----
[root@JCR-vm ssl]# openssl x509 -in server.crt -text -noout
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number:
            67:fb:1b:59:93:c5:f9:56:7a:85:85:84:1d:0e:9e:06:18:1d:7c:e8
        Signature Algorithm: sha256WithRSAEncryption
        Issuer: C=US, ST=TestState, L=TestCity, O=MyOrg, OU=MyUnit, CN=jfrog.prodevans.com
        Validity
            Not Before: Jul 31 11:15:24 2025 GMT
            Not After : Jul 31 11:15:24 2026 GMT
        Subject: C=US, ST=TestState, L=TestCity, O=MyOrg, OU=MyUnit, CN=jfrog.prodevans.com
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: (2048 bit)
                Modulus:
                    00:e3:8f:08:1a:f5:a1:50:e4:5f:ce:fb:26:41:11:
                    21:12:48:0e:e2:0f:22:b6:a1:00:cb:3a:29:25:be:
                    d4:44:68:60:4c:3b:59:14:dd:85:e1:dd:79:7a:88:
                    c5:d1:2e:6d:f3:ae:37:4b:2d:2a:35:a4:57:5f:f3:
                    cd:55:ab:ca:83:fd:b9:8c:f4:76:1a:3f:33:dc:6c:
                    19:e0:a5:26:a2:30:dd:b7:33:34:7a:a2:9e:93:68:
                    05:5d:2b:f0:e2:e6:18:dc:04:68:c6:3e:19:de:8f:
                    8d:69:31:2d:ec:77:21:a5:91:34:18:89:50:8f:f7:
                    0d:6d:aa:f1:6e:23:c2:8f:5a:c4:45:9e:c4:c3:aa:
                    58:66:94:e7:3c:14:ac:a4:a6:0b:e4:fa:7a:14:d0:
                    de:48:a1:a2:1c:65:ee:b5:91:16:1f:be:4d:ba:0c:
                    c2:f4:fb:1b:32:1c:ea:a8:40:7e:22:a1:20:44:95:
                    0f:2b:e5:99:f5:1e:06:4f:e1:3d:68:0a:ec:a6:59:
                    3f:6b:44:3c:97:e5:8c:97:bb:1c:cd:40:59:9f:a1:
                    77:6a:e1:94:d5:5b:14:24:25:a9:50:ff:ef:70:34:
                    fd:ad:cf:75:56:15:2e:8b:0b:b2:ef:4d:ee:da:1b:
                    9f:e3:b6:19:a3:a8:82:c4:4c:6e:67:44:0b:ce:84:
                    90:7b
                Exponent: 65537 (0x10001)
        X509v3 extensions:
            X509v3 Subject Alternative Name:
                IP Address:192.168.95.69, IP Address:192.168.95.70, IP Address:192.168.95.71, IP Address:192.168.95.72, IP Address:192.168.95.73
            X509v3 Subject Key Identifier:
                91:2F:B2:32:19:04:D5:C5:9F:32:C9:9F:EF:EC:5F:39:8A:F4:F0:D1
    Signature Algorithm: sha256WithRSAEncryption
    Signature Value:
        30:0a:8a:80:11:07:9d:da:5c:35:0f:04:61:ad:00:95:2d:57:
        5e:47:37:83:45:e7:bd:21:45:7f:5d:7f:93:4c:0b:3e:7c:d1:
        b8:40:8e:fe:97:40:50:25:61:23:76:ad:dc:42:a8:a1:a7:ab:
        8b:ee:e3:b2:a7:bd:26:d9:c1:ed:1a:ac:ea:71:99:9a:af:a7:
        45:6e:2f:d7:45:96:5b:d4:46:b2:12:db:95:e2:ff:0c:7a:13:
        7d:30:b6:9d:ed:fa:e6:d2:d4:c7:7b:67:3c:6c:2b:7b:02:f3:
        65:4f:34:c4:94:ec:fb:0a:6a:1d:ea:02:89:6f:29:8e:0f:52:
        f2:60:ad:e5:00:d2:16:85:e2:71:f2:aa:b0:0f:59:39:ee:99:
        f0:2e:28:7d:96:c7:db:fa:95:3e:40:df:8d:e3:10:62:13:12:
        69:cc:74:89:74:be:c3:de:53:60:e2:ce:38:9b:14:bc:16:d2:
        41:22:2b:68:0a:f8:8f:93:4b:17:62:6f:bd:14:e3:3d:ca:c9:
        7d:4a:f1:9d:7a:d5:a2:06:5b:4c:29:01:d9:db:a4:9e:10:e1:
        7c:f2:8a:a0:b2:ca:a0:7b:d6:5f:31:c8:d6:c4:c6:42:d6:5a:
        0b:21:05:37:54:8a:ef:c7:b2:6b:67:6e:fd:04:6f:0f:66:1e:
        fe:ba:f5:9f
[root@JCR-vm ssl]# scp server.crt server.key root@192.168.95.71:/etc/ngins/ssl/
The authenticity of host '192.168.95.71 (192.168.95.71)' can't be established.
ED25519 key fingerprint is SHA256:b26YoekEpTFWNdTp6PNtAI14eUMa+XrDICmbeizT4ZQ.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '192.168.95.71' (ED25519) to the list of known hosts.
root@192.168.95.71's password:
Couldn't create directory: No such file or directory
[root@JCR-vm ssl]# scp server.crt server.key root@192.168.95.71:/etc/ngins/ssl/
root@192.168.95.71's password:
Couldn't create directory: No such file or directory
[root@JCR-vm ssl]# scp server.crt server.key root@192.168.95.71:/etc/nginx/ssl/
root@192.168.95.71's password:
server.crt                                                                                                                 100% 1359     2.6MB/s   00:00
server.key                                                                                                                 100% 1708     4.3MB/s   00:00
[root@JCR-vm ssl]# cd .. root@192.168.95.71:/etc/nginx/ssl/
-bash: cd: too many arguments
[root@JCR-vm ssl]# cd ..
[root@JCR-vm nginx]# pwd
/etc/nginx
[root@JCR-vm nginx]# scp nginx.conf root@192.168.95.71:/etc/nginx/ssl/
root@192.168.95.71's password:
nginx.conf                                                                                                                 100% 2296   536.8KB/s   00:00
[root@JCR-vm nginx]# scp nginx.conf root@192.168.95.71:/etc/nginx/
root@192.168.95.71's password:
nginx.conf                                                                                                                 100% 2296     1.8MB/s   00:00
[root@JCR-vm nginx]# ip r
default via 192.168.10.1 dev ens192
192.168.0.0/16 dev ens192 proto kernel scope link src 192.168.95.70 metric 100
[root@JCR-vm nginx]# sudo systemctl stop postgresql-16.service

[root@JCR-vm nginx]#
[root@JCR-vm nginx]#
[root@JCR-vm nginx]#
[root@JCR-vm nginx]# sudo systemctl stop postgresql-16.service sudo yum remove postgresql16*
^C
[root@JCR-vm nginx]# ^C
[root@JCR-vm nginx]# ^C
[root@JCR-vm nginx]# sudo yum remove postgresql16*
Updating Subscription Management repositories.
Dependencies resolved.
=============================================================================================================================================================
 Package                                    Architecture                 Version                                   Repository                           Size
=============================================================================================================================================================
Removing:
 postgresql16                               x86_64                       16.9-3PGDG.rhel9                          @@commandline                       9.8 M
 postgresql16-contrib                       x86_64                       16.9-3PGDG.rhel9                          @@commandline                       2.7 M
 postgresql16-libs                          x86_64                       16.9-3PGDG.rhel9                          @@commandline                       1.2 M
 postgresql16-server                        x86_64                       16.9-3PGDG.rhel9                          @@commandline                        29 M

Transaction Summary
=============================================================================================================================================================
Remove  4 Packages

Freed space: 43 M
Is this ok [y/N]: y
Running transaction check
Transaction check succeeded.
Running transaction test
Transaction test succeeded.
Running transaction
  Preparing        :                                                                                                                                     1/1
  Erasing          : postgresql16-contrib-16.9-3PGDG.rhel9.x86_64                                                                                        1/4
  Running scriptlet: postgresql16-server-16.9-3PGDG.rhel9.x86_64                                                                                         2/4
  Erasing          : postgresql16-server-16.9-3PGDG.rhel9.x86_64                                                                                         2/4
  Running scriptlet: postgresql16-server-16.9-3PGDG.rhel9.x86_64                                                                                         2/4
  Erasing          : postgresql16-16.9-3PGDG.rhel9.x86_64                                                                                                3/4
  Running scriptlet: postgresql16-16.9-3PGDG.rhel9.x86_64                                                                                                3/4
  Erasing          : postgresql16-libs-16.9-3PGDG.rhel9.x86_64                                                                                           4/4
  Running scriptlet: postgresql16-libs-16.9-3PGDG.rhel9.x86_64                                                                                           4/4
  Verifying        : postgresql16-16.9-3PGDG.rhel9.x86_64                                                                                                1/4
  Verifying        : postgresql16-contrib-16.9-3PGDG.rhel9.x86_64                                                                                        2/4
  Verifying        : postgresql16-libs-16.9-3PGDG.rhel9.x86_64                                                                                           3/4
  Verifying        : postgresql16-server-16.9-3PGDG.rhel9.x86_64                                                                                         4/4
Installed products updated.

Removed:
  postgresql16-16.9-3PGDG.rhel9.x86_64                postgresql16-contrib-16.9-3PGDG.rhel9.x86_64         postgresql16-libs-16.9-3PGDG.rhel9.x86_64
  postgresql16-server-16.9-3PGDG.rhel9.x86_64

Complete!
[root@JCR-vm nginx]# sudo rm -rf /var/lib/pgsql/16/
[root@JCR-vm nginx]# sudo rm -rf /var/lib/pgsql/data/
[root@JCR-vm nginx]# sudo rm -f /etc/yum.repos.d/pgdg-redhat-all.repo
[root@JCR-vm nginx]# sudo yum clean all
Updating Subscription Management repositories.
58 files removed
[root@JCR-vm nginx]#
[root@JCR-vm nginx]#
[root@JCR-vm nginx]#
[root@JCR-vm nginx]#
[root@JCR-vm nginx]# ssh 192.168.95.71
^C
[root@JCR-vm nginx]# ssh 192.168.95.70
The authenticity of host '192.168.95.70 (192.168.95.70)' can't be established.
ED25519 key fingerprint is SHA256:y2jhfx6O8rQ7JJTcLq+LhMjhCWMK3t0/zTCgAI0Iub0.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '192.168.95.70' (ED25519) to the list of known hosts.
root@192.168.95.70's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 18:17:48 2025 from 192.168.95.71
[root@JCR-vm ~]# exit
logout
Connection to 192.168.95.70 closed.
[root@JCR-vm nginx]# ls
conf.d        fastcgi.conf.default    koi-utf     mime.types.default  scgi_params          uwsgi_params
default.d     fastcgi_params          koi-win     nginx.conf          scgi_params.default  uwsgi_params.default
fastcgi.conf  fastcgi_params.default  mime.types  nginx.conf.default  ssl                  win-utf
[root@JCR-vm nginx]# ssh 192.168.95.73
The authenticity of host '192.168.95.73 (192.168.95.73)' can't be established.
ED25519 key fingerprint is SHA256:6AmRdL55F740oBG5S4K+WFjQz9yt6Fhg9Oq9Hj7DZ+k.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '192.168.95.73' (ED25519) to the list of known hosts.
root@192.168.95.73's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 18:18:18 2025 from 192.168.95.71
[root@localhost ~]# ls
anaconda-ks.cfg  k8s
[root@localhost ~]# exit
logout
Connection to 192.168.95.73 closed.
[root@JCR-vm nginx]# ssh 192.168.95.71
^C
[root@JCR-vm nginx]# ssh 192.168.95.69
The authenticity of host '192.168.95.69 (192.168.95.69)' can't be established.
ED25519 key fingerprint is SHA256:UMisHyl9B2jg334LzycpJp3DJtnBXbUecdxQaO4qDdQ.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '192.168.95.69' (ED25519) to the list of known hosts.
root@192.168.95.69's password:
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Thu Jul 31 18:20:08 2025 from 192.168.95.71
[root@internet-vm ~]# ls
anaconda-ks.cfg  java  jfrog  k8s  nginx  postgress
[root@internet-vm ~]# cd k8s/
[root@internet-vm k8s]# ls
conntrack-tools-1.4.7-4.el9_5.x86_64.rpm  kubeadm-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm
containerd.io-1.7.27-3.1.el9.x86_64.rpm   kubectl-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm
cri-tools-1.32.0-150500.1.1.x86_64.rpm    kubelet-1.32.7-150500.1.1.x86_64.rpm        libnetfilter_queue-1.0.5-1.el9.x86_64.rpm
haproxy-2.4.22-4.el9.x86_64.rpm           kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm
[root@internet-vm k8s]# rpm -ivh haproxy-2.4.22-4.el9.x86_64.rpm
Verifying...                          ################################# [100%]
Preparing...                          ################################# [100%]
        package haproxy-2.4.22-4.el9.x86_64 is already installed
[root@internet-vm k8s]# vi /etc/haproxy/haproxy.cfg
[root@internet-vm k8s]# sudo systemctl enable haproxy
sudo systemctl start haproxy
Created symlink /etc/systemd/system/multi-user.target.wants/haproxy.service → /usr/lib/systemd/system/haproxy.service.
^Coot@internet-vm k8s]# sudo systemctl start haproxy^C
[root@internet-vm k8s]# sudo systemctl status haproxy
● haproxy.service - HAProxy Load Balancer
     Loaded: loaded (/usr/lib/systemd/system/haproxy.service; enabled; preset: disabled)
     Active: active (running) since Thu 2025-07-31 18:25:56 IST; 15s ago
    Process: 109899 ExecStartPre=/usr/sbin/haproxy -f $CONFIG -f $CFGDIR -c -q $OPTIONS (code=exited, status=0/SUCCESS)
   Main PID: 109901 (haproxy)
      Tasks: 3 (limit: 22987)
     Memory: 4.4M
        CPU: 44ms
     CGroup: /system.slice/haproxy.service
             ├─109901 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -f /etc/haproxy/conf.d -p /run/haproxy.pid
             └─109903 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -f /etc/haproxy/conf.d -p /run/haproxy.pid

Jul 31 18:25:56 internet-vm systemd[1]: Starting HAProxy Load Balancer...
Jul 31 18:25:56 internet-vm haproxy[109901]: [WARNING]  (109901) : parsing [/etc/haproxy/haproxy.cfg:49] : 'option httplog' not usable with frontend 'kubern>
Jul 31 18:25:56 internet-vm haproxy[109901]: [WARNING]  (109901) : config : 'option forwardfor' ignored for frontend 'kubernetes' as it requires HTTP mode.
Jul 31 18:25:56 internet-vm haproxy[109901]: [WARNING]  (109901) : config : 'option forwardfor' ignored for backend 'k8s-masters' as it requires HTTP mode.
Jul 31 18:25:56 internet-vm haproxy[109901]: [NOTICE]   (109901) : New worker #1 (109903) forked
Jul 31 18:25:56 internet-vm systemd[1]: Started HAProxy Load Balancer.
Jul 31 18:25:57 internet-vm haproxy[109903]: [WARNING]  (109903) : Server k8s-masters/master2 is DOWN, reason: Layer4 connection problem, info: "Connection >
Jul 31 18:25:57 internet-vm haproxy[109903]: [WARNING]  (109903) : Server k8s-masters/master3 is DOWN, reason: Layer4 connection problem, info: "Connection >

[root@internet-vm k8s]# ll
total 101492
-rw-r--r--. 1 root root   245555 Jul 31 17:50 conntrack-tools-1.4.7-4.el9_5.x86_64.rpm
-rw-r--r--. 1 root root 46425075 Jul 31 17:50 containerd.io-1.7.27-3.1.el9.x86_64.rpm
-rw-r--r--. 1 root root  7466328 Jul 31 17:50 cri-tools-1.32.0-150500.1.1.x86_64.rpm
-rw-r--r--. 1 root root  2292574 Jul 31 17:50 haproxy-2.4.22-4.el9.x86_64.rpm
-rw-r--r--. 1 root root 12315908 Jul 31 17:50 kubeadm-1.32.7-150500.1.1.x86_64.rpm
-rw-r--r--. 1 root root 11354832 Jul 31 17:50 kubectl-1.32.7-150500.1.1.x86_64.rpm
-rw-r--r--. 1 root root 15307556 Jul 31 17:50 kubelet-1.32.7-150500.1.1.x86_64.rpm
-rw-r--r--. 1 root root  8413224 Jul 31 17:50 kubernetes-cni-1.6.0-150500.1.1.x86_64.rpm
-rw-r--r--. 1 root root    26117 Jul 31 17:50 libnetfilter_cthelper-1.0.0-22.el9.x86_64.rpm
-rw-r--r--. 1 root root    25961 Jul 31 17:50 libnetfilter_cttimeout-1.0.0-19.el9.x86_64.rpm
-rw-r--r--. 1 root root    31359 Jul 31 17:50 libnetfilter_queue-1.0.5-1.el9.x86_64.rpm
[root@internet-vm k8s]# vi kubeadm-config.yaml
[root@internet-vm k8s]# kubeadm init --config kubeadm-config.yaml --upload-certs
W0731 18:27:47.076499  109992 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
[init] Using Kubernetes version: v1.32.0
[preflight] Running pre-flight checks
W0731 18:27:47.081937  109992 checks.go:1077] [preflight] WARNING: Couldn't create the interface used for talking to the container runtime: failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint "unix:///var/run/containerd/containerd.sock": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService
        [WARNING Hostname]: hostname "internet-vm" could not be reached
        [WARNING Hostname]: hostname "internet-vm": lookup internet-vm on 8.8.8.8:53: no such host
error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR Port-6443]: Port 6443 is in use
        [ERROR FileContent--proc-sys-net-ipv4-ip_forward]: /proc/sys/net/ipv4/ip_forward contents are not set to 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
[root@internet-vm k8s]# vi /etc/haproxy/haproxy.cfg
[root@internet-vm k8s]# ssh 192.168.95.71
The authenticity of host '192.168.95.71 (192.168.95.71)' can't be established.
ED25519 key fingerprint is SHA256:b26YoekEpTFWNdTp6PNtAI14eUMa+XrDICmbeizT4ZQ.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '192.168.95.71' (ED25519) to the list of known hosts.
root@192.168.95.71's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 16:11:12 2025 from 10.9.0.30
[root@localhost ~]# uptime
 18:29:12 up 1 day, 22:54,  5 users,  load average: 0.37, 8.16, 6.18
[root@localhost ~]# cd k8s/
[root@localhost k8s]# ls
1
[root@localhost k8s]# exit
logout
Connection to 192.168.95.71 closed.
[root@internet-vm k8s]# sudo systemctl stop haproxy
[root@internet-vm k8s]# sudo systemctl disable haproxy
Removed "/etc/systemd/system/multi-user.target.wants/haproxy.service".
[root@internet-vm k8s]# cat /etc/haproxy/haproxy.cfg
#---------------------------------------------------------------------
# Example configuration for a possible web application.  See the
# full configuration options online.
#
#   https://www.haproxy.org/download/1.8/doc/configuration.txt
#
#---------------------------------------------------------------------

#---------------------------------------------------------------------
# Global settings
#---------------------------------------------------------------------
global
    # to have these messages end up in /var/log/haproxy.log you will
    # need to:
    #
    # 1) configure syslog to accept network log events.  This is done
    #    by adding the '-r' option to the SYSLOGD_OPTIONS in
    #    /etc/sysconfig/syslog
    #
    # 2) configure local2 events to go to the /var/log/haproxy.log
    #   file. A line like the following can be added to
    #   /etc/sysconfig/syslog
    #
    #    local2.*                       /var/log/haproxy.log
    #
    log         127.0.0.1 local2

    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon

    # turn on stats unix socket
    stats socket /var/lib/haproxy/stats

    # utilize system-wide crypto-policies
    ssl-default-bind-ciphers PROFILE=SYSTEM
    ssl-default-server-ciphers PROFILE=SYSTEM

#---------------------------------------------------------------------
# common defaults that all the 'listen' and 'backend' sections will
# use if not designated in their block
#---------------------------------------------------------------------
defaults
    mode                    http
    log                     global
    option                  httplog
    option                  dontlognull
    option http-server-close
    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout http-keep-alive 10s
    timeout check           10s
    maxconn                 3000

#---------------------------------------------------------------------
# main frontend which proxys to the backends
#---------------------------------------------------------------------
frontend kubernetes
    bind *:6443
    mode tcp
    default_backend k8s-masters

backend k8s-masters
    mode tcp
    balance roundrobin
    option tcp-check
    server master1 192.168.95.69:6443 check
    server master2 192.168.95.70:6443 check
    server master3 192.168.95.71:6443 check


#---------------------------------------------------------------------
# static backend for serving up images, stylesheets and such
#---------------------------------------------------------------------
# round robin balancing between the various backends
#---------------------------------------------------------------------
[root@internet-vm k8s]# ssh 192.168.95.71
root@192.168.95.71's password:
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 31 18:29:08 2025 from 192.168.95.69
[root@localhost ~]# yum install haproxy
Updating Subscription Management repositories.
Repository kubernetes is listed more than once in the configuration
Last metadata expiration check: 0:52:18 ago on Thursday 31 July 2025 05:38:24 PM.
Package haproxy-2.4.22-4.el9.x86_64 is already installed.
Dependencies resolved.
Nothing to do.
Complete!
[root@localhost ~]# vi /etc/haproxy/haproxy.cfg
[root@localhost ~]# sudo systemctl enable haproxy
sudo systemctl start haproxy
Created symlink /etc/systemd/system/multi-user.target.wants/haproxy.service → /usr/lib/systemd/system/haproxy.service.
^Coot@localhost ~]# sudo systemctl start haproxy^C
[root@localhost ~]# sudo systemctl status haproxy
● haproxy.service - HAProxy Load Balancer
     Loaded: loaded (/usr/lib/systemd/system/haproxy.service; enabled; preset: disabled)
     Active: active (running) since Thu 2025-07-31 18:31:28 IST; 11s ago
    Process: 105619 ExecStartPre=/usr/sbin/haproxy -f $CONFIG -f $CFGDIR -c -q $OPTIONS (code=exited, status=0/SUCCESS)
   Main PID: 105621 (haproxy)
      Tasks: 3 (limit: 23020)
     Memory: 6.9M
        CPU: 32ms
     CGroup: /system.slice/haproxy.service
             ├─105621 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -f /etc/haproxy/conf.d -p /run/haproxy.pid
             └─105623 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -f /etc/haproxy/conf.d -p /run/haproxy.pid

Jul 31 18:31:28 localhost.localdomain systemd[1]: Starting HAProxy Load Balancer...
Jul 31 18:31:28 localhost.localdomain haproxy[105621]: [WARNING]  (105621) : parsing [/etc/haproxy/haproxy.cfg:49] : 'option httplog' not usable with fronte>
Jul 31 18:31:28 localhost.localdomain haproxy[105621]: [WARNING]  (105621) : config : 'option forwardfor' ignored for frontend 'kubernetes' as it requires H>
Jul 31 18:31:28 localhost.localdomain haproxy[105621]: [WARNING]  (105621) : config : 'option forwardfor' ignored for backend 'k8s-masters' as it requires H>
Jul 31 18:31:28 localhost.localdomain haproxy[105621]: [NOTICE]   (105621) : New worker #1 (105623) forked
Jul 31 18:31:28 localhost.localdomain systemd[1]: Started HAProxy Load Balancer.
Jul 31 18:31:28 localhost.localdomain haproxy[105623]: [WARNING]  (105623) : Server k8s-masters/master1 is DOWN, reason: Layer4 connection problem, info: "C>
Jul 31 18:31:29 localhost.localdomain haproxy[105623]: [WARNING]  (105623) : Server k8s-masters/master2 is DOWN, reason: Layer4 connection problem, info: "C>
lines 1-20/20 (END)
^C
[root@localhost ~]# ^C
[root@localhost ~]# exit
logout
Connection to 192.168.95.71 closed.
[root@internet-vm k8s]# kubeadm init --config kubeadm-config.yaml --upload-certs
W0731 18:31:51.580178  110210 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
[init] Using Kubernetes version: v1.32.0
[preflight] Running pre-flight checks
W0731 18:31:51.585797  110210 checks.go:1077] [preflight] WARNING: Couldn't create the interface used for talking to the container runtime: failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint "unix:///var/run/containerd/containerd.sock": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService
        [WARNING Hostname]: hostname "internet-vm" could not be reached
        [WARNING Hostname]: hostname "internet-vm": lookup internet-vm on 8.8.8.8:53: no such host
error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR FileContent--proc-sys-net-ipv4-ip_forward]: /proc/sys/net/ipv4/ip_forward contents are not set to 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
[root@internet-vm k8s]# cat /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
[root@internet-vm k8s]# sysctl -w /proc/sys/net/ipv4/ip_forward=1
sysctl: cannot stat /proc/sys//proc/sys/net/ipv4/ip_forward: No such file or directory
[root@internet-vm k8s]# sysctl net.ipv4.ip_forward=1
net.ipv4.ip_forward = 1
[root@internet-vm k8s]# kubeadm init --config kubeadm-config.yaml --upload-certs
W0731 18:32:57.889823  110276 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
[init] Using Kubernetes version: v1.32.0
[preflight] Running pre-flight checks
W0731 18:32:57.893692  110276 checks.go:1077] [preflight] WARNING: Couldn't create the interface used for talking to the container runtime: failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint "unix:///var/run/containerd/containerd.sock": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService
        [WARNING Hostname]: hostname "internet-vm" could not be reached
        [WARNING Hostname]: hostname "internet-vm": lookup internet-vm on 8.8.8.8:53: no such host
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
error execution phase preflight: [preflight] Some fatal errors occurred:
failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint "unix:///var/run/containerd/containerd.sock": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]# kubeadm init --config kubeadm-config.yaml --upload-certs --ignore-preflight-errors=
W0731 18:33:56.624271  110332 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
[init] Using Kubernetes version: v1.32.0
[preflight] Running pre-flight checks
W0731 18:33:56.627864  110332 checks.go:1077] [preflight] WARNING: Couldn't create the interface used for talking to the container runtime: failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint "unix:///var/run/containerd/containerd.sock": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService
        [WARNING Hostname]: hostname "internet-vm" could not be reached
        [WARNING Hostname]: hostname "internet-vm": lookup internet-vm on 8.8.8.8:53: no such host
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
error execution phase preflight: [preflight] Some fatal errors occurred:
failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint "unix:///var/run/containerd/containerd.sock": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]# kubeadm init --config kubeadm-config.yaml --upload-certs --ignore-preflight-errors
flag needs an argument: --ignore-preflight-errors
To see the stack trace of this error execute with --v=5 or higher
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]# kubeadm init --config kubeadm-config.yaml --upload-certs --ignore-preflight-errors=1
W0731 18:34:13.353885  110360 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
[init] Using Kubernetes version: v1.32.0
[preflight] Running pre-flight checks
W0731 18:34:13.357678  110360 checks.go:1077] [preflight] WARNING: Couldn't create the interface used for talking to the container runtime: failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint "unix:///var/run/containerd/containerd.sock": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService
        [WARNING Hostname]: hostname "internet-vm" could not be reached
        [WARNING Hostname]: hostname "internet-vm": lookup internet-vm on 8.8.8.8:53: no such host
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
error execution phase preflight: [preflight] Some fatal errors occurred:
failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint "unix:///var/run/containerd/containerd.sock": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]# cat kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.32.0
controlPlaneEndpoint: "192.168.95.69:6443"
imageRepository: "192.168.95.71:9443"
[root@internet-vm k8s]# cat /etc/containerd/config.toml
#   Copyright 2018-2022 Docker Inc.

#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at

#       http://www.apache.org/licenses/LICENSE-2.0

#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.

disabled_plugins = ["cri"]

#root = "/var/lib/containerd"
#state = "/run/containerd"
#subreaper = true
#oom_score = 0

#[grpc]
#  address = "/run/containerd/containerd.sock"
#  uid = 0
#  gid = 0

#[debug]
#  address = "/run/containerd/debug.sock"
#  uid = 0
#  gid = 0
#  level = "info"
[root@internet-vm k8s]# vim /etc/containerd/config.toml
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]# systemctl restart containerd.service
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]# systemctl status containerd.service
● containerd.service - containerd container runtime
     Loaded: loaded (/usr/lib/systemd/system/containerd.service; enabled; preset: disabled)
     Active: active (running) since Thu 2025-07-31 18:36:59 IST; 7s ago
       Docs: https://containerd.io
    Process: 110499 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS)
   Main PID: 110502 (containerd)
      Tasks: 7
     Memory: 15.0M
        CPU: 77ms
     CGroup: /system.slice/containerd.service
             └─110502 /usr/bin/containerd

Jul 31 18:36:59 internet-vm containerd[110502]: time="2025-07-31T18:36:59.354461636+05:30" level=info msg="skip loading plugin \"io.containerd.tracing.proce>
Jul 31 18:36:59 internet-vm containerd[110502]: time="2025-07-31T18:36:59.354482623+05:30" level=info msg="loading plugin \"io.containerd.internal.v1.tracin>
Jul 31 18:36:59 internet-vm containerd[110502]: time="2025-07-31T18:36:59.354504800+05:30" level=info msg="skip loading plugin \"io.containerd.internal.v1.t>
Jul 31 18:36:59 internet-vm containerd[110502]: time="2025-07-31T18:36:59.354523807+05:30" level=info msg="loading plugin \"io.containerd.grpc.v1.healthchec>
Jul 31 18:36:59 internet-vm containerd[110502]: time="2025-07-31T18:36:59.354544820+05:30" level=info msg="loading plugin \"io.containerd.nri.v1.nri\"..." t>
Jul 31 18:36:59 internet-vm containerd[110502]: time="2025-07-31T18:36:59.354560342+05:30" level=info msg="NRI interface is disabled by configuration."
Jul 31 18:36:59 internet-vm containerd[110502]: time="2025-07-31T18:36:59.355267888+05:30" level=info msg=serving... address=/run/containerd/containerd.sock>
Jul 31 18:36:59 internet-vm containerd[110502]: time="2025-07-31T18:36:59.355418726+05:30" level=info msg=serving... address=/run/containerd/containerd.sock
Jul 31 18:36:59 internet-vm containerd[110502]: time="2025-07-31T18:36:59.355561119+05:30" level=info msg="containerd successfully booted in 0.034692s"
Jul 31 18:36:59 internet-vm systemd[1]: Started containerd container runtime.
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]# kubeadm init --config kubeadm-config.yaml --upload-certs
W0731 18:37:15.826708  110527 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
[init] Using Kubernetes version: v1.32.0
[preflight] Running pre-flight checks
W0731 18:37:15.831986  110527 checks.go:1077] [preflight] WARNING: Couldn't create the interface used for talking to the container runtime: failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint "unix:///var/run/containerd/containerd.sock": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService
        [WARNING Hostname]: hostname "internet-vm" could not be reached
        [WARNING Hostname]: hostname "internet-vm": lookup internet-vm on 8.8.8.8:53: no such host
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
error execution phase preflight: [preflight] Some fatal errors occurred:
failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint "unix:///var/run/containerd/containerd.sock": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]# ls -l /var/run/containerd/containerd.sock
srw-rw----. 1 root root 0 Jul 31 18:36 /var/run/containerd/containerd.sock
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]# kubeadm init --config kubeadm-config.yaml --upload-certs
W0731 18:38:11.456518  110580 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
[init] Using Kubernetes version: v1.32.0
[preflight] Running pre-flight checks
W0731 18:38:11.461204  110580 checks.go:1077] [preflight] WARNING: Couldn't create the interface used for talking to the container runtime: failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint "unix:///var/run/containerd/containerd.sock": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService
        [WARNING Hostname]: hostname "internet-vm" could not be reached
        [WARNING Hostname]: hostname "internet-vm": lookup internet-vm on 8.8.8.8:53: no such host
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
error execution phase preflight: [preflight] Some fatal errors occurred:
failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint "unix:///var/run/containerd/containerd.sock": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]# netstat -nr
Kernel IP routing table
Destination     Gateway         Genmask         Flags   MSS Window  irtt Iface
0.0.0.0         192.168.10.1    0.0.0.0         UG        0 0          0 ens192
192.168.0.0     0.0.0.0         255.255.0.0     U         0 0          0 ens192
[root@internet-vm k8s]# route delete default gw 192.168.10.1
[root@internet-vm k8s]# netstat -nr
Kernel IP routing table
Destination     Gateway         Genmask         Flags   MSS Window  irtt Iface
192.168.0.0     0.0.0.0         255.255.0.0     U         0 0          0 ens192
[root@internet-vm k8s]# kubeadm init --config kubeadm-config.yaml --upload-certs
W0731 18:38:36.813359  110613 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
W0731 18:38:36.815084  110613 common.go:199] WARNING: could not obtain a bind address for the API Server: no default routes found in "/proc/net/route" or "/proc/net/ipv6_route"; using: 0.0.0.0
cannot use "0.0.0.0" as the bind address for the API Server
To see the stack trace of this error execute with --v=5 or higher
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]# cat kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.32.0
controlPlaneEndpoint: "192.168.95.69:6443"
imageRepository: "192.168.95.71:9443"
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]# kubeadm init --config kubeadm-config.yaml --control-plane-endpoint "192.169.95.71:6443" --upload-certs
can not mix '--config' with arguments [control-plane-endpoint]
To see the stack trace of this error execute with --v=5 or higher
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]# cat kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.32.0
controlPlaneEndpoint: "192.168.95.69:6443"
imageRepository: "192.168.95.71:9443"
[root@internet-vm k8s]# vi kubeadm-config.yaml
[root@internet-vm k8s]# kubeadm init --config kubeadm-config.yaml --upload-certs
W0731 18:40:17.218337  110701 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
W0731 18:40:17.219934  110701 common.go:199] WARNING: could not obtain a bind address for the API Server: no default routes found in "/proc/net/route" or "/proc/net/ipv6_route"; using: 0.0.0.0
cannot use "0.0.0.0" as the bind address for the API Server
To see the stack trace of this error execute with --v=5 or higher
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]# ip a s
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: ens192: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 00:0c:29:b2:40:7a brd ff:ff:ff:ff:ff:ff
    altname enp11s0
    inet 192.168.95.69/16 brd 192.168.255.255 scope global noprefixroute ens192
       valid_lft forever preferred_lft forever
    inet6 fe80::20c:29ff:feb2:407a/64 scope link noprefixroute
       valid_lft forever preferred_lft forever
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]# cat kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.32.0
controlPlaneEndpoint: "192.168.95.71:6443"
imageRepository: "192.168.95.71:9443"
[root@internet-vm k8s]# nc -zv 192.168.95.71 6443
Ncat: Version 7.92 ( https://nmap.org/ncat )
Ncat: Connected to 192.168.95.71:6443.
Ncat: 0 bytes sent, 0 bytes received in 0.02 seconds.
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]# route add default gw 192.168.10.1
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]# kubeadm init --config kubeadm-config.yaml --upload-certs
W0731 18:43:10.058834  110823 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
[init] Using Kubernetes version: v1.32.0
[preflight] Running pre-flight checks
W0731 18:43:10.064513  110823 checks.go:1077] [preflight] WARNING: Couldn't create the interface used for talking to the container runtime: failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint "unix:///var/run/containerd/containerd.sock": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService
        [WARNING Hostname]: hostname "internet-vm" could not be reached
        [WARNING Hostname]: hostname "internet-vm": lookup internet-vm on 8.8.8.8:53: no such host
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
error execution phase preflight: [preflight] Some fatal errors occurred:
failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint "unix:///var/run/containerd/containerd.sock": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]# vi kubeadm-config.yaml
[root@internet-vm k8s]# vi kubeadm-config.yaml
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]# kubeadm init --config kubeadm-config.yaml --upload-certs
W0731 18:44:07.135697  110882 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
W0731 18:44:07.136367  110882 initconfiguration.go:332] error unmarshaling configuration schema.GroupVersionKind{Group:"kubeadm.k8s.io", Version:"v1beta3", Kind:"ClusterConfiguration"}: strict decoding error: unknown field "nodeRegistration"
[init] Using Kubernetes version: v1.32.0
[preflight] Running pre-flight checks
W0731 18:44:07.139562  110882 checks.go:1077] [preflight] WARNING: Couldn't create the interface used for talking to the container runtime: failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint "unix:///var/run/containerd/containerd.sock": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService
        [WARNING Hostname]: hostname "internet-vm" could not be reached
        [WARNING Hostname]: hostname "internet-vm": lookup internet-vm on 8.8.8.8:53: no such host
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
error execution phase preflight: [preflight] Some fatal errors occurred:
failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint "unix:///var/run/containerd/containerd.sock": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]# vi kubeadm-config.yaml
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]# kubeadm init --config kubeadm-config.yaml --upload-certs
[init] Using Kubernetes version: v1.32.0
[preflight] Running pre-flight checks
W0731 18:49:05.202415  111089 checks.go:1077] [preflight] WARNING: Couldn't create the interface used for talking to the container runtime: failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint "unix:///var/run/containerd/containerd.sock": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService
        [WARNING Hostname]: hostname "internet-vm" could not be reached
        [WARNING Hostname]: hostname "internet-vm": lookup internet-vm on 8.8.8.8:53: no such host
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
error execution phase preflight: [preflight] Some fatal errors occurred:
failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint "unix:///var/run/containerd/containerd.sock": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
[root@internet-vm k8s]#
[root@internet-vm k8s]#
[root@internet-vm k8s]#


========



